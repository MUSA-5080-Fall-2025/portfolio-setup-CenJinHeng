[
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "geometry: shape and location\nattributes: data about the feature\ntwo forms in GIS: raster (pixel, continous) and vector (points, lines and polygons)\nformat of geo data: shp, geojson, KML/KMZ (Google Earth)…\n\n\n\n\n\nincludes shp (storaging geometry), shx, dbf (tabel)\n\n\n\n\n\ndrop geo column if it is troublesome\n\nst_intersects(): Counties affected by flooding\nst_touches(): Neighboring counties\nst_within(): Schools within district boundaries\nst_contains(): Districts containing hospitals\nst_overlaps(): Overlapping service areas\nst_disjoint(): Counties separate from urban areas\n\n\n\n\n\n\nThe Earth is round, but maps are flat\nProblems: hard to preserve area, distance, and angles simultaneously\nCalculation steps\n\nstep 1: approximate the earth Ellipsoid\nstep 2: tie the Ellipsoid into the real Earth\nstep 3: put down the lat/lng grid\n\nGeographic (Geodetic) coordinate systems (Lat/lng)\n\nMercator: keeps lines(lat) constantly long, areas far from the central line are in wrong shape\nTransverse: using Lng\nConic\ncylindrical\nplaner\n\nProjected coordinate systems\n\nUTM\n\nGCS:\n\nLat/Lng coordinated\ngood for large scale, bad for area/distance calculations\n\nPCS:\n\nX/Y coordinates on a flat plane\nunits: meters, feet\ngood for local analysis, accurate measurement, bad for large areas, global data sets\n\nst_set_crs() is to set CRS if missing, st_transform() is to change CRS"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#spatial-data",
    "href": "weekly-notes/week-04-notes.html#spatial-data",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "geometry: shape and location\nattributes: data about the feature\ntwo forms in GIS: raster (pixel, continous) and vector (points, lines and polygons)\nformat of geo data: shp, geojson, KML/KMZ (Google Earth)…"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#shp",
    "href": "weekly-notes/week-04-notes.html#shp",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "includes shp (storaging geometry), shx, dbf (tabel)"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#sf-package",
    "href": "weekly-notes/week-04-notes.html#sf-package",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "drop geo column if it is troublesome\n\nst_intersects(): Counties affected by flooding\nst_touches(): Neighboring counties\nst_within(): Schools within district boundaries\nst_contains(): Districts containing hospitals\nst_overlaps(): Overlapping service areas\nst_disjoint(): Counties separate from urban areas"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coordinate-reference-systems",
    "href": "weekly-notes/week-04-notes.html#coordinate-reference-systems",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "The Earth is round, but maps are flat\nProblems: hard to preserve area, distance, and angles simultaneously\nCalculation steps\n\nstep 1: approximate the earth Ellipsoid\nstep 2: tie the Ellipsoid into the real Earth\nstep 3: put down the lat/lng grid\n\nGeographic (Geodetic) coordinate systems (Lat/lng)\n\nMercator: keeps lines(lat) constantly long, areas far from the central line are in wrong shape\nTransverse: using Lng\nConic\ncylindrical\nplaner\n\nProjected coordinate systems\n\nUTM\n\nGCS:\n\nLat/Lng coordinated\ngood for large scale, bad for area/distance calculations\n\nPCS:\n\nX/Y coordinates on a flat plane\nunits: meters, feet\ngood for local analysis, accurate measurement, bad for large areas, global data sets\n\nst_set_crs() is to set CRS if missing, st_transform() is to change CRS"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes - Course Introduction",
    "section": "",
    "text": "bias in visualization summary can hide some important details example: Anscombe’s Quartet\n\ncan not get ACS for census block, only decenial\ncensus block groups have big margin of error (ACS), T island problem\ncensus tracts are better\n\nthe smaller of the sample, the bigger margin of error\n\n\n\nggplot ( data = your_data ) + aes ( x = variable1, y = variable2 ) + geom_something ( ) + additional_layers (color… )\naes: - X,y - color - fill - size - shape - alpha _ transparency\n\n\n\n\ndistribution\n\n\n\n\n\nleft join is preserve the table at first (often option)\nright join …. The second table\nfull join is to preserve all the tables (necessary sometimes)\ninner joint is to find the result held by both tables"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#why-visualization-matters",
    "href": "weekly-notes/week-03-notes.html#why-visualization-matters",
    "title": "Week 3 Notes - Course Introduction",
    "section": "",
    "text": "bias in visualization summary can hide some important details example: Anscombe’s Quartet\n\ncan not get ACS for census block, only decenial\ncensus block groups have big margin of error (ACS), T island problem\ncensus tracts are better\n\nthe smaller of the sample, the bigger margin of error"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#grammar-of-graphics",
    "href": "weekly-notes/week-03-notes.html#grammar-of-graphics",
    "title": "Week 3 Notes - Course Introduction",
    "section": "",
    "text": "ggplot ( data = your_data ) + aes ( x = variable1, y = variable2 ) + geom_something ( ) + additional_layers (color… )\naes: - X,y - color - fill - size - shape - alpha _ transparency"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#exploratory-data-analysis",
    "href": "weekly-notes/week-03-notes.html#exploratory-data-analysis",
    "title": "Week 3 Notes - Course Introduction",
    "section": "",
    "text": "distribution"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#join",
    "href": "weekly-notes/week-03-notes.html#join",
    "title": "Week 3 Notes - Course Introduction",
    "section": "",
    "text": "left join is preserve the table at first (often option)\nright join …. The second table\nfull join is to preserve all the tables (necessary sometimes)\ninner joint is to find the result held by both tables"
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#assignment-overview",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\n# Load required packages\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\nlibrary(knitr)\n\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\", install = TRUE, overwrite=TRUE)\n\n[1] \"42bf8a20a3df1def380f330cf7edad0dd5842ce6\"\n\n# Load spatial data\npa_counties &lt;- st_read(\"data/Pennsylvania_County_Boundaries.shp\")\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `/Users/jinhengcen/Documents/portfolio-setup-CenJinHeng/assignments/assignment_2/data/Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nhospitals &lt;- st_read(\"data/hospitals.geojson\")\n\nReading layer `hospitals' from data source \n  `/Users/jinhengcen/Documents/portfolio-setup-CenJinHeng/assignments/assignment_2/data/hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\n\n# Check that all data loaded correctly\npa_counties\n\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\nFirst 10 features:\n   OBJECTID MSLINK COUNTY_NAM COUNTY_NUM FIPS_COUNT COUNTY_ARE COUNTY_PER\n1       336     46 MONTGOMERY         46        091       &lt;NA&gt;       &lt;NA&gt;\n2       337      8   BRADFORD         08        015       &lt;NA&gt;       &lt;NA&gt;\n3       338      9      BUCKS         09        017       &lt;NA&gt;       &lt;NA&gt;\n4       339     58      TIOGA         58        117       &lt;NA&gt;       &lt;NA&gt;\n5       340     59      UNION         59        119       &lt;NA&gt;       &lt;NA&gt;\n6       341     60    VENANGO         60        121       &lt;NA&gt;       &lt;NA&gt;\n7       342     62 WASHINGTON         62        125       &lt;NA&gt;       &lt;NA&gt;\n8       343     63      WAYNE         63        127       &lt;NA&gt;       &lt;NA&gt;\n9       344     42     MCKEAN         42        083       &lt;NA&gt;       &lt;NA&gt;\n10      345     43     MERCER         43        085       &lt;NA&gt;       &lt;NA&gt;\n   NUMERIC_LA COUNTY_N_1 AREA_SQ_MI SOUND SPREAD_SHE IMAGE_NAME NOTE_FILE VIDEO\n1           5         46   487.4271  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n2           2          8  1161.3379  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n3           5          9   622.0836  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n4           2         58  1137.2480  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n5           2         59   319.1893  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n6           3         60   683.3676  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n7           1         62   862.1077  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n8           2         63   750.8286  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n9           1         42   985.2700  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n10          3         43   682.3598  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n   DISTRICT_N PA_CTY_COD MAINT_CTY_ DISTRICT_O                       geometry\n1          06         46          4        6-4 MULTIPOLYGON (((-8398884 48...\n2          03         08          9        3-9 MULTIPOLYGON (((-8558633 51...\n3          06         09          1        6-1 MULTIPOLYGON (((-8367360 49...\n4          03         59          7        3-7 MULTIPOLYGON (((-8558633 51...\n5          03         60          8        3-8 MULTIPOLYGON (((-8562865 49...\n6          01         61          5        1-5 MULTIPOLYGON (((-8870781 50...\n7          12         63          4       12-4 MULTIPOLYGON (((-8935296 48...\n8          04         64          6        4-6 MULTIPOLYGON (((-8368003 50...\n9          02         42          5        2-5 MULTIPOLYGON (((-8705967 51...\n10         01         43          4        1-4 MULTIPOLYGON (((-8956031 50...\n\nhospitals\n\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8960798 ymin: 4829916 xmax: -8334160 ymax: 5181077\nProjected CRS: WGS 84 / Pseudo-Mercator\nFirst 10 features:\n                  CHIEF_EXEC              CHIEF_EX_1\n1              Peter J Adamo               President\n2           Autumn DeShields Chief Executive Officer\n3               Shawn Parekh Chief Executive Officer\n4                DIANE HRITZ Chief Executive Officer\n5             Tim Harclerode Chief Executive Officer\n6  Richard McLaughlin MD MBA Chief Executive Officer\n7             Laura Murnyack             Interim CEO\n8                  Adam Beck Chief Executive Officer\n9                Pamela Keen Chief Executive Officer\n10              Mark Papalia           President/CEO\n                                 FACILITY_U LONGITUDE       COUNTY\n1              https://www.phhealthcare.org -79.91131   Washington\n2                 https://www.malvernbh.com -75.17005 Philadelphia\n3            https://roxboroughmemorial.com -75.20963 Philadelphia\n4                https://www.ashospital.net -80.27907   Washington\n5                 https://www.conemaugh.org -79.02513     Somerset\n6                   https://towerhealth.org -75.61213   Montgomery\n7         https://bucktailmedicalcenter.org -77.73649      Clinton\n8  https://www.selectspecialtyhospitals.com -76.88013      Dauphin\n9          https://www.childrenshomepgh.org -79.93736    Allegheny\n10    https://www.kanecommunityhospital.com -78.81705       McKean\n                                                  FACILITY_N\n1                                  Penn Highlands Mon Valley\n2                                  MALVERN BEHAVIORAL HEALTH\n3                               Roxborough Memorial Hospital\n4                                 ADVANCED SURGICAL HOSPITAL\n5                    DLP Conemaugh Meyersdale Medical Center\n6                                    Pottstown Hospital, LLC\n7                                    Bucktail medical Center\n8  SELECT SPECIALTY HOSPITAL CENTRAL PENNSYLVANIA HARRISBURG\n9                          The Children's Home of Pittsburgh\n10              University of Pittsburgh Medical Center Kane\n                           STREET   CITY_OR_BO LATITUDE   TELEPHONE_ ZIP_CODE\n1          1163 Country Club Road  Monongahela 40.18193 724-258-1000    15063\n2  1930 South Broad Street Unit 4 Philadelphia 39.92619 610-480-8919    19145\n3               5800 Ridge Avenue Philadelphia 40.02869 215-483-9900    19128\n4        100 TRICH DRIVE\\nSUITE 1   WASHINGTON 40.15655   7248840710    15301\n5              200 Hospital Drive   Meyersdale 39.80913 814-634-5911    15552\n6           1600 East High Street    Pottstown 40.24273   6103277000    19464\n7                1001 Pine Street       Renovo 41.32789 570-923-1000    17764\n8          111 South Front Street   Harrisburg 40.25841 717-724-6604    17110\n9                5324 Penn Avenue   Pittsburgh 40.46424 412-441-4884    15224\n10                4372 US Route 6         Kane 41.67188   8148378585    16735\n                   geometry\n1  POINT (-8895686 4892415)\n2  POINT (-8367892 4855223)\n3  POINT (-8372298 4870112)\n4  POINT (-8936626 4888717)\n5  POINT (-8797037 4838244)\n6  POINT (-8417104 4901278)\n7  POINT (-8653586 5060826)\n8  POINT (-8558256 4903566)\n9  POINT (-8898587 4933636)\n10 POINT (-8773874 5111954)\n\ncensus_tracts\n\nSimple feature collection with 3445 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963433 ymin: 4825308 xmax: -8314399 ymax: 5201489\nProjected CRS: WGS 84 / Pseudo-Mercator\nFirst 10 features:\n   STATEFP COUNTYFP TRACTCE              GEOIDFQ       GEOID   NAME\n1       42      001  031101 1400000US42001031101 42001031101 311.01\n2       42      013  100400 1400000US42013100400 42013100400   1004\n3       42      013  100500 1400000US42013100500 42013100500   1005\n4       42      013  100800 1400000US42013100800 42013100800   1008\n5       42      013  101900 1400000US42013101900 42013101900   1019\n6       42      011  011200 1400000US42011011200 42011011200    112\n7       42      011  000200 1400000US42011000200 42011000200      2\n8       42      011  011500 1400000US42011011500 42011011500    115\n9       42      011  000600 1400000US42011000600 42011000600      6\n10      42      011  001900 1400000US42011001900 42011001900     19\n              NAMELSAD STUSPS   NAMELSADCO   STATE_NAME LSAD   ALAND AWATER\n1  Census Tract 311.01     PA Adams County Pennsylvania   CT 3043185      0\n2    Census Tract 1004     PA Blair County Pennsylvania   CT  993724      0\n3    Census Tract 1005     PA Blair County Pennsylvania   CT 1130204      0\n4    Census Tract 1008     PA Blair County Pennsylvania   CT  996553      0\n5    Census Tract 1019     PA Blair County Pennsylvania   CT  573726      0\n6     Census Tract 112     PA Berks County Pennsylvania   CT 1539365   9308\n7       Census Tract 2     PA Berks County Pennsylvania   CT 1949529 159015\n8     Census Tract 115     PA Berks County Pennsylvania   CT 1978380  12469\n9       Census Tract 6     PA Berks County Pennsylvania   CT 1460473      0\n10     Census Tract 19     PA Berks County Pennsylvania   CT  182420      0\n                         geometry\n1  MULTIPOLYGON (((-8575060 48...\n2  MULTIPOLYGON (((-8730207 49...\n3  MULTIPOLYGON (((-8729297 49...\n4  MULTIPOLYGON (((-8728636 49...\n5  MULTIPOLYGON (((-8728378 49...\n6  MULTIPOLYGON (((-8455198 49...\n7  MULTIPOLYGON (((-8455907 49...\n8  MULTIPOLYGON (((-8460184 49...\n9  MULTIPOLYGON (((-8450851 49...\n10 MULTIPOLYGON (((-8451174 49...\n\n\nQuestions to answer: - How many hospitals are in your dataset? 223 - How many census tracts? 3445 - What coordinate reference system is each dataset in? I converted them all in WGS 84 / Pseudo-Mercator —\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\n# Get demographic data from ACS\ntract_data &lt;- get_acs(\n  geography = \"tract\",\n  state     = \"42\",\n  variables = c(\n    total_population = \"B01003_001\",\n    median_household_income = \"B19013_001\"\n  ),\n  year = 2023,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\nage_data &lt;- get_acs(\n  geography = \"tract\",\n  state     = \"42\",\n  variables = c(\n    male_65 &lt;- c(\"B01001_020\",\"B01001_021\",\"B01001_022\",\"B01001_023\",\"B01001_024\",\"B01001_025\"),\n    female_65 &lt;- c(\"B01001_044\",\"B01001_045\",\"B01001_046\",\"B01001_047\",\"B01001_048\",\"B01001_049\")\n  ),\n  year = 2023,\n  survey = \"acs5\",\n) %&gt;% \n  group_by (GEOID) %&gt;%\n  summarise(pop_65 = sum(estimate, na.rm = TRUE))\n\nage_data\n\n# A tibble: 3,446 × 2\n   GEOID       pop_65\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 42001030101    577\n 2 42001030103    570\n 3 42001030104    858\n 4 42001030200   1070\n 5 42001030300    782\n 6 42001030400   1231\n 7 42001030500    788\n 8 42001030600   1129\n 9 42001030700   1216\n10 42001030801    762\n# ℹ 3,436 more rows\n\ntract_data &lt;- tract_data %&gt;%\n  left_join(age_data, by = \"GEOID\")\n\ntract_data\n\n# A tibble: 3,446 × 7\n   GEOID       NAME   total_populationE total_populationM median_household_inc…¹\n   &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt;                  &lt;dbl&gt;\n 1 42001030101 Censu…              2666                29                  82716\n 2 42001030103 Censu…              2414               332                 111227\n 3 42001030104 Censu…              3417               333                  66848\n 4 42001030200 Censu…              5379               502                  72431\n 5 42001030300 Censu…              4390               205                  84643\n 6 42001030400 Censu…              5557               245                  97694\n 7 42001030500 Censu…              3761               232                  69583\n 8 42001030600 Censu…              4884                30                  85625\n 9 42001030700 Censu…              6674               356                  80585\n10 42001030801 Censu…              3357               320                  82500\n# ℹ 3,436 more rows\n# ℹ abbreviated name: ¹​median_household_incomeE\n# ℹ 2 more variables: median_household_incomeM &lt;dbl&gt;, pop_65 &lt;dbl&gt;\n\ntract_data %&gt;%\n  summarise(missing_data = sum(is.na(median_household_incomeE)))\n\n# A tibble: 1 × 1\n  missing_data\n         &lt;int&gt;\n1           66\n\ntract_data %&gt;%\n  summarise(median_income = sum(median(median_household_incomeE, na.rm = TRUE)))\n\n# A tibble: 1 × 1\n  median_income\n          &lt;dbl&gt;\n1        72944.\n\n# Join to tract boundaries\n\ncensus_tracts_joined &lt;- census_tracts %&gt;%\n  left_join(tract_data, by = \"GEOID\")\n\nQuestions to answer: - What year of ACS data are you using? 2023 - How many tracts have missing income data? 66 - What is the median income across all PA census tracts? 72943.5\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\n\ncensus_tracts_joined &lt;- census_tracts_joined %&gt;%\n  mutate(\n    percentage_65 = pop_65/total_populationE\n  )\n\nvulnerable_tracts &lt;- census_tracts_joined %&gt;%\n  filter(median_household_incomeE &lt; 48000,\n         percentage_65 &gt; 0.20\n         )\n\nQuestions to answer: - What income threshold did you choose and why? For the income threshold, I chose 48,000 based on the 2025 poverty guidelines by the USCIS. For a family with four people, 150% of the federal poverty level is $48,225. - What elderly population threshold did you choose and why? For the age threshold, I chose the percentage of people over 65 years old is over 20%, which can be seen as an elderly community. - How many tracts meet your vulnerability criteria? 126 - What percentage of PA census tracts are considered vulnerable by your definition? the percentage of vulnerable tracts = 126 / 3445 = 3.66%\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n# Transform to appropriate projected CRS\n\nvulnerable_tracts_proj &lt;- vulnerable_tracts %&gt;%\n  st_transform(5070) # 5070 is for the whole country. For south PA alone, we can use 3365, which is more precise.\n\nhospitals_proj &lt;- hospitals %&gt;%\n  st_transform(5070)\n\n\n# Calculate distance from each tract centroid to nearest hospital\n\ntract_centroids &lt;- st_centroid(vulnerable_tracts_proj)\n\nnearest_hospital_idx &lt;- st_nearest_feature(tract_centroids, hospitals_proj)\n\ndist_to_hospital &lt;- st_distance(\n  tract_centroids,\n  hospitals_proj[nearest_hospital_idx, ],\n  by_element = TRUE\n)\n\n\nvulnerable_tracts_proj &lt;- vulnerable_tracts_proj %&gt;%\n  mutate(\n    dist_to_hospital = as.numeric(dist_to_hospital),\n    dist_to_hospital_mi = dist_to_hospital * 0.000621371\n  )\n\ndistant_data &lt;- vulnerable_tracts_proj %&gt;%\n  summarise(\n    avg_distance = mean(dist_to_hospital_mi, na.rm = TRUE),\n    max_dist = max(dist_to_hospital_mi, na.rm = TRUE),\n  )\n\ndistant_data\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1282205 ymin: 1990587 xmax: 1757922 ymax: 2252774\nProjected CRS: NAD83 / Conus Albers\n  avg_distance max_dist                       geometry\n1     2.803376 18.55538 MULTIPOLYGON (((1347636 204...\n\nnum_over15 &lt;- vulnerable_tracts_proj %&gt;%\n  filter(dist_to_hospital_mi &gt; 15) %&gt;%\n  summarise(\n    n_over15 = n(),\n  )\n\nnum_over15\n\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1461788 ymin: 2192308 xmax: 1464364 ymax: 2193760\nProjected CRS: NAD83 / Conus Albers\n  n_over15                       geometry\n1        1 MULTIPOLYGON (((1461853 219...\n\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles - Explain why you chose your projection\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? 2.8 miles - What is the maximum distance? 18.6 miles - How many vulnerable tracts are more than 15 miles from the nearest hospital? 1\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\n\nundeserved &lt;- vulnerable_tracts_proj %&gt;%\n  filter(dist_to_hospital_mi &gt; 15)\n\nundeserved \n\nSimple feature collection with 1 feature and 22 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1461788 ymin: 2192308 xmax: 1464364 ymax: 2193760\nProjected CRS: NAD83 / Conus Albers\n  STATEFP COUNTYFP TRACTCE              GEOIDFQ       GEOID NAME.x\n1      42      023  960100 1400000US42023960100 42023960100   9601\n           NAMELSAD STUSPS     NAMELSADCO   STATE_NAME LSAD   ALAND AWATER\n1 Census Tract 9601     PA Cameron County Pennsylvania   CT 1838256 107252\n                                           NAME.y total_populationE\n1 Census Tract 9601; Cameron County; Pennsylvania              2005\n  total_populationM median_household_incomeE median_household_incomeM pop_65\n1               134                    36167                     6116    504\n                        geometry percentage_65 dist_to_hospital\n1 MULTIPOLYGON (((1461853 219...     0.2513716            29862\n  dist_to_hospital_mi\n1            18.55538\n\n\nQuestions to answer: - How many tracts are underserved? only 1! - What percentage of vulnerable tracts are underserved? 1/126 = 0.79% - Does this surprise you? Why or why not? No, I looked at the distance data of the vulnerable tracts, most of them (92.9%) are under 10 miles (16000m). I think 15 miles is a very big number, so we just have one tract fulfill all the requirements.\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n# Spatial join tracts to counties\nvulnerable_tracts_proj &lt;- st_transform(vulnerable_tracts_proj, st_crs(pa_counties))\nundeserved  &lt;- st_transform(undeserved , st_crs(pa_counties))\n\ncounties_num = census_tracts %&gt;%\n  st_centroid() %&gt;%\n  st_join(pa_counties%&gt;% select(COUNTY_NAM)) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_tract =  n(),\n  )%&gt;%\n  arrange(n_tract)\n\ncounties_vuln &lt;- vulnerable_tracts_proj %&gt;%\n  st_centroid() %&gt;%\n  st_join(pa_counties%&gt;% select(COUNTY_NAM)) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_vuln =  n(),\n    avg_dist_vuln_mi = mean(dist_to_hospital_mi, na.rm = TRUE),\n    total_vuln_pop = sum(pop_65, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(total_vuln_pop))\n\ncounties_unders &lt;- undeserved %&gt;%\n  st_centroid() %&gt;%\n  st_join(pa_counties%&gt;% select(COUNTY_NAM)) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_unders =  n(),\n  ) %&gt;%\n  arrange(desc(n_unders))\n\n\n\n# Aggregate statistics by county\n\ncounties_stat = counties_num %&gt;%\n  left_join(counties_vuln, by = \"COUNTY_NAM\") %&gt;%\n  left_join(counties_unders, by = \"COUNTY_NAM\") %&gt;%\n  mutate(\n    percentage_unders = n_unders / n_vuln,\n    percentage_vuln = n_vuln / n_tract\n  )%&gt;%\n  arrange(desc(percentage_vuln))\n\nRequired county-level statistics: - Number of vulnerable tracts - Number of underserved tracts\n- Percentage of vulnerable tracts that are underserved - Average distance to nearest hospital for vulnerable tracts - Total vulnerable population\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? CAMERON is the highest, with 100% in the number. Since I only have 1 underserved tract, I would show top 5 counties with the highest percentage of vulnerable tracts: (1) CAMERON 50.0% (2) FAYETTE 19.4% (3) CAMBRIA 19.0% (4) JEFFERSON 15.4% (5) WARREN 15.4% - Which counties have the most vulnerable people living far from hospitals? PHILADELPHIA, with the number of 18022 - Are there any patterns in where underserved counties are located? Since I only have 1 underserved tract, I would observe patterns of vulnerable counties. (1) Most of them are from rural areas with less tracts number, so they have higher percentage of vulnerable tracts (2) They have large numbers of vulnerable population, which means lots of low-income and old people living there. Most of them have number over 1000.\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\n# metrics for the priority: vulnerable index = normalized_number_of_vulnerable_tracts*0.3 + normalized_average_distant _hospital*0.5 + normalized_total_population*0.2\n\nscale_minmax &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  if (!is.finite(diff(rng)) || diff(rng) == 0) return(rep(0.5, length(x)))\n  (x - rng[1]) / diff(rng)\n}\n\ncounties_stat_index &lt;- counties_stat %&gt;%\n  mutate(\n    n_vuln_norm = scale_minmax(n_vuln),\n    avg_dist_vuln_mi_norm = scale_minmax(avg_dist_vuln_mi),\n    total_vuln_pop_norm = scale_minmax(total_vuln_pop),\n    percentage_vuln_norm = scale_minmax(percentage_vuln)\n  ) %&gt;%\n   mutate(\n     vuln_index =\n       0.3  * n_vuln_norm +\n       0.5 * avg_dist_vuln_mi_norm +\n       0.2 * total_vuln_pop_norm\n   ) %&gt;%\n  arrange(desc(vuln_index))\n\ncounties_stat_index\n\n# A tibble: 68 × 13\n   COUNTY_NAM     n_tract n_vuln avg_dist_vuln_mi total_vuln_pop n_unders\n   &lt;chr&gt;            &lt;int&gt;  &lt;int&gt;            &lt;dbl&gt;          &lt;dbl&gt;    &lt;int&gt;\n 1 ALLEGHENY          394     28            2.39           15213       NA\n 2 CAMERON              2      1           18.6              504        1\n 3 PHILADELPHIA       407     19            0.887          18022       NA\n 4 HUNTINGDON          13      1           14.2              730       NA\n 5 NORTHUMBERLAND      25      3           10.8             1603       NA\n 6 CAMBRIA             42      8            4.15            4588       NA\n 7 VENANGO             17      1            8.40             363       NA\n 8 FAYETTE             36      7            2.55            5780       NA\n 9 ERIE                72      7            2.35            5923       NA\n10 WARREN              13      2            6.19            1257       NA\n# ℹ 58 more rows\n# ℹ 7 more variables: percentage_unders &lt;dbl&gt;, percentage_vuln &lt;dbl&gt;,\n#   n_vuln_norm &lt;dbl&gt;, avg_dist_vuln_mi_norm &lt;dbl&gt;, total_vuln_pop_norm &lt;dbl&gt;,\n#   percentage_vuln_norm &lt;dbl&gt;, vuln_index &lt;dbl&gt;\n\n#make table\n\ncounties_table &lt;- counties_stat_index %&gt;%\n  select(\n    county_name = COUNTY_NAM,\n    vulnerable_index = vuln_index,\n    vulnerable_tract_number = n_vuln,\n    average_dist_to_hospital = avg_dist_vuln_mi,\n    total_vulnerable_pop = total_vuln_pop\n  ) %&gt;%\n  mutate(\n    vulnerable_index = round(vulnerable_index, 3),\n    average_dist_to_hospital = round(average_dist_to_hospital, 2),\n    total_vulnerable_pop = comma(total_vulnerable_pop)\n  )%&gt;%\n  arrange(desc(vulnerable_index))\n\nkable(\n  counties_table,\n  caption = \"PA County-level Vulnerability Summary\",\n  align = \"lcccc\"\n)\n\n\nPA County-level Vulnerability Summary\n\n\n\n\n\n\n\n\n\ncounty_name\nvulnerable_index\nvulnerable_tract_number\naverage_dist_to_hospital\ntotal_vulnerable_pop\n\n\n\n\nALLEGHENY\n0.519\n28\n2.39\n15,213\n\n\nCAMERON\n0.502\n1\n18.56\n504\n\n\nPHILADELPHIA\n0.409\n19\n0.89\n18,022\n\n\nHUNTINGDON\n0.383\n1\n14.18\n730\n\n\nNORTHUMBERLAND\n0.321\n3\n10.81\n1,603\n\n\nCAMBRIA\n0.226\n8\n4.15\n4,588\n\n\nVENANGO\n0.218\n1\n8.40\n363\n\n\nFAYETTE\n0.183\n7\n2.55\n5,780\n\n\nERIE\n0.179\n7\n2.35\n5,923\n\n\nWARREN\n0.178\n2\n6.19\n1,257\n\n\nLANCASTER\n0.175\n1\n6.60\n1,034\n\n\nLUZERNE\n0.153\n6\n2.51\n4,195\n\n\nWESTMORELAND\n0.150\n6\n2.86\n3,057\n\n\nLAWRENCE\n0.149\n3\n4.52\n1,793\n\n\nNORTHAMPTON\n0.130\n1\n5.03\n850\n\n\nCLINTON\n0.127\n1\n4.89\n913\n\n\nDELAWARE\n0.120\n6\n1.32\n4,179\n\n\nLACKAWANNA\n0.104\n5\n1.67\n2,871\n\n\nLEHIGH\n0.092\n1\n3.42\n1,417\n\n\nBEAVER\n0.076\n2\n2.53\n1,291\n\n\nBLAIR\n0.076\n2\n2.18\n2,079\n\n\nINDIANA\n0.070\n1\n2.63\n1,447\n\n\nWASHINGTON\n0.068\n2\n2.39\n915\n\n\nLYCOMING\n0.066\n1\n2.90\n470\n\n\nMERCER\n0.063\n3\n1.42\n1,866\n\n\nJEFFERSON\n0.057\n2\n1.72\n1,566\n\n\nMIFFLIN\n0.038\n1\n1.87\n516\n\n\nYORK\n0.028\n1\n1.55\n425\n\n\nCRAWFORD\n0.015\n1\n0.93\n787\n\n\nWAYNE\n0.014\n1\n0.83\n951\n\n\nSOMERSET\n0.008\n1\n0.82\n399\n\n\nBUTLER\n0.000\n1\n0.56\n400\n\n\nNA\nNA\nNA\nNA\nNA\n\n\nFOREST\nNA\nNA\nNA\nNA\n\n\nFULTON\nNA\nNA\nNA\nNA\n\n\nMONTOUR\nNA\nNA\nNA\nNA\n\n\nSULLIVAN\nNA\nNA\nNA\nNA\n\n\nJUNIATA\nNA\nNA\nNA\nNA\n\n\nPOTTER\nNA\nNA\nNA\nNA\n\n\nWYOMING\nNA\nNA\nNA\nNA\n\n\nSNYDER\nNA\nNA\nNA\nNA\n\n\nELK\nNA\nNA\nNA\nNA\n\n\nGREENE\nNA\nNA\nNA\nNA\n\n\nPERRY\nNA\nNA\nNA\nNA\n\n\nTIOGA\nNA\nNA\nNA\nNA\n\n\nUNION\nNA\nNA\nNA\nNA\n\n\nBEDFORD\nNA\nNA\nNA\nNA\n\n\nMCKEAN\nNA\nNA\nNA\nNA\n\n\nSUSQUEHANNA\nNA\nNA\nNA\nNA\n\n\nCLARION\nNA\nNA\nNA\nNA\n\n\nBRADFORD\nNA\nNA\nNA\nNA\n\n\nCOLUMBIA\nNA\nNA\nNA\nNA\n\n\nCARBON\nNA\nNA\nNA\nNA\n\n\nARMSTRONG\nNA\nNA\nNA\nNA\n\n\nCLEARFIELD\nNA\nNA\nNA\nNA\n\n\nPIKE\nNA\nNA\nNA\nNA\n\n\nADAMS\nNA\nNA\nNA\nNA\n\n\nFRANKLIN\nNA\nNA\nNA\nNA\n\n\nLEBANON\nNA\nNA\nNA\nNA\n\n\nCENTRE\nNA\nNA\nNA\nNA\n\n\nSCHUYLKILL\nNA\nNA\nNA\nNA\n\n\nMONROE\nNA\nNA\nNA\nNA\n\n\nCUMBERLAND\nNA\nNA\nNA\nNA\n\n\nDAUPHIN\nNA\nNA\nNA\nNA\n\n\nBERKS\nNA\nNA\nNA\nNA\n\n\nCHESTER\nNA\nNA\nNA\nNA\n\n\nBUCKS\nNA\nNA\nNA\nNA\n\n\nMONTGOMERY\nNA\nNA\nNA\nNA\n\n\n\n\n\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n# Create county-level access map\n# I use percentage of vulnerable tract here\n\nlibrary(ggplot2)\n\ncounties_stat_sf &lt;- pa_counties %&gt;%\n  select(COUNTY_NAM, geometry) %&gt;%\n  left_join(counties_stat, by = \"COUNTY_NAM\")\n\np1 &lt;- ggplot() +\n  # county base map\n  geom_sf(\n    data = counties_stat_sf,\n    aes(fill = percentage_vuln),\n    color = \"white\", size = 0.2\n  ) +\n  # hospitals\n  geom_sf(\n    data = hospitals_proj,\n    shape = 21, fill = \"red\",\n    size = 1.8, alpha = 0.9\n  ) +\n  # legend\n  scale_fill_viridis_c(\n    name = \"% vulnerable tracts\",\n    labels = label_percent(accuracy = 1),\n    limits = c(0, 1),\n    option = \"C\",      \n    direction = -1\n  ) +\n  coord_sf() +\n  theme_void(base_size = 12) +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text  = element_text(size = 9),\n    plot.title   = element_text(face = \"bold\", size = 16),\n    plot.subtitle= element_text(size = 12)\n  ) +\n  labs(\n    title    = \"Healthcare Access Challenges by County\",\n    subtitle = \"share of vulnerable census tracts\",\n    caption  = \"Source: analysis based on income and percentage of people over 65 years old.\"\n  ) +\n  guides(fill = guide_colorbar(\n    barheight = unit(70, \"pt\"),\n    barwidth  = unit(8,  \"pt\"),\n    ticks.colour = \"black\"\n  ))\n\np1\n\n\n\n\n\n\n\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Create detailed tract-level map\n\np2 &lt;- ggplot() +\n  # tract base map\n  geom_sf(\n    data = census_tracts,\n    color = \"grey\", size = 0.1\n  ) +\n  # county base map\n  geom_sf(\n    data = pa_counties,\n    color = \"lightblue\", size = 0.5,fill = NA,\n  ) +\n  # hospitals\n  geom_sf(\n    data = hospitals_proj,\n    shape = 16, color = \"red\",\n    size = 1, alpha = 0.9\n  ) +\n  # underserved community\n  geom_sf(\n    data = undeserved,\n    color = \"blue\"\n  )+\n  coord_sf() +\n  theme_void(base_size = 12) +\n  theme(\n    plot.title   = element_text(face = \"bold\", size = 16),\n    plot.subtitle= element_text(size = 12)\n  ) +\n  labs(\n    title    = \"Underserved Communities\",\n    subtitle = \"Blue tracts represent underserved communities; red points show hospital locations.\"\n  ) +\n  guides(fill = guide_colorbar(\n    barheight = unit(70, \"pt\"),\n    barwidth  = unit(8,  \"pt\"),\n    ticks.colour = \"black\"\n  ))\n\np2\n\n\n\n\n\n\n\n#only one very small blue tract in the center of the map\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\n\n# Histogram or density plot of distances\nggplot(vulnerable_tracts_proj) +\n  aes(x = dist_to_hospital_mi) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of distance to the nearest hospital\",\n    x = \"distance to the nearest hospital\",\n    y = \"Number of vulnerable tracts\",\n    caption  = \"Most of tracts have distance between 0 and 5.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Box plot comparing distances across regions\nggplot(vulnerable_tracts_proj) +\n  aes(x = NAMELSADCO, y = dist_to_hospital_mi, fill = NAMELSADCO) +\n  geom_boxplot() +\n  labs(\n    title = \"Distances by County Category\",\n    x = \"County\",\n    y = \"Distance\",\n    caption  = \"While most counties show little variation, four counties have huge internal differences\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")+\n  theme(\n  axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 7)\n)\n\n\n\n\n\n\n\n# Bar chart of underserved tracts by county\n#only 1 underserved tracts in my study, it is Census Tract 9601 in the Cameron County\n\n\n# Scatter plot of distance vs. vulnerable population size\nggplot(vulnerable_tracts_proj) +\n  aes(x = dist_to_hospital_mi, y = total_populationE) +\n  geom_point(color = \"steelblue\", alpha = 0.6, size = 2) +\n  geom_smooth(\n    method = \"lm\",              \n    se = TRUE,                  \n    color = \"darkred\",          \n    fill = \"pink\",              \n    linewidth = 1\n  ) +\n  labs(\n    title = \"distance vs. vulnerable population size\",\n    x = \"Distance to the nearest hospital\",\n    y = \"Vulnerable population size\",\n    caption  = str_wrap(\"More vulnerable population, little distance to hospital. Since hospitals are designed to located near population clusters, and we didn't limit this effect in this study \", width = 90)\n  ) +\n  theme_minimal() +\n  scale_x_continuous() +\n  scale_y_continuous(labels = comma)\n\n\n\n\n\n\n\n\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs. vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\n\nEducation & Youth Services\nOption A: Educational Desert Analysis - Data: Schools, Libraries, Recreation Centers, Census tracts (child population) - Question: “Which neighborhoods lack adequate educational infrastructure for children?” - Operations: Buffer schools/libraries (0.5 mile walking distance), identify coverage gaps, overlay with child population density - Policy relevance: School district planning, library placement, after-school program siting\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nEnvironmental Justice\nOption C: Green Space Equity - Data: Parks, Street Trees, Census tracts (race/income demographics) - Question: “Do low-income and minority neighborhoods have equitable access to green space?” - Operations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics - Policy relevance: Climate resilience, environmental justice, urban forestry investment —\n\n\nPublic Safety & Justice\nOption D: Crime & Community Resources - Data: Crime Incidents, Recreation Centers, Libraries, Street Lights - Question: “Are high-crime areas underserved by community resources?” - Operations: Aggregate crime counts to census tracts or neighborhoods, count community resources per area, spatial correlation analysis - Policy relevance: Community investment, violence prevention strategies —\n\n\nInfrastructure & Services\nOption E: Polling Place Accessibility - Data: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates) - Question: “Are polling places accessible for elderly and disabled voters?” - Operations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access - Policy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\nHealth & Wellness\nOption F: Recreation & Population Health - Data: Recreation Centers, Playgrounds, Parks, Census tracts (demographics) - Question: “Is lack of recreation access associated with vulnerable populations?” - Operations: Calculate recreation facilities per capita by neighborhood, buffer facilities for walking access, overlay with demographic indicators - Policy relevance: Public health investment, recreation programming, obesity prevention\n\n\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: “Are population-dense areas adequately covered by emergency services?” - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nArts & Culture\nOption H: Cultural Asset Distribution - Data: Public Art, Museums, Historic sites/markers, Neighborhoods - Question: “Do all neighborhoods have equitable access to cultural amenities?” - Operations: Count cultural assets per neighborhood, normalize by population, compare distribution across demographic groups - Policy relevance: Cultural equity, tourism, quality of life, neighborhood identity\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you’re feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\n\nlibrary(sf)\n\nschools &lt;- st_read(\"Data/Schools/Schools.shp\")\n\nReading layer `Schools' from data source \n  `/Users/jinhengcen/Documents/portfolio-setup-CenJinHeng/assignments/assignment_2/data/Schools/Schools.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 495 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8378628 ymin: 4852555 xmax: -8345686 ymax: 4884813\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ncrime &lt;- st_read(\"Data/Crime/Crime.shp\")\n\nReading layer `Crime' from data source \n  `/Users/jinhengcen/Documents/portfolio-setup-CenJinHeng/assignments/assignment_2/data/Crime/Crime.shp' \n  using driver `ESRI Shapefile'\nreplacing null geometries with empty geometries\nSimple feature collection with 160388 features and 13 fields (with 6744 geometries empty)\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.50237 ymin: 39.87688 xmax: -74.95784 ymax: 42.22434\nGeodetic CRS:  WGS 84\n\nbike_network &lt;- st_read(\"Data/Bike_Network/Bike_Network.shp\")\n\nReading layer `Bike_Network' from data source \n  `/Users/jinhengcen/Documents/portfolio-setup-CenJinHeng/assignments/assignment_2/data/Bike_Network/Bike_Network.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5225 features and 8 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -8378937 ymin: 4847835 xmax: -8345146 ymax: 4883978\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nschools &lt;- st_transform(schools, 2272)\ncrime &lt;- st_transform(crime, 2272)\nbike_network &lt;- st_transform(bike_network, 2272)\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n## school type\nschools %&gt;%\n  st_drop_geometry() %&gt;%\n  count(type_speci, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(type_speci, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  coord_flip() +\n  labs(\n    title = \"Distribution of school types\",\n    x = \"School type\",\n    y = \"Number\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n# Crime Hour\ncrime %&gt;%\n  st_drop_geometry() %&gt;%\n  count(hour) %&gt;%\n  ggplot(aes(x = hour, y = n)) +\n  geom_col(fill = \"firebrick\") +\n  labs(\n    title = \"Distribution of crime hour\",\n    x = \"Hour\",\n    y = \"Number of incidents\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n#crime types\ncrime %&gt;%\n  st_drop_geometry() %&gt;%\n  count(text_gener, sort = TRUE) %&gt;%\n  slice_max(n, n = 10) %&gt;%  \n  ggplot(aes(x = reorder(text_gener, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Crime Types\",\n    x = \"Types\",\n    y = \"Number of incidents\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n# Bike lane\nbike_length_summary &lt;- bike_network %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(TYPE) %&gt;%\n  summarise(total_length = sum(Shape__Len, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_length))\n\nggplot(bike_length_summary, aes(x = reorder(TYPE, total_length), y = total_length)) +\n  geom_bar(stat = \"identity\", fill = \"forestgreen\") +\n  coord_flip() +\n  labs(\n    title = \"Distribution of Bike Lane Type\",\n    x = \"Type\",\n    y = \"Total Length\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nQuestions to answer: - What dataset did you choose and why? Three data sets are chosen: - Schools data in shp format - Crime incidents data points in shp format - Bike_Network in shp format Since I am going to calculate some metrics like the numbers of crime incidents and the length of bike lanes within school buffers, these data are needed. - What is the data source and date? They are all from OpenDataPhilly. All of them are up_to_date. The crime incidents data is from 2024, because we are in the October of the 2025, so we don’t have data for the whole 2025. Using 2024 can avoid some time variation. - How many features does it contain? All of them have geometry, which means we can run them in spatial analysis. For the school data set, we got names, types, grade levels, phone numbers, street names, and zip codes. For the crime data, we have precise time, crime types, location details. For the bike network data, we knew street names, types, lengths and oneway.\n- What CRS is it in? Did you need to transform it? EPSG:2272, which is used in south Pennsylvania.\n\n\nPose a research question\n\nWrite a clear research statement that your analysis will answer.\nExamples: - “Do vulnerable tracts have adequate public transit access to hospitals?” - “Are EMS stations appropriately located near vulnerable populations?” - “Do areas with low vehicle access have worse hospital access?”\nMy question is “Are school zones safe for walking/biking, or are they crime hotspots?”\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+): - Buffers - Spatial joins - Spatial filtering with predicates - Distance calculations - Intersections or unions - Point-in-polygon aggregation\nYour Task:\n\n# Your spatial analysis\n\n# I have loaded data sets and make simple statistical analysis for each of them in the previous chunk\n\n# Create buffer for schools\n\nschools &lt;- schools %&gt;% mutate(school_id = row_number())\nzones &lt;- st_buffer(schools, dist = 1000)\n\n# Calculate the crime number\n\ncrime_by_zone &lt;- crime %&gt;%\n  st_join(zones %&gt;% select(school_id), join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  count(school_id, name = \"crime_count\")\n\n\n# Calculate the bike lane length\nbike_in_zone &lt;- st_intersection(bike_network, zones %&gt;% select(school_id))\n\nbike_len_by_zone &lt;- bike_in_zone %&gt;%\n  mutate(len_ft = as.numeric(st_length(.))) %&gt;% \n  st_drop_geometry() %&gt;%\n  group_by(school_id) %&gt;%\n  summarise(bike_len_ft = sum(len_ft, na.rm = TRUE), .groups = \"drop\")\n\n# Combine the result\nzone_stats &lt;- zones %&gt;%\n  left_join(crime_by_zone,    by = \"school_id\") %&gt;%\n  left_join(bike_len_by_zone, by = \"school_id\")\n\n# Calculate average number of the crime incidents and bike lane length\nzone_summary &lt;- zone_stats %&gt;%\n  st_drop_geometry() %&gt;%\n  summarise(\n    mean_crime = mean(crime_count, na.rm = TRUE),\n    mean_bike_len = mean(bike_len_ft, na.rm = TRUE)\n  )\n\ncity_summary &lt;- tibble(\n  mean_crime = nrow(crime) / nrow(zones),\n  mean_bike_len = sum(st_length(bike_network)) / nrow(zones)\n)\n\n# Combine results\ncomparison &lt;- rbind(\n  data.frame(Category = \"School Zones\", \n             mean_crime = zone_summary$mean_crime, \n             mean_bike_len = zone_summary$mean_bike_len),\n  data.frame(Category = \"Citywide Average\",\n             mean_crime = city_summary$mean_crime,\n             mean_bike_len = city_summary$mean_bike_len)\n)\n\n# Draw diagram\nggplot(comparison, aes(x = Category, y = mean_crime, fill = Category)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  labs(\n    title = \"Comperison of Average Crime Incidents: School Zones VS City\",\n    y = \"Averge Crime Incidents\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nggplot(comparison, aes(x = Category, y = mean_bike_len, fill = Category)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  labs(\n    title = \"Comperison of Bike Lane Length: School Zones VS City\",\n    y = \"Average Bike Lane Length\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n# Mapping\n\n# Get tracts of Philly\ntracts_phl &lt;- get_acs(\n  geography = \"tract\",\n  state     = \"PA\",\n  county    = \"Philadelphia\",\n  variables = \"B01003_001\",\n  survey    = \"acs5\",\n  year      = 2022,\n  geometry  = TRUE,\n  cache_table = TRUE\n) %&gt;%\n  select(GEOID, NAME, pop = estimate, geometry) %&gt;%\n  st_transform(2272)    \n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# point_on_surface\nzone_points &lt;- zone_stats |&gt;\n  mutate(geometry = st_point_on_surface(geometry),\n         bike_len_mile = bike_len_ft / 5280) |&gt;\n  st_as_sf()\n\n# Crime point map\nggplot() +\n  geom_sf(data = tracts_phl, fill = NA, color = \"grey\", linewidth = 0.2) +\n  geom_sf(data = zone_points, aes(size = crime_count), color = \"firebrick\", alpha = 0.6) +\n  scale_size_continuous(name = \"Crime count\", range = c(0, 4)) +\n  labs(title = \"School Zones: Crime Counts (Point size = count)\", x = NULL, y = NULL) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n# Bike lane point map\nggplot() +\n  geom_sf(data = tracts_phl, fill = NA, color = \"grey\", linewidth = 0.2) +\n  geom_sf(data = zone_points, aes(size = bike_len_mile), color = \"forestgreen\", alpha = 0.6) +\n  scale_size_continuous(name = \"Bike lane (miles)\", range = c(0.1, 2)) +\n  labs(title = \"School Zones: Bike Lane Length (Point size = miles)\", x = NULL, y = NULL) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation:\nSchool zones have fewer crime incidents and bike lane length compared to city average. More pedestrian-friendly projects should be applied to school areas. For the spatial difference, higher concentrations of crimes are observed around schools in central and southern Philadelphia. School zones with longer bike lane coverage are mostly located in the city’s core and western areas. Planning efforts should focus on improving safety in school zones with higher crime risks, especially in central and southern Philadelphia. Expanding connected and protected bike lanes around schools can promote safer and more accessible environments for students."
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTake a few moments to clean up your markdown document and then write a line or two or three about how you may have incorporated feedback that you recieved after your first assignment."
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#submission-requirements",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "labs/lab_0.html",
    "href": "labs/lab_0.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab_0.html#data-structure-exploration",
    "href": "labs/lab_0.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford…\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"…\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0…\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"…\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, …\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866…\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30…\n\n# Check the column names\nnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer:\n\nHow many rows and columns does the dataset have?\nWhat types of variables do you see (numeric, character, etc.)?\nAre there any column names that might cause problems? Why?\n\nYour answers:\n\nRows: 50,000\nColumns: 7\nVariable types: character, numeric\nProblematic names: Columns like “Engine size”, “Fuel type”, and “Year of manufacture” are problematic, cuz they both have spaces inside their names."
  },
  {
    "objectID": "labs/lab_0.html#tibble-vs-data-frame",
    "href": "labs/lab_0.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\ncar_data %&gt;% head(10)\n\n# A tibble: 10 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestion: What differences do you notice in how they print?\nYour answer: data frames display all the rows in the rendering window, while tibbles show the first few rows."
  },
  {
    "objectID": "labs/lab_0.html#selecting-columns",
    "href": "labs/lab_0.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\nlibrary(dplyr)\n\n# Select just Model and Mileage columns\ncar_data %&gt;% select (Model, Mileage)\n\n# A tibble: 50,000 × 2\n   Model      Mileage\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Fiesta      127300\n 2 718 Cayman   57850\n 3 Mondeo       39190\n 4 RAV4        210814\n 5 Polo        127869\n 6 Focus        33603\n 7 Mondeo       86686\n 8 Prius        30663\n 9 Polo         73470\n10 Focus       262514\n# ℹ 49,990 more rows\n\n# Select Manufacturer, Price, and Fuel type\ncar_data %&gt;% select (Manufacturer, Price, `Fuel type`)\n\n# A tibble: 50,000 × 3\n   Manufacturer Price `Fuel type`\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      \n 1 Ford          3074 Petrol     \n 2 Porsche      49704 Petrol     \n 3 Ford         24072 Diesel     \n 4 Toyota        1705 Hybrid     \n 5 VW            4101 Petrol     \n 6 Ford         29204 Petrol     \n 7 Ford         14350 Diesel     \n 8 Toyota       30297 Hybrid     \n 9 VW            9977 Petrol     \n10 Ford          1049 Diesel     \n# ℹ 49,990 more rows\n\n# Challenge: Select all columns EXCEPT Engine Size\ncar_data %&gt;% select (-`Engine size`)\n\n# A tibble: 50,000 × 6\n   Manufacturer Model      `Fuel type` `Year of manufacture` Mileage Price\n   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta     Petrol                       2002  127300  3074\n 2 Porsche      718 Cayman Petrol                       2016   57850 49704\n 3 Ford         Mondeo     Diesel                       2014   39190 24072\n 4 Toyota       RAV4       Hybrid                       1988  210814  1705\n 5 VW           Polo       Petrol                       2006  127869  4101\n 6 Ford         Focus      Petrol                       2018   33603 29204\n 7 Ford         Mondeo     Diesel                       2010   86686 14350\n 8 Toyota       Prius      Hybrid                       2015   30663 30297\n 9 VW           Polo       Petrol                       2012   73470  9977\n10 Ford         Focus      Diesel                       1992  262514  1049\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab_0.html#renaming-columns",
    "href": "labs/lab_0.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\ncar_data &lt;- car_data %&gt;% rename(year = `Year of manufacture`)\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"Engine size\"  \"Fuel type\"    \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: There are spaces in Year of manufacture."
  },
  {
    "objectID": "labs/lab_0.html#calculate-car-age",
    "href": "labs/lab_0.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\ncar_data &lt;- car_data %&gt;% mutate(age = 2025 - year)\n\n# Create a mileage_per_year column  \ncar_data &lt;- car_data %&gt;% mutate(mileage_per_year = Mileage/age)\n\n# Look at your new columns\ncar_data %&gt;% select(Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 × 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab_0.html#categorize-cars",
    "href": "labs/lab_0.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is mid-range (use case_when)\ncar_data &lt;- car_data %&gt;% mutate(\n  price_category = case_when(\n    Price &lt; 15000 ~ \"budget\",\n    Price &gt; 15000 & Price &lt; 30000 ~ \"midrange\",\n    Price &gt; 30000 ~ \"mid-range\"\n  )\n)\n\n# Check your categories select the new column and show it\ncar_data %&gt;% select(price_category)\n\n# A tibble: 50,000 × 1\n   price_category\n   &lt;chr&gt;         \n 1 budget        \n 2 mid-range     \n 3 midrange      \n 4 budget        \n 5 budget        \n 6 midrange      \n 7 budget        \n 8 mid-range     \n 9 budget        \n10 budget        \n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab_0.html#basic-filtering",
    "href": "labs/lab_0.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\ncar_data %&gt;% filter( Manufacturer == \"Toyota\")\n\n# A tibble: 12,554 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 4 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 5 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n 6 Toyota       Yaris           1.2 Petrol       1992  245990   720    33\n 7 Toyota       RAV4            2   Hybrid       2018   28381 52671     7\n 8 Toyota       Prius           1   Hybrid       2003  115291  6512    22\n 9 Toyota       Prius           1   Hybrid       1990  238571   961    35\n10 Toyota       Prius           1.8 Hybrid       2017   31958 38961     8\n# ℹ 12,544 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with mileage less than 30,000\ncar_data %&gt;% filter( Mileage &lt; 30000)\n\n# A tibble: 5,402 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 Ford         Mondeo               1.6 Diesel       2015   21834 28435    10\n 9 VW           Passat               1.6 Diesel       2018   22122 36634     7\n10 VW           Passat               1.4 Diesel       2020   21413 39310     5\n# ℹ 5,392 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find luxury cars (from price category) with low mileage\ncar_data %&gt;% filter( price_category == \"mid-range\" & Mileage &lt; 30000)\n\n# A tibble: 3,256 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 VW           Passat               1.6 Diesel       2018   22122 36634     7\n 9 VW           Passat               1.4 Diesel       2020   21413 39310     5\n10 Toyota       RAV4                 2.4 Petrol       2021    6829 66031     4\n# ℹ 3,246 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab_0.html#multiple-conditions",
    "href": "labs/lab_0.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Toyota OR VW\ncar_data %&gt;% filter ( Manufacturer == \"Toyota\"|Manufacturer == \"VW\")\n\n# A tibble: 27,467 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 VW           Polo            1   Petrol       2006  127869  4101    19\n 3 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 4 VW           Polo            1.2 Petrol       2012   73470  9977    13\n 5 VW           Golf            2   Diesel       2014   83047 17173    11\n 6 VW           Golf            1.2 Diesel       2007   92697  7792    18\n 7 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 8 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 9 VW           Golf            1.6 Diesel       1989  222390   933    36\n10 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n# ℹ 27,457 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with price between $20,000 and $35,000\ncar_data %&gt;% filter (Price &gt; 20000 & Price &lt; 35000)\n\n# A tibble: 7,301 × 10\n   Manufacturer Model  `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Mondeo           1.6 Diesel       2014   39190 24072    11\n 2 Ford         Focus            1.4 Petrol       2018   33603 29204     7\n 3 Toyota       Prius            1.4 Hybrid       2015   30663 30297    10\n 4 Toyota       Prius            1.4 Hybrid       2016   43893 29946     9\n 5 Toyota       Prius            1.4 Hybrid       2016   43130 30085     9\n 6 VW           Passat           1.6 Petrol       2016   64344 23641     9\n 7 Ford         Mondeo           1.6 Diesel       2015   21834 28435    10\n 8 BMW          M5               4.4 Petrol       2008  109941 31711    17\n 9 BMW          Z4               2.2 Petrol       2014   61332 26084    11\n10 Porsche      911              3.5 Petrol       2003  107705 24378    22\n# ℹ 7,291 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find diesel cars less than 10 years old\ncar_data %&gt;% filter (age &lt; 10 & `Fuel type` == \"diesel\")\n\n# A tibble: 0 × 10\n# ℹ 10 variables: Manufacturer &lt;chr&gt;, Model &lt;chr&gt;, Engine size &lt;dbl&gt;,\n#   Fuel type &lt;chr&gt;, year &lt;dbl&gt;, Mileage &lt;dbl&gt;, Price &lt;dbl&gt;, age &lt;dbl&gt;,\n#   mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 0"
  },
  {
    "objectID": "labs/lab_0.html#basic-summaries",
    "href": "labs/lab_0.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\navg_mileage_by_fuel &lt;- car_data %&gt;%\n  group_by(`Fuel type`) %&gt;%\n  summarize(avg_mileage = mean(Mileage, na.rm = TRUE))\n\navg_mileage_by_fuel\n\n# A tibble: 3 × 2\n  `Fuel type` avg_mileage\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Diesel          112667.\n2 Hybrid          111622.\n3 Petrol          112795.\n\n# Count cars by manufacturer\ncount_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarise(num_cars = n())\n\ncount_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer num_cars\n  &lt;chr&gt;           &lt;int&gt;\n1 BMW              4965\n2 Ford            14959\n3 Porsche          2609\n4 Toyota          12554\n5 VW              14913"
  },
  {
    "objectID": "labs/lab_0.html#categorical-summaries",
    "href": "labs/lab_0.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories\nFrequency_price_categories &lt;- car_data %&gt;%\n  group_by(price_category) %&gt;%\n  summarize(num_car = n())\n\nFrequency_price_categories\n\n# A tibble: 4 × 2\n  price_category num_car\n  &lt;chr&gt;            &lt;int&gt;\n1 budget           34040\n2 mid-range         6178\n3 midrange          9779\n4 &lt;NA&gt;                 3"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html",
    "href": "assignments/assignment_1/assignment_1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the New Jersey Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#scenario",
    "href": "assignments/assignment_1/assignment_1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the New Jersey Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#learning-objectives",
    "href": "assignments/assignment_1/assignment_1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#submission-instructions",
    "href": "assignments/assignment_1/assignment_1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#data-retrieval",
    "href": "assignments/assignment_1/assignment_1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",   # Median household income\n    total_population = \"B01003_001\"       # Total pop\n  ),\n  state = my_state,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\ncounty_data &lt;- county_data %&gt;%\n  mutate(county_name = NAME %&gt;% \n           str_remove(\" County, New Jersey\")\n         )\n\n# Display the first few rows\nhead(county_data,10)\n\n# A tibble: 10 × 7\n   GEOID NAME  median_incomeE median_incomeM total_populationE total_populationM\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n 1 34001 Atla…          73113           1917            274339                NA\n 2 34003 Berg…         118714           1607            953243                NA\n 3 34005 Burl…         102615           1436            461853                NA\n 4 34007 Camd…          82005           1414            522581                NA\n 5 34009 Cape…          83870           3707             95456                NA\n 6 34011 Cumb…          62310           2205            153588                NA\n 7 34013 Esse…          73785           1477            853374                NA\n 8 34015 Glou…          99668           2605            302621                NA\n 9 34017 Huds…          86854           1782            712029                NA\n10 34019 Hunt…         133534           3236            129099                NA\n# ℹ 1 more variable: county_name &lt;chr&gt;"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#data-quality-assessment",
    "href": "assignments/assignment_1/assignment_1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\ncounty_data &lt;- county_data %&gt;%\n  mutate(median_incomeMOE = (median_incomeM / median_incomeE) * 100,\n        rel_categories = case_when(\n            median_incomeMOE &lt; 5 ~ \"High Confidence\",\n            median_incomeMOE &gt;= 5 & median_incomeMOE &lt; 10 ~ \"Moderate Confidence\",\n            median_incomeMOE &gt;= 10 ~ \"Low Confidence\"\n        )\n  )\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\nsummary_data &lt;- county_data %&gt;%\n  count(rel_categories) %&gt;%\n  mutate(percentage = n/sum(n))"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#high-uncertainty-counties",
    "href": "assignments/assignment_1/assignment_1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n#?arrange\n#?select\n#glimpse(county_data)\n\ntop_5 &lt;- county_data %&gt;%\n  arrange(desc(median_incomeMOE)) %&gt;%\n  slice_head( n = 5 ) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, median_incomeMOE, rel_categories)\n\ntop_5\n\n# A tibble: 5 × 5\n  county_name median_incomeE median_incomeM median_incomeMOE rel_categories \n  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;          \n1 Cape May             83870           3707             4.42 High Confidence\n2 Salem                73378           3047             4.15 High Confidence\n3 Cumberland           62310           2205             3.54 High Confidence\n4 Atlantic             73113           1917             2.62 High Confidence\n5 Gloucester           99668           2605             2.61 High Confidence\n\n# Format as table with kable() - include appropriate column names and caption\n#?kable\nkable(top_5,digits = getOption(\"digits\"), caption = \"Top 5 counties in NJ with the highest median income MOE\")\n\n\nTop 5 counties in NJ with the highest median income MOE\n\n\n\n\n\n\n\n\n\ncounty_name\nmedian_incomeE\nmedian_incomeM\nmedian_incomeMOE\nrel_categories\n\n\n\n\nCape May\n83870\n3707\n4.419936\nHigh Confidence\n\n\nSalem\n73378\n3047\n4.152471\nHigh Confidence\n\n\nCumberland\n62310\n2205\n3.538758\nHigh Confidence\n\n\nAtlantic\n73113\n1917\n2.621969\nHigh Confidence\n\n\nGloucester\n99668\n2605\n2.613677\nHigh Confidence\n\n\n\n\n\nData Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]\nCounties with high MOE are not suitable for consideration using algorithmic decision-making, as the margin errors are substantial, which means some important details could be lost in the calculation process. The result of the algorithm could be unreliable and can not convince people. For this income data in New Jersey, Cape May is not a good choice. Factors contributing to the high uncertainty could include income variance across different groups, sample sizes, and data collecting methods."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#focus-area-selection",
    "href": "assignments/assignment_1/assignment_1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# NOTE: Since all the NJ counties are High Confidence, I chose the highest and lowest ones, combined with a moderate one\nselected_counties &lt;- county_data %&gt;%\n  filter(county_name == \"Cape May\"|county_name == \"Bergen\"|county_name == \"Union\")\n\nselected_counties\n\n# A tibble: 3 × 9\n  GEOID NAME   median_incomeE median_incomeM total_populationE total_populationM\n  &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1 34003 Berge…         118714           1607            953243                NA\n2 34009 Cape …          83870           3707             95456                NA\n3 34039 Union…          95000           2210            572079                NA\n# ℹ 3 more variables: county_name &lt;chr&gt;, median_incomeMOE &lt;dbl&gt;,\n#   rel_categories &lt;chr&gt;\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nselected_counties &lt;- selected_counties %&gt;%\n  select(county_name, median_incomeE, median_incomeMOE, rel_categories)\nkable(selected_counties,digits = getOption(\"digits\"), caption = \"Selected Counties\")\n\n\nSelected Counties\n\n\ncounty_name\nmedian_incomeE\nmedian_incomeMOE\nrel_categories\n\n\n\n\nBergen\n118714\n1.353673\nHigh Confidence\n\n\nCape May\n83870\n4.419936\nHigh Confidence\n\n\nUnion\n95000\n2.326316\nHigh Confidence\n\n\n\n\n\nComment on the output: It appears that the median income level and the MOE are somewhat correlated, the higher the median income, the lower the MOE."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#tract-level-demographics",
    "href": "assignments/assignment_1/assignment_1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nvars = c(\n    white = \"B03002_003\",   \n    Black = \"B03002_004\",\n    Hispanic = \"B03002_012\",\n    total_population = \"B03002_001\"\n  )\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\ntract_data &lt;- get_acs(\n  geography = \"tract\",\n  variables = vars,\n  state = my_state,\n  county = c(\"003\", \"009\", \"039\"),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\ntract_data &lt;- tract_data %&gt;%\n  mutate(\n    white_percentage = (whiteE / total_populationE)*100,\n    black_percentage = (BlackE / total_populationE)*100,\n    his_percentage = (HispanicE / total_populationE)*100\n  )\n\n# Add readable tract and county name columns using str_extract() or similar\n#?str_extract()\ntract_data &lt;- tract_data %&gt;%\n  mutate(\n    tract_name  = str_trim(str_split_fixed(NAME, \";\", 3)[,1]),\n    county_name = str_trim(str_remove(str_split_fixed(NAME, \";\", 3)[,2], \" County\"))\n  )"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#demographic-analysis",
    "href": "assignments/assignment_1/assignment_1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\ntop_his_tract &lt;- tract_data %&gt;%\n  arrange(desc(his_percentage)) %&gt;%\n  slice_head(n = 1)\n\ntop_his_tract\n\n# A tibble: 1 × 15\n  GEOID  NAME  whiteE whiteM BlackE BlackM HispanicE HispanicM total_populationE\n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n1 34039… Cens…    218    120    310    219      2935       624              3483\n# ℹ 6 more variables: total_populationM &lt;dbl&gt;, white_percentage &lt;dbl&gt;,\n#   black_percentage &lt;dbl&gt;, his_percentage &lt;dbl&gt;, tract_name &lt;chr&gt;,\n#   county_name &lt;chr&gt;\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\nsummary_data_2 &lt;- tract_data %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    num_tract = n(),\n    avg_white_p = mean(white_percentage, na.rm = TRUE),\n    avg_black_p = mean(black_percentage, na.rm = TRUE),\n    avg_his_p = mean(his_percentage, na.rm = TRUE)\n  )\n\n# Create a nicely formatted table of your results using kable()\nkable(summary_data_2,digits = getOption(\"digits\"), caption = \"Average percentage of races\")\n\n\nAverage percentage of races\n\n\ncounty_name\nnum_tract\navg_white_p\navg_black_p\navg_his_p\n\n\n\n\nBergen\n203\n53.08512\n5.390523\n21.595950\n\n\nCape May\n33\n86.01229\n2.830430\n7.664504\n\n\nUnion\n120\n35.83627\n20.277607\n34.532339"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment_1/assignment_1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\ntract_data &lt;- tract_data %&gt;%\n  mutate(\n    white_MOE = (whiteM / whiteE)*100,\n    black_MOE = (BlackM / BlackE)*100,\n    his_MOE = (HispanicM / HispanicE)*100,\n  )\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\ntract_data &lt;- tract_data %&gt;%\n  mutate(\n    flag = if_else(\n      white_MOE &gt; 15 | black_MOE &gt; 15 | his_MOE &gt; 15,\n      \"Flag\",\n      \"Moderate\"\n    )\n  )\n\n# Create summary statistics showing how many tracts have data quality issues\nnum_flag &lt;- tract_data %&gt;%\n  filter(flag == \"Flag\") %&gt;%\n  summarise(\n    num = n()\n  ) #356"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#pattern-analysis",
    "href": "assignments/assignment_1/assignment_1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\n#NOTE: I chose white alone and calculated the white_flag, cuz all tracts are flagged in the previous step \n\ntract_data &lt;- tract_data %&gt;%\n  mutate(\n    flag_white = if_else(\n      white_MOE &gt; 15,\n      \"Flag\",\n      \"Moderate\"\n    )\n  )\n\nsummary_data_3 &lt;- tract_data %&gt;%\n  group_by(flag_white) %&gt;%\n  summarise(\n    num = n(),\n    avg_pop = mean(total_populationE, na.rm = TRUE),\n    avg_white_p = mean(white_percentage, na.rm = TRUE),\n  )\n\nkable(summary_data_3,digits = getOption(\"digits\"), caption = \"summary of flagged and unflaged group\")\n\n\nsummary of flagged and unflaged group\n\n\nflag_white\nnum\navg_pop\navg_white_p\n\n\n\n\nFlag\n256\n4440.484\n40.63099\n\n\nModerate\n100\n4840.140\n74.68121\n\n\n\n\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?] It seems like that a community with large population and high concentration of certain kind of people are likely to be not flagged. This can be explained by the sample size that MOE would be smaller if the sample size goes larger."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment_1/assignment_1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses?\n\nEquity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings?\nRoot Cause Analysis: What underlying factors drive both data quality issues and bias risk?\nStrategic Recommendations: What should the Department implement to address these systematic issues?\n\nExecutive Summary:\nIn this study, I used MOE percentages as key metrics. I divided the large dataset into several groups to see if there were differences between them. I aim to identify counties or communities that are not suitable for data-driven decision-making processes in economic and racial aspects.\nMy findings are: communities with smaller population size are in the risk of algorithmic bias, what’s more, diverse communities with different races and various groups of people have higher MOE percentage. Economically disadvantaged communities with income inequality are also liked to have the bias issue.\nThe reasons for the data quality issues and bias risk can be: (1) Sample size. Smaller populations usually have higher sample error and missing data. Minority are hard to reach out by the census survey as well. (2) Socioeconomic and Demographic variance. Diverse communties have large internal variation, which leads to higher marginal errors.\nDepartments should not just rely on the algorithm to make their decisions. On-site investigation and community engagement is crucial to know the truth. There are stories, knowledge, and so on that can not reflect from numbers. Planners should be critical about the results of data-driven analysis."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#specific-recommendations",
    "href": "assignments/assignment_1/assignment_1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\nsummary_county &lt;- county_data %&gt;%\n  select(county_name,median_incomeE,median_incomeMOE,rel_categories)\n\nsummary_county\n\n# A tibble: 21 × 4\n   county_name median_incomeE median_incomeMOE rel_categories \n   &lt;chr&gt;                &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;          \n 1 Atlantic             73113             2.62 High Confidence\n 2 Bergen              118714             1.35 High Confidence\n 3 Burlington          102615             1.40 High Confidence\n 4 Camden               82005             1.72 High Confidence\n 5 Cape May             83870             4.42 High Confidence\n 6 Cumberland           62310             3.54 High Confidence\n 7 Essex                73785             2.00 High Confidence\n 8 Gloucester           99668             2.61 High Confidence\n 9 Hudson               86854             2.05 High Confidence\n10 Hunterdon           133534             2.42 High Confidence\n# ℹ 11 more rows\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\nsummary_county &lt;- summary_county %&gt;%\n  mutate(\n    recommendation = \n  case_when(\n    rel_categories == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n    rel_categories == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n    rel_categories == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n  )\n  )\n\n# Format as a professional table with kable()\n\nkable(summary_county,digits = getOption(\"digits\"), caption = \"Recommendation for algorithmic decisions\")\n\n\nRecommendation for algorithmic decisions\n\n\n\n\n\n\n\n\n\ncounty_name\nmedian_incomeE\nmedian_incomeMOE\nrel_categories\nrecommendation\n\n\n\n\nAtlantic\n73113\n2.621969\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBergen\n118714\n1.353673\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBurlington\n102615\n1.399406\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCamden\n82005\n1.724285\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCape May\n83870\n4.419936\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCumberland\n62310\n3.538758\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEssex\n73785\n2.001762\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGloucester\n99668\n2.613677\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHudson\n86854\n2.051719\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHunterdon\n133534\n2.423353\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMercer\n92697\n2.325857\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMiddlesex\n105206\n1.461894\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMonmouth\n118527\n1.605541\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMorris\n130808\n2.084735\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOcean\n82379\n1.766227\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPassaic\n84465\n1.851654\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSalem\n73378\n4.152471\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSomerset\n131948\n2.485828\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSussex\n111094\n2.460979\nHigh Confidence\nSafe for algorithmic decisions\n\n\nUnion\n95000\n2.326316\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWarren\n92620\n2.598791\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: Low MOE group (≈ 1.85 – 1.35) Counties: Passaic, Ocean, Camden, Monmouth, Middlesex, Burlington, Bergen\nCounties requiring additional oversight: Mid MOE group (≈ 2.46 – 2.00) Counties: Sussex, Hunterdon, Union, Mercer, Morris, Hudson, Essex\nCounties needing alternative approaches: High MOE group (≈ 4.42 – 2.60) Counties: Cape May, Salem, Cumberland, Atlantic, Gloucester, Warren, Somerset"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#questions-for-further-investigation",
    "href": "assignments/assignment_1/assignment_1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nDo counties with higher margins of error cluster geographically?\nDo certain counties show persistently high uncertainty?\nAre higher MOEs systematically associated with specific demographic factors?"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#submission-checklist",
    "href": "assignments/assignment_1/assignment_1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\n[✅] All code chunks run without errors\n[✅ All “[Fill this in]” prompts have been completed\n[✅] Tables are properly formatted and readable\n[✅] Executive summary addresses all four required components\n[✅] Portfolio navigation includes this assignment\n[✅] Census API key is properly set\n[✅] Document renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nHi, everyone! My name is Jinheng, and I am second year student in city planning program, smart cities concentration. I have strong interest in urban data science. I took GIS class and introduction to smart cities class last year, and I would like to explore more in this class.\n\n\n\n\nEmail: jinhengc@upenn.edu\nGitHub: @CenJinHeng"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Hi, everyone! My name is Jinheng, and I am second year student in city planning program, smart cities concentration. I have strong interest in urban data science. I took GIS class and introduction to smart cities class last year, and I would like to explore more in this class."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: jinhengc@upenn.edu\nGitHub: @CenJinHeng"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes - Course Introduction",
    "section": "",
    "text": "set of rules to solve problems\nissues of data-driven policy-making: what is true for a group of people is not always true for individuals"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#algorithm",
    "href": "weekly-notes/week-02-notes.html#algorithm",
    "title": "Week 2 Notes - Course Introduction",
    "section": "",
    "text": "set of rules to solve problems\nissues of data-driven policy-making: what is true for a group of people is not always true for individuals"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#difference-between-data-science-and-data-analysis",
    "href": "weekly-notes/week-02-notes.html#difference-between-data-science-and-data-analysis",
    "title": "Week 2 Notes - Course Introduction",
    "section": "Difference between data science and data analysis",
    "text": "Difference between data science and data analysis\n\ndata science: focus on algorithms/methods development\ndata analytics: application"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#machine-learning-and-ai",
    "href": "weekly-notes/week-02-notes.html#machine-learning-and-ai",
    "title": "Week 2 Notes - Course Introduction",
    "section": "Machine learning and AI",
    "text": "Machine learning and AI\n\nMachine learning: classification&prediction learn from data\nAI: adjust and improve across iterations"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#accidental-data",
    "href": "weekly-notes/week-02-notes.html#accidental-data",
    "title": "Week 2 Notes - Course Introduction",
    "section": "Accidental data",
    "text": "Accidental data\n\nget data from open-source sources like social media platforms, instagram"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#data-analytics-is-subjective",
    "href": "weekly-notes/week-02-notes.html#data-analytics-is-subjective",
    "title": "Week 2 Notes - Course Introduction",
    "section": "Data analytics is subjective",
    "text": "Data analytics is subjective\n\ndata cleaning\ndata coding\ndata collection\ninterpret results\nwhat variables to put in the model"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#decennial-census-american-community-survey-acs",
    "href": "weekly-notes/week-02-notes.html#decennial-census-american-community-survey-acs",
    "title": "Week 2 Notes - Course Introduction",
    "section": "Decennial Census & American Community Survey (ACS)",
    "text": "Decennial Census & American Community Survey (ACS)\n\nDecennial Census\n\nfor everyone, full sample\n9 basic questions\nconstitutional requirement\n\nAmerican Community Survey\n\n3% surveyed annually\ndetailed questions\n5-yr estimates are more reliable"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "[List main concepts from lecture]\nGithub key concepts: repo, commit, push, and pull\n[Technical skills covered]\nGithub\nbasic R: install a new package\nmarkdowm"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "[List main concepts from lecture]\nGithub key concepts: repo, commit, push, and pull\n[Technical skills covered]\nGithub\nbasic R: install a new package\nmarkdowm"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\ndplyr: select(), filter(), mutate(), summarize(), group_by()\n%&gt;% is to connect the lines, pipeline\n[Quarto features learned] Quarto is the next generation of markdown"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand] difference between table and data frame? They are exactly the same thing but in two means\n[Areas needing more practice] R studio, github repo"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work]\nprocess a large_scale data set to provide some insights for policy decision_maker!\nbuild a website to communicate with public, improving political trust and transparency"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting] make my website page\n[How I’ll apply this knowledge] help me to process data in my planning studio, and use Github page to show my works and communicate with people!"
  }
]