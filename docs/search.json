[
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6 Notes - Course Introduction",
    "section": "",
    "text": "Start with a simple linear regression using structural features only (e.g., LivingArea → SalePrice).\nInterpretation:\n\nCoefficients show marginal effects (e.g., $ per sq ft).\nEven if coefficients are significant, R² can be low.\n\nLimitation:\n\nLarge share of price variation remains unexplained without location and neighborhood context.\n\n\n\n\n\n\n\nCategorical variables (e.g., neighborhood) enter the model via dummy variables.\nR automatically:\n\nCreates (n-1) dummies.\nChooses one category as reference.\n\nInterpretation:\n\nDummy coefficient = price premium/discount relative to reference, holding other variables constant.\n\nNeighborhood fixed effects:\n\nAbsorb unobserved neighborhood characteristics (schools, amenities, reputation).\nTypically produce large gains in explanatory power and predictive accuracy.\nTrade-off: less interpretability of why differences exist.\n\n\n\n\n\n\n\nUse interactions when the effect of one variable depends on another.\n\nExample: LivingArea × WealthyNeighborhood.\n\nWithout interaction:\n\nSame slope for all groups; only intercept shifts.\n\nWith interaction:\n\nBoth intercept and slope can differ by group.\nCaptures heterogeneous returns to size across market segments.\n\nInterpretation:\n\nInteraction coefficient adjusts the slope for specific categories.\nCheck if interaction improves fit (R², CV) and is substantively meaningful.\n\n\n\n\n\n\n\nUse polynomial terms (e.g., Age and Age²) when relationships are not linear.\n\nTypical pattern: U-shaped or inverted-U.\n\nImplementation:\n\nUse I(Age^2) in formulas to treat squared term literally.\n\nInterpretation:\n\nCoefficients not directly intuitive.\nMarginal effect of Age = β₁ + 2β₂·Age.\n\nEvaluate:\n\nCompare R² and F-test between linear and polynomial models.\nUse residual plots to check improvement.\n\n\n\n\n\n\n\nTobler’s First Law: nearby observations are more related than distant ones.\nHousing prices depend on:\n\nLocal crime, amenities, accessibility, neighborhood environment.\n\nThree common spatial feature constructions:\n\nBuffer counts:\n\nCount events (e.g., crimes) within a fixed radius.\n\nk-Nearest Neighbors (kNN):\n\nAverage distance to k nearest events.\n\nDistance to key points:\n\nDistance to CBD, transit, parks, etc.\n\n\nThese features convert spatial context into usable numeric predictors.\n\n\n\n\n\n\nModel layering:\n\nStructural only → + spatial features → + neighborhood fixed effects.\n\nTypical pattern:\n\nEach step improves predictive performance.\nSpatial features capture continuous location effects.\nFixed effects absorb remaining unobserved neighborhood-level heterogeneity.\n\nImportant:\n\nCoefficients on spatial variables can change once fixed effects are included (less confounding).\n\n\n\n\n\n\n\nUse k-fold CV (e.g., 10-fold) to evaluate out-of-sample performance.\nProblem:\n\nSparse categories (few observations in some neighborhoods) can lead to:\n\n“New level” errors in test folds.\nUnstable estimates.\n\n\nSolutions:\n\nCheck counts per category before CV.\nGroup rare categories into an “Other/Small_Neighborhoods” class.\nAlternatively, drop categories with extremely low counts (must be documented and justified).\n\nUse CV metrics (RMSE, MAE) to compare:\n\nStructural vs spatial vs fixed-effect models.\n\n\n\n\n\n\n\nBuild a simple structural baseline.\nAdd categorical variables and interpret relative effects.\nIntroduce interactions where theory suggests heterogeneous effects.\nAdd polynomial terms to capture non-linearities.\nEngineer spatial features (buffers, kNN, distances).\nAdd neighborhood fixed effects to capture unobserved context.\nUse k-fold CV to select models based on predictive performance.\nInspect residuals and diagnose specification issues at each step."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#baseline-structural-model",
    "href": "weekly-notes/week-06-notes.html#baseline-structural-model",
    "title": "Week 6 Notes - Course Introduction",
    "section": "",
    "text": "Start with a simple linear regression using structural features only (e.g., LivingArea → SalePrice).\nInterpretation:\n\nCoefficients show marginal effects (e.g., $ per sq ft).\nEven if coefficients are significant, R² can be low.\n\nLimitation:\n\nLarge share of price variation remains unexplained without location and neighborhood context."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#categorical-variables-and-fixed-effects",
    "href": "weekly-notes/week-06-notes.html#categorical-variables-and-fixed-effects",
    "title": "Week 6 Notes - Course Introduction",
    "section": "",
    "text": "Categorical variables (e.g., neighborhood) enter the model via dummy variables.\nR automatically:\n\nCreates (n-1) dummies.\nChooses one category as reference.\n\nInterpretation:\n\nDummy coefficient = price premium/discount relative to reference, holding other variables constant.\n\nNeighborhood fixed effects:\n\nAbsorb unobserved neighborhood characteristics (schools, amenities, reputation).\nTypically produce large gains in explanatory power and predictive accuracy.\nTrade-off: less interpretability of why differences exist."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#interaction-terms",
    "href": "weekly-notes/week-06-notes.html#interaction-terms",
    "title": "Week 6 Notes - Course Introduction",
    "section": "",
    "text": "Use interactions when the effect of one variable depends on another.\n\nExample: LivingArea × WealthyNeighborhood.\n\nWithout interaction:\n\nSame slope for all groups; only intercept shifts.\n\nWith interaction:\n\nBoth intercept and slope can differ by group.\nCaptures heterogeneous returns to size across market segments.\n\nInterpretation:\n\nInteraction coefficient adjusts the slope for specific categories.\nCheck if interaction improves fit (R², CV) and is substantively meaningful."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#polynomial-terms-non-linear-effects",
    "href": "weekly-notes/week-06-notes.html#polynomial-terms-non-linear-effects",
    "title": "Week 6 Notes - Course Introduction",
    "section": "",
    "text": "Use polynomial terms (e.g., Age and Age²) when relationships are not linear.\n\nTypical pattern: U-shaped or inverted-U.\n\nImplementation:\n\nUse I(Age^2) in formulas to treat squared term literally.\n\nInterpretation:\n\nCoefficients not directly intuitive.\nMarginal effect of Age = β₁ + 2β₂·Age.\n\nEvaluate:\n\nCompare R² and F-test between linear and polynomial models.\nUse residual plots to check improvement."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#spatial-features-why-space-matters",
    "href": "weekly-notes/week-06-notes.html#spatial-features-why-space-matters",
    "title": "Week 6 Notes - Course Introduction",
    "section": "",
    "text": "Tobler’s First Law: nearby observations are more related than distant ones.\nHousing prices depend on:\n\nLocal crime, amenities, accessibility, neighborhood environment.\n\nThree common spatial feature constructions:\n\nBuffer counts:\n\nCount events (e.g., crimes) within a fixed radius.\n\nk-Nearest Neighbors (kNN):\n\nAverage distance to k nearest events.\n\nDistance to key points:\n\nDistance to CBD, transit, parks, etc.\n\n\nThese features convert spatial context into usable numeric predictors."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#combining-structural-spatial-and-fixed-effects",
    "href": "weekly-notes/week-06-notes.html#combining-structural-spatial-and-fixed-effects",
    "title": "Week 6 Notes - Course Introduction",
    "section": "",
    "text": "Model layering:\n\nStructural only → + spatial features → + neighborhood fixed effects.\n\nTypical pattern:\n\nEach step improves predictive performance.\nSpatial features capture continuous location effects.\nFixed effects absorb remaining unobserved neighborhood-level heterogeneity.\n\nImportant:\n\nCoefficients on spatial variables can change once fixed effects are included (less confounding)."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#cross-validation-with-categorical-variables",
    "href": "weekly-notes/week-06-notes.html#cross-validation-with-categorical-variables",
    "title": "Week 6 Notes - Course Introduction",
    "section": "",
    "text": "Use k-fold CV (e.g., 10-fold) to evaluate out-of-sample performance.\nProblem:\n\nSparse categories (few observations in some neighborhoods) can lead to:\n\n“New level” errors in test folds.\nUnstable estimates.\n\n\nSolutions:\n\nCheck counts per category before CV.\nGroup rare categories into an “Other/Small_Neighborhoods” class.\nAlternatively, drop categories with extremely low counts (must be documented and justified).\n\nUse CV metrics (RMSE, MAE) to compare:\n\nStructural vs spatial vs fixed-effect models."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#practical-modeling-workflow-hedonic-spatial-regression",
    "href": "weekly-notes/week-06-notes.html#practical-modeling-workflow-hedonic-spatial-regression",
    "title": "Week 6 Notes - Course Introduction",
    "section": "",
    "text": "Build a simple structural baseline.\nAdd categorical variables and interpret relative effects.\nIntroduce interactions where theory suggests heterogeneous effects.\nAdd polynomial terms to capture non-linearities.\nEngineer spatial features (buffers, kNN, distances).\nAdd neighborhood fixed effects to capture unobserved context.\nUse k-fold CV to select models based on predictive performance.\nInspect residuals and diagnose specification issues at each step."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "[List main concepts from lecture]\nGithub key concepts: repo, commit, push, and pull\n[Technical skills covered]\nGithub\nbasic R: install a new package\nmarkdowm"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "[List main concepts from lecture]\nGithub key concepts: repo, commit, push, and pull\n[Technical skills covered]\nGithub\nbasic R: install a new package\nmarkdowm"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\ndplyr: select(), filter(), mutate(), summarize(), group_by()\n%&gt;% is to connect the lines, pipeline\n[Quarto features learned] Quarto is the next generation of markdown"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand] difference between table and data frame? They are exactly the same thing but in two means\n[Areas needing more practice] R studio, github repo"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work]\nprocess a large_scale data set to provide some insights for policy decision_maker!\nbuild a website to communicate with public, improving political trust and transparency"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting] make my website page\n[How I’ll apply this knowledge] help me to process data in my planning studio, and use Github page to show my works and communicate with people!"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "geometry: shape and location\nattributes: data about the feature\ntwo forms in GIS: raster (pixel, continous) and vector (points, lines and polygons)\nformat of geo data: shp, geojson, KML/KMZ (Google Earth)…\n\n\n\n\n\nincludes shp (storaging geometry), shx, dbf (tabel)\n\n\n\n\n\ndrop geo column if it is troublesome\n\nst_intersects(): Counties affected by flooding\nst_touches(): Neighboring counties\nst_within(): Schools within district boundaries\nst_contains(): Districts containing hospitals\nst_overlaps(): Overlapping service areas\nst_disjoint(): Counties separate from urban areas\n\n\n\n\n\n\nThe Earth is round, but maps are flat\nProblems: hard to preserve area, distance, and angles simultaneously\nCalculation steps\n\nstep 1: approximate the earth Ellipsoid\nstep 2: tie the Ellipsoid into the real Earth\nstep 3: put down the lat/lng grid\n\nGeographic (Geodetic) coordinate systems (Lat/lng)\n\nMercator: keeps lines(lat) constantly long, areas far from the central line are in wrong shape\nTransverse: using Lng\nConic\ncylindrical\nplaner\n\nProjected coordinate systems\n\nUTM\n\nGCS:\n\nLat/Lng coordinated\ngood for large scale, bad for area/distance calculations\n\nPCS:\n\nX/Y coordinates on a flat plane\nunits: meters, feet\ngood for local analysis, accurate measurement, bad for large areas, global data sets\n\nst_set_crs() is to set CRS if missing, st_transform() is to change CRS"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#spatial-data",
    "href": "weekly-notes/week-04-notes.html#spatial-data",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "geometry: shape and location\nattributes: data about the feature\ntwo forms in GIS: raster (pixel, continous) and vector (points, lines and polygons)\nformat of geo data: shp, geojson, KML/KMZ (Google Earth)…"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#shp",
    "href": "weekly-notes/week-04-notes.html#shp",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "includes shp (storaging geometry), shx, dbf (tabel)"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#sf-package",
    "href": "weekly-notes/week-04-notes.html#sf-package",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "drop geo column if it is troublesome\n\nst_intersects(): Counties affected by flooding\nst_touches(): Neighboring counties\nst_within(): Schools within district boundaries\nst_contains(): Districts containing hospitals\nst_overlaps(): Overlapping service areas\nst_disjoint(): Counties separate from urban areas"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coordinate-reference-systems",
    "href": "weekly-notes/week-04-notes.html#coordinate-reference-systems",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "The Earth is round, but maps are flat\nProblems: hard to preserve area, distance, and angles simultaneously\nCalculation steps\n\nstep 1: approximate the earth Ellipsoid\nstep 2: tie the Ellipsoid into the real Earth\nstep 3: put down the lat/lng grid\n\nGeographic (Geodetic) coordinate systems (Lat/lng)\n\nMercator: keeps lines(lat) constantly long, areas far from the central line are in wrong shape\nTransverse: using Lng\nConic\ncylindrical\nplaner\n\nProjected coordinate systems\n\nUTM\n\nGCS:\n\nLat/Lng coordinated\ngood for large scale, bad for area/distance calculations\n\nPCS:\n\nX/Y coordinates on a flat plane\nunits: meters, feet\ngood for local analysis, accurate measurement, bad for large areas, global data sets\n\nst_set_crs() is to set CRS if missing, st_transform() is to change CRS"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "Goal: Estimate the function f(X) that relates predictors (X) to an outcome (Y).\nModel: Y = f(X) + ε\n\nf(X): systematic relationship (fixed but unknown)\nε: random error (irreducible)\n\nTwo main approaches:\n\nParametric: assumes a specific form (e.g., linear), easier to interpret.\nNon-parametric: flexible, requires more data, harder to interpret.\n\nLinear regression is a parametric method assuming linearity.\n\n\n\n\n\n\nInference: understand how X affects Y.\n\nFocus on coefficients and statistical significance.\n\nPrediction: estimate Y for new observations.\n\nFocus on accuracy, not interpretation.\n\nA model can be statistically good but ethically harmful.\n\n\n\n\n\n\nOLS (Ordinary Least Squares): estimates βs minimizing squared errors.\nExample: median income ~ population (PA counties).\nInterpretation:\n\nβ₀ = intercept (baseline)\nβ₁ = change in Y for one-unit change in X\n\nR²: proportion of variance explained (in-sample fit).\n\nIn-sample fit ≠ out-of-sample performance.\n\nOverfitting: too complex, fits noise.\nUse train/test split or cross-validation (CV) to evaluate generalization.\n\n10-fold CV provides more stable estimates.\nKey metrics: RMSE, MAE, R².\n\n\n\n\n\n\nLinear regression assumptions: 1. Linearity: relationship between X and Y is linear. - Check residual plot → random scatter = good. 2. Constant variance (Homoscedasticity): - Violations → heteroskedasticity. - Check with Breusch-Pagan test. 3. Normality of residuals: - Use Q-Q plot. - Matters for inference, less for prediction. 4. No multicollinearity (only in multiple regression): - Check Variance Inflation Factor (VIF). 5. No influential outliers: - Check Cook’s D and leverage. - Investigate, don’t automatically remove.\n\n\n\n\n\nAdd more predictors (education, poverty rate, etc.).\nUse transformations (e.g., log) to handle nonlinearity.\nInclude categorical variables (e.g., metro/non-metro).\nAlways re-check assumptions after modifications.\n\n\n\n\n\n\nModel quality ≠ fairness.\nOutliers may represent important or marginalized cases.\nNever remove data points without investigation.\nEvaluate model implications in policy contexts.\n\n\n\n\n\n\nUnderstand problem and define goal.\nVisualize relationships.\nFit the model.\nEvaluate fit and predictive performance.\nCheck assumptions and diagnostics.\nRefine with transformations or added variables.\nConsider ethical implications."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#statistical-learning-framework",
    "href": "weekly-notes/week-05-notes.html#statistical-learning-framework",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "Goal: Estimate the function f(X) that relates predictors (X) to an outcome (Y).\nModel: Y = f(X) + ε\n\nf(X): systematic relationship (fixed but unknown)\nε: random error (irreducible)\n\nTwo main approaches:\n\nParametric: assumes a specific form (e.g., linear), easier to interpret.\nNon-parametric: flexible, requires more data, harder to interpret.\n\nLinear regression is a parametric method assuming linearity."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#purposes-of-modeling",
    "href": "weekly-notes/week-05-notes.html#purposes-of-modeling",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "Inference: understand how X affects Y.\n\nFocus on coefficients and statistical significance.\n\nPrediction: estimate Y for new observations.\n\nFocus on accuracy, not interpretation.\n\nA model can be statistically good but ethically harmful."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#model-building-and-evaluation",
    "href": "weekly-notes/week-05-notes.html#model-building-and-evaluation",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "OLS (Ordinary Least Squares): estimates βs minimizing squared errors.\nExample: median income ~ population (PA counties).\nInterpretation:\n\nβ₀ = intercept (baseline)\nβ₁ = change in Y for one-unit change in X\n\nR²: proportion of variance explained (in-sample fit).\n\nIn-sample fit ≠ out-of-sample performance.\n\nOverfitting: too complex, fits noise.\nUse train/test split or cross-validation (CV) to evaluate generalization.\n\n10-fold CV provides more stable estimates.\nKey metrics: RMSE, MAE, R²."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#model-assumptions-and-diagnostics",
    "href": "weekly-notes/week-05-notes.html#model-assumptions-and-diagnostics",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "Linear regression assumptions: 1. Linearity: relationship between X and Y is linear. - Check residual plot → random scatter = good. 2. Constant variance (Homoscedasticity): - Violations → heteroskedasticity. - Check with Breusch-Pagan test. 3. Normality of residuals: - Use Q-Q plot. - Matters for inference, less for prediction. 4. No multicollinearity (only in multiple regression): - Check Variance Inflation Factor (VIF). 5. No influential outliers: - Check Cook’s D and leverage. - Investigate, don’t automatically remove."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#improving-the-model",
    "href": "weekly-notes/week-05-notes.html#improving-the-model",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "Add more predictors (education, poverty rate, etc.).\nUse transformations (e.g., log) to handle nonlinearity.\nInclude categorical variables (e.g., metro/non-metro).\nAlways re-check assumptions after modifications."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#ethical-and-analytical-awareness",
    "href": "weekly-notes/week-05-notes.html#ethical-and-analytical-awareness",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "Model quality ≠ fairness.\nOutliers may represent important or marginalized cases.\nNever remove data points without investigation.\nEvaluate model implications in policy contexts."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#regression-workflow-summary",
    "href": "weekly-notes/week-05-notes.html#regression-workflow-summary",
    "title": "Week 4 Notes - Course Introduction",
    "section": "",
    "text": "Understand problem and define goal.\nVisualize relationships.\nFit the model.\nEvaluate fit and predictive performance.\nCheck assumptions and diagnostics.\nRefine with transformations or added variables.\nConsider ethical implications."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#research-question",
    "href": "midterm/slides/Midterm_slides_v3.html#research-question",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Research Question",
    "text": "Research Question\n\nHow accurately can we predict 2023–2024 residential sale prices in Philadelphia using property characteristics and neighborhood/contextual features, in order to improve the city’s automated property tax assessment model?\n\nWhy This Matters\n\nMore accurate assessments -&gt; fairer taxes and fewer disputes.\nNeighborhood and spatial insights capture real market dynamics beyond building features.\nTransparent and data-driven modeling builds trust and accountability in city governance."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#data-overview",
    "href": "midterm/slides/Midterm_slides_v3.html#data-overview",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Data Overview",
    "text": "Data Overview\nOur Data Foundation\n\nPhiladelphia Property Sales (2023–2024)\nOver 17,200 residential transactions citywide capturing sale price, location, and property structural details.\nNeighborhood & Contextual Data\nCombined city (OpenDataPhilly) and census (ACS 2023) information on:\n\nDemographics: race, household income, employment, education\nPublic safety: proximity to violent and petty crime\nAccessibility: transit stops, bike lanes\nAmenities & services: parks, schools, hospitals, fire department, food retail"
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#what-do-home-prices-look-like",
    "href": "midterm/slides/Midterm_slides_v3.html#what-do-home-prices-look-like",
    "title": "Philadelphia Housing Price Prediction",
    "section": "What Do Home Prices Look Like?",
    "text": "What Do Home Prices Look Like?\n\nHistogram of Sale PricesKey Findings\n\nMost homes sell for under $400,000.\n\nA small number of luxury properties push up the high end.\n\nReflects strong housing inequality across neighborhoods."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#where-are-expensive-homes",
    "href": "midterm/slides/Midterm_slides_v3.html#where-are-expensive-homes",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Where Are Expensive Homes?",
    "text": "Where Are Expensive Homes?\n\n\nKey Findings:\n\nCenter City and Northwest Philadelphia are highest-value clusters.\n\nRiver Wards and University City show emerging appreciation.\n\nNorth Philadelphia shows predominantly lower-priced housing.\n\n\n\n\n\n\nGeographical Distribution of Sale Prices"
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#neighborhood-home-prices",
    "href": "midterm/slides/Midterm_slides_v3.html#neighborhood-home-prices",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Neighborhood & Home Prices?",
    "text": "Neighborhood & Home Prices?\n\nBubble Map of Sale Prices vs. Household Income by NeighborhoodKey Findings:\n\nNeighborhood wealth remains a dominant factor in home values.\nLow-income, high-poverty neighborhoods continue to see limited appreciation."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#how-does-model-performance-improve",
    "href": "midterm/slides/Midterm_slides_v3.html#how-does-model-performance-improve",
    "title": "Philadelphia Housing Price Prediction",
    "section": "How Does Model Performance Improve?",
    "text": "How Does Model Performance Improve?\n\n\n\nModel\nCV RMSE\nR²\n\n\n\n\nStructural Only\n136,121\n0.67\n\n\nSpatial\n108,112\n0.79\n\n\nFixed Effects\n93,038\n0.85\n\n\nHyper-model\n64,974\n0.92\n\n\n\nBottom Line\nEach additional data layer improves accuracy, with neighborhood / market-value features producing the largest jump (lowest RMSE and highest R²)."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#what-drives-the-improvement",
    "href": "midterm/slides/Midterm_slides_v3.html#what-drives-the-improvement",
    "title": "Philadelphia Housing Price Prediction",
    "section": "What Drives the Improvement?",
    "text": "What Drives the Improvement?\n\nFrom taking into accounts only the structural/building quality variables in our first model;\nAdding Spatial Data accounts for proximity to parks, public schools, food retail, and violent crime.\nAdding Census & Socioeconomic Context captures income, education, and housing quality.\n\nAdding Neighborhood Fixed Effects controls for unobserved local traits.\n\nGrouping Small Neighborhoods stabilizes estimates where data are sparse (sales&lt;10).\nIncluding Market Value Benchmarks aligns predictions with broader price trends.\n\nTakeaway:\nPhiladelphia’s housing market is highly localized — accuracy improves most when models reflect neighborhood structure and market context. The Hyper model which optimized by groups small neighborhoods, is more parsimonious while maintaining high predictive performance."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#best-models",
    "href": "midterm/slides/Midterm_slides_v3.html#best-models",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Best Models",
    "text": "Best Models\n\n\n\nModel\nRMSE ($)\nR²\n\n\n\n\nModel 3: Fixed Effects\n$93,038\n0.85\n\n\nModel 4: Hyper Model\n$64,974\n0.92\n\n\n\nBoth models capture structural, spatial, census, and socioeconomic factors,\nwhile incorporating neighborhood effects to reflect local market variation.\nKey Insight:\nAdding small-neighborhood grouping and market value signals delivers a major leap in accuracy — boosting R² from 0.85 → 0.93. By going down to the block level, our model captures fine-grained local variation, showing that neighborhood and market context are essential for fair property assessments."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#key-findings",
    "href": "midterm/slides/Midterm_slides_v3.html#key-findings",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Key Findings",
    "text": "Key Findings\nTop Predictors\n\nNeighborhoods (Prestige/unobserved characteristics)\nMarket Value\n\nInteresting Effects\n\nHigher % Black in the tract is associated with lower sale prices, holding all else constant.\nHigher % age 65+ (more seniors) is associated with higher sale prices.\nViolent incidents within ~600 feet of the home are associated with lower sale prices."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#hardest-to-predict",
    "href": "midterm/slides/Midterm_slides_v3.html#hardest-to-predict",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Hardest To Predict",
    "text": "Hardest To Predict\n\nVisualization of Residual by Neighborhood\nResiduals are calculated as actual price minus predicted price.\nDowntown neighborhoods tend to have under predicted values, while suburban areas show over predicted values"
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#policy-recommendations",
    "href": "midterm/slides/Midterm_slides_v3.html#policy-recommendations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Policy Recommendations",
    "text": "Policy Recommendations\nKey Actions\n\nLending and rehab programs should include an equity adjustment so borrowers in historically underpriced Black areas can actually access capital.\nKeep long-term older homeowners in place with repair help and tax relief, because neighborhood stability itself raises surrounding home values.\nNever deploy pricing algorithms like this for taxes or loan caps without a bias check; the model should be used to monitor inequity, not lock it in.\nFund repairs and upgrades for small owners in undervalued areas — not just in already-rich areas — so families who maintain their homes can actually build wealth.\nUse models like this as a bias check — not to set taxes or loan limits directly, but to detect where the market may be undervaluing certain neighborhoods so policy can respond."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#limitations-next-steps",
    "href": "midterm/slides/Midterm_slides_v3.html#limitations-next-steps",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\nLimitations\n\nData coverage: After cleaning, the dataset dropped from ~24K to ~17K sales; if removals weren’t random, the model may overrepresent “normal” transactions and underrepresent edge cases.\nLagged timing: A single nearby incident doesn’t instantly cut value; what matters is a sustained history of violence that shapes how buyers perceive safety and status on that block.\nInterior detail: We can’t actually see renovation quality (finishes, systems, upgrades), so the “interior condition” label is coarse — and assessed market value is likely soaking up a lot of that missing nuance.\n\nNext Steps\n\nIncorporate Spatial Models: Explore spatial regression and spatial lag models to better capture location-based dependencies and spatial effects on housing prices.\nField Validation & Data Enrichment: Conduct field surveys and integrate cross-validated dataset to assess model and ensure predictions accurately reflect real market conditions."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#thank-you",
    "href": "midterm/slides/Midterm_slides_v3.html#thank-you",
    "title": "Philadelphia Housing Price Prediction",
    "section": "THANK YOU",
    "text": "THANK YOU"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nHi, everyone! My name is Jinheng, and I am second year student in city planning program, smart cities concentration. I have strong interest in urban data science. I took GIS class and introduction to smart cities class last year, and I would like to explore more in this class.\n\n\n\n\nEmail: jinhengc@upenn.edu\nGitHub: @CenJinHeng"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Hi, everyone! My name is Jinheng, and I am second year student in city planning program, smart cities concentration. I have strong interest in urban data science. I took GIS class and introduction to smart cities class last year, and I would like to explore more in this class."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: jinhengc@upenn.edu\nGitHub: @CenJinHeng"
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#assignment-overview",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data:\n\nPennsylvania county boundaries\nPennsylvania hospitals (from lecture data)\nPennsylvania census tracts\n\nYour Task:\n\n# Load required packages\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\nlibrary(knitr)\n\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  # Suppress tigris progress bars\n\n# Load spatial data\npa_counties &lt;- st_read(\"data/Pennsylvania_County_Boundaries.shp\")\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `/Users/jinhengcen/Documents/portfolio-setup-CenJinHeng/assignments/assignment_2/data/Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nhospitals &lt;- st_read(\"data/hospitals.geojson\")\n\nReading layer `hospitals' from data source \n  `/Users/jinhengcen/Documents/portfolio-setup-CenJinHeng/assignments/assignment_2/data/hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\n\n# Check that all data loaded correctly\n\nQuestions to answer:\n\nHow many hospitals are in your dataset? 223\nHow many census tracts? 3445\nWhat coordinate reference system is each dataset in? I converted them all in WGS 84 / Pseudo-Mercator\n\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables:\n\nTotal population\nMedian household income\nPopulation 65 years and over (you may need to sum multiple age categories)\n\nYour Task:\n\n# Get demographic data from ACS\ntract_data &lt;- get_acs(\n  geography = \"tract\",\n  state     = \"42\",\n  variables = c(\n    total_population = \"B01003_001\",\n    median_household_income = \"B19013_001\"\n  ),\n  year = 2023,\n  survey = \"acs5\",\n  output = \"wide\",\n  progress = FALSE\n)\n\nage_data &lt;- get_acs(\n  geography = \"tract\",\n  state     = \"42\",\n  variables = c(\n    male_65 &lt;- c(\"B01001_020\",\"B01001_021\",\"B01001_022\",\"B01001_023\",\"B01001_024\",\"B01001_025\"),\n    female_65 &lt;- c(\"B01001_044\",\"B01001_045\",\"B01001_046\",\"B01001_047\",\"B01001_048\",\"B01001_049\")\n  ),\n  year = 2023,\n  survey = \"acs5\",\n  progress = FALSE\n) %&gt;% \n  group_by (GEOID) %&gt;%\n  summarise(pop_65 = sum(estimate, na.rm = TRUE))\n\n\ntract_data &lt;- tract_data %&gt;%\n  left_join(age_data, by = \"GEOID\")\n\n\ntract_data %&gt;%\n  summarise(missing_data = sum(is.na(median_household_incomeE)))\n\n# A tibble: 1 × 1\n  missing_data\n         &lt;int&gt;\n1           66\n\ntract_data %&gt;%\n  summarise(median_income = sum(median(median_household_incomeE, na.rm = TRUE)))\n\n# A tibble: 1 × 1\n  median_income\n          &lt;dbl&gt;\n1        72944.\n\n# Join to tract boundaries\n\ncensus_tracts_joined &lt;- census_tracts %&gt;%\n  left_join(tract_data, by = \"GEOID\")\n\nQuestions to answer:\n\nWhat year of ACS data are you using? 2023\nHow many tracts have missing income data? 66\nWhat is the median income across all PA census tracts? 72943.5\n\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\n\ncensus_tracts_joined &lt;- census_tracts_joined %&gt;%\n  mutate(\n    percentage_65 = pop_65/total_populationE\n  )\n\nvulnerable_tracts &lt;- census_tracts_joined %&gt;%\n  filter(median_household_incomeE &lt; 48000,\n         percentage_65 &gt; 0.20\n         )\n\nQuestions to answer:\n\nWhat income threshold did you choose and why? For the income threshold, I chose 48,000 based on the 2025 poverty guidelines by the USCIS. For a family with four people, 150% of the federal poverty level is $48,225.\nWhat elderly population threshold did you choose and why? For the age threshold, I chose the percentage of people over 65 years old is over 20%, which can be seen as an elderly community.\nHow many tracts meet your vulnerability criteria? 126\nWhat percentage of PA census tracts are considered vulnerable by your definition? the percentage of vulnerable tracts = 126 / 3445 = 3.66%\n\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n# Transform to appropriate projected CRS\n\nvulnerable_tracts_proj &lt;- vulnerable_tracts %&gt;%\n  st_transform(5070) # 5070 is for the whole country. For south PA alone, we can use 3365, which is more precise.\n\nhospitals_proj &lt;- hospitals %&gt;%\n  st_transform(5070)\n\n\n# Calculate distance from each tract centroid to nearest hospital\n\ntract_centroids &lt;- st_centroid(vulnerable_tracts_proj)\n\nnearest_hospital_idx &lt;- st_nearest_feature(tract_centroids, hospitals_proj)\n\ndist_to_hospital &lt;- st_distance(\n  tract_centroids,\n  hospitals_proj[nearest_hospital_idx, ],\n  by_element = TRUE\n)\n\n\nvulnerable_tracts_proj &lt;- vulnerable_tracts_proj %&gt;%\n  mutate(\n    dist_to_hospital = as.numeric(dist_to_hospital),\n    dist_to_hospital_mi = dist_to_hospital * 0.000621371\n  )\n\ndistant_data &lt;- vulnerable_tracts_proj %&gt;%\n  summarise(\n    avg_distance = mean(dist_to_hospital_mi, na.rm = TRUE),\n    max_dist = max(dist_to_hospital_mi, na.rm = TRUE),\n  )\n\n\nnum_over15 &lt;- vulnerable_tracts_proj %&gt;%\n  filter(dist_to_hospital_mi &gt; 15) %&gt;%\n  summarise(\n    n_over15 = n(),\n  )\n\nRequirements:\n\nUse an appropriate projected coordinate system for Pennsylvania\nCalculate distances in miles\nExplain why you chose your projection\n\nQuestions to answer:\n\nWhat is the average distance to the nearest hospital for vulnerable tracts? 2.8 miles\nWhat is the maximum distance? 18.6 miles\nHow many vulnerable tracts are more than 15 miles from the nearest hospital? 1\n\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\n\nundeserved &lt;- vulnerable_tracts_proj %&gt;%\n  filter(dist_to_hospital_mi &gt; 15)\n\nQuestions to answer:\n\nHow many tracts are underserved? only 1!\nWhat percentage of vulnerable tracts are underserved? 1/126 = 0.79%\nDoes this surprise you? Why or why not? No, I looked at the distance data of the vulnerable tracts, most of them (92.9%) are under 10 miles (16000m). I think 15 miles is a very big number, so we just have one tract fulfill all the requirements.\n\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n# Spatial join tracts to counties\nvulnerable_tracts_proj &lt;- st_transform(vulnerable_tracts_proj, st_crs(pa_counties))\nundeserved  &lt;- st_transform(undeserved , st_crs(pa_counties))\n\ncounties_num = census_tracts %&gt;%\n  st_centroid() %&gt;%\n  st_join(pa_counties%&gt;% select(COUNTY_NAM)) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_tract =  n(),\n  )%&gt;%\n  arrange(n_tract)\n\ncounties_vuln &lt;- vulnerable_tracts_proj %&gt;%\n  st_centroid() %&gt;%\n  st_join(pa_counties%&gt;% select(COUNTY_NAM)) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_vuln =  n(),\n    avg_dist_vuln_mi = mean(dist_to_hospital_mi, na.rm = TRUE),\n    total_vuln_pop = sum(pop_65, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(total_vuln_pop))\n\ncounties_unders &lt;- undeserved %&gt;%\n  st_centroid() %&gt;%\n  st_join(pa_counties%&gt;% select(COUNTY_NAM)) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_unders =  n(),\n  ) %&gt;%\n  arrange(desc(n_unders))\n\n\n\n# Aggregate statistics by county\n\ncounties_stat = counties_num %&gt;%\n  left_join(counties_vuln, by = \"COUNTY_NAM\") %&gt;%\n  left_join(counties_unders, by = \"COUNTY_NAM\") %&gt;%\n  mutate(\n    percentage_unders = n_unders / n_vuln,\n    percentage_vuln = n_vuln / n_tract\n  )%&gt;%\n  arrange(desc(percentage_vuln))\n\nRequired county-level statistics:\n\nNumber of vulnerable tracts\nNumber of underserved tracts\n\nPercentage of vulnerable tracts that are underserved\nAverage distance to nearest hospital for vulnerable tracts\nTotal vulnerable population\n\nQuestions to answer:\n\nWhich 5 counties have the highest percentage of underserved vulnerable tracts? CAMERON is the highest, with 100% in the number. Since I only have 1 underserved tract, I would show top 5 counties with the highest percentage of vulnerable tracts:\n\n\nCAMERON 50.0%\nFAYETTE 19.4%\nCAMBRIA 19.0%\nJEFFERSON 15.4%\nWARREN 15.4%\n\n\nWhich counties have the most vulnerable people living far from hospitals? PHILADELPHIA, with the number of 18022\nAre there any patterns in where underserved counties are located? Since I only have 1 underserved tract, I would observe patterns of vulnerable counties.\n\n\nMost of them are from rural areas with less tracts number, so they have higher percentage of vulnerable tracts\nThey have large numbers of vulnerable population, which means lots of low-income and old people living there. Most of them have number over 1000.\n\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\n# metrics for the priority: vulnerable index = normalized_number_of_vulnerable_tracts*0.3 + normalized_average_distant _hospital*0.5 + normalized_total_population*0.2\n\nscale_minmax &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  if (!is.finite(diff(rng)) || diff(rng) == 0) return(rep(0.5, length(x)))\n  (x - rng[1]) / diff(rng)\n}\n\ncounties_stat_index &lt;- counties_stat %&gt;%\n  mutate(\n    n_vuln_norm = scale_minmax(n_vuln),\n    avg_dist_vuln_mi_norm = scale_minmax(avg_dist_vuln_mi),\n    total_vuln_pop_norm = scale_minmax(total_vuln_pop),\n    percentage_vuln_norm = scale_minmax(percentage_vuln)\n  ) %&gt;%\n   mutate(\n     vuln_index =\n       0.3  * n_vuln_norm +\n       0.5 * avg_dist_vuln_mi_norm +\n       0.2 * total_vuln_pop_norm\n   ) %&gt;%\n  arrange(desc(vuln_index))\n\n\n#make table\n\ncounties_table &lt;- counties_stat_index %&gt;%\n  select(\n    county_name = COUNTY_NAM,\n    vulnerable_index = vuln_index,\n    vulnerable_tract_number = n_vuln,\n    average_dist_to_hospital = avg_dist_vuln_mi,\n    total_vulnerable_pop = total_vuln_pop\n  ) %&gt;%\n  mutate(\n    vulnerable_index = round(vulnerable_index, 3),\n    average_dist_to_hospital = round(average_dist_to_hospital, 2),\n    total_vulnerable_pop = comma(total_vulnerable_pop)\n  )%&gt;%\n  arrange(desc(vulnerable_index))\n\nkable(\n  counties_table,\n  caption = \"PA County-level Vulnerability Summary\",\n  align = \"lcccc\"\n)\n\n\nPA County-level Vulnerability Summary\n\n\n\n\n\n\n\n\n\ncounty_name\nvulnerable_index\nvulnerable_tract_number\naverage_dist_to_hospital\ntotal_vulnerable_pop\n\n\n\n\nALLEGHENY\n0.519\n28\n2.39\n15,213\n\n\nCAMERON\n0.502\n1\n18.56\n504\n\n\nPHILADELPHIA\n0.409\n19\n0.89\n18,022\n\n\nHUNTINGDON\n0.383\n1\n14.18\n730\n\n\nNORTHUMBERLAND\n0.321\n3\n10.81\n1,603\n\n\nCAMBRIA\n0.226\n8\n4.15\n4,588\n\n\nVENANGO\n0.218\n1\n8.40\n363\n\n\nFAYETTE\n0.183\n7\n2.55\n5,780\n\n\nERIE\n0.179\n7\n2.35\n5,923\n\n\nWARREN\n0.178\n2\n6.19\n1,257\n\n\nLANCASTER\n0.175\n1\n6.60\n1,034\n\n\nLUZERNE\n0.153\n6\n2.51\n4,195\n\n\nWESTMORELAND\n0.150\n6\n2.86\n3,057\n\n\nLAWRENCE\n0.149\n3\n4.52\n1,793\n\n\nNORTHAMPTON\n0.130\n1\n5.03\n850\n\n\nCLINTON\n0.127\n1\n4.89\n913\n\n\nDELAWARE\n0.120\n6\n1.32\n4,179\n\n\nLACKAWANNA\n0.104\n5\n1.67\n2,871\n\n\nLEHIGH\n0.092\n1\n3.42\n1,417\n\n\nBEAVER\n0.076\n2\n2.53\n1,291\n\n\nBLAIR\n0.076\n2\n2.18\n2,079\n\n\nINDIANA\n0.070\n1\n2.63\n1,447\n\n\nWASHINGTON\n0.068\n2\n2.39\n915\n\n\nLYCOMING\n0.066\n1\n2.90\n470\n\n\nMERCER\n0.063\n3\n1.42\n1,866\n\n\nJEFFERSON\n0.057\n2\n1.72\n1,566\n\n\nMIFFLIN\n0.038\n1\n1.87\n516\n\n\nYORK\n0.028\n1\n1.55\n425\n\n\nCRAWFORD\n0.015\n1\n0.93\n787\n\n\nWAYNE\n0.014\n1\n0.83\n951\n\n\nSOMERSET\n0.008\n1\n0.82\n399\n\n\nBUTLER\n0.000\n1\n0.56\n400\n\n\nNA\nNA\nNA\nNA\nNA\n\n\nFOREST\nNA\nNA\nNA\nNA\n\n\nFULTON\nNA\nNA\nNA\nNA\n\n\nMONTOUR\nNA\nNA\nNA\nNA\n\n\nSULLIVAN\nNA\nNA\nNA\nNA\n\n\nJUNIATA\nNA\nNA\nNA\nNA\n\n\nPOTTER\nNA\nNA\nNA\nNA\n\n\nWYOMING\nNA\nNA\nNA\nNA\n\n\nSNYDER\nNA\nNA\nNA\nNA\n\n\nELK\nNA\nNA\nNA\nNA\n\n\nGREENE\nNA\nNA\nNA\nNA\n\n\nPERRY\nNA\nNA\nNA\nNA\n\n\nTIOGA\nNA\nNA\nNA\nNA\n\n\nUNION\nNA\nNA\nNA\nNA\n\n\nBEDFORD\nNA\nNA\nNA\nNA\n\n\nMCKEAN\nNA\nNA\nNA\nNA\n\n\nSUSQUEHANNA\nNA\nNA\nNA\nNA\n\n\nCLARION\nNA\nNA\nNA\nNA\n\n\nBRADFORD\nNA\nNA\nNA\nNA\n\n\nCOLUMBIA\nNA\nNA\nNA\nNA\n\n\nCARBON\nNA\nNA\nNA\nNA\n\n\nARMSTRONG\nNA\nNA\nNA\nNA\n\n\nCLEARFIELD\nNA\nNA\nNA\nNA\n\n\nPIKE\nNA\nNA\nNA\nNA\n\n\nADAMS\nNA\nNA\nNA\nNA\n\n\nFRANKLIN\nNA\nNA\nNA\nNA\n\n\nLEBANON\nNA\nNA\nNA\nNA\n\n\nCENTRE\nNA\nNA\nNA\nNA\n\n\nSCHUYLKILL\nNA\nNA\nNA\nNA\n\n\nMONROE\nNA\nNA\nNA\nNA\n\n\nCUMBERLAND\nNA\nNA\nNA\nNA\n\n\nDAUPHIN\nNA\nNA\nNA\nNA\n\n\nBERKS\nNA\nNA\nNA\nNA\n\n\nCHESTER\nNA\nNA\nNA\nNA\n\n\nBUCKS\nNA\nNA\nNA\nNA\n\n\nMONTGOMERY\nNA\nNA\nNA\nNA\n\n\n\n\n\nRequirements:\n\nUse knitr::kable() or similar for formatting\nInclude descriptive column names\nFormat numbers appropriately (commas for population, percentages, etc.)\nAdd an informative caption\nSort by priority (you decide the metric)"
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n# Create county-level access map\n# I use percentage of vulnerable tract here\n\nlibrary(ggplot2)\n\ncounties_stat_sf &lt;- pa_counties %&gt;%\n  select(COUNTY_NAM, geometry) %&gt;%\n  left_join(counties_stat, by = \"COUNTY_NAM\")\n\np1 &lt;- ggplot() +\n  # county base map\n  geom_sf(\n    data = counties_stat_sf,\n    aes(fill = percentage_vuln),\n    color = \"white\", size = 0.2\n  ) +\n  # hospitals\n  geom_sf(\n    data = hospitals_proj,\n    shape = 21, fill = \"red\",\n    size = 1.8, alpha = 0.9\n  ) +\n  # legend\n  scale_fill_viridis_c(\n    name = \"% vulnerable tracts\",\n    labels = label_percent(accuracy = 1),\n    limits = c(0, 1),\n    option = \"C\",      \n    direction = -1\n  ) +\n  coord_sf() +\n  theme_void(base_size = 12) +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text  = element_text(size = 9),\n    plot.title   = element_text(face = \"bold\", size = 16),\n    plot.subtitle= element_text(size = 12)\n  ) +\n  labs(\n    title    = \"Healthcare Access Challenges by County\",\n    subtitle = \"share of vulnerable census tracts\",\n    caption  = \"Source: analysis based on income and percentage of people over 65 years old.\"\n  ) +\n  guides(fill = guide_colorbar(\n    barheight = unit(70, \"pt\"),\n    barwidth  = unit(8,  \"pt\"),\n    ticks.colour = \"black\"\n  ))\n\np1\n\n\n\n\n\n\n\n\nRequirements:\n\nFill counties by percentage of vulnerable tracts that are underserved\nInclude hospital locations as points\nUse an appropriate color scheme\nInclude clear title, subtitle, and caption\nUse theme_void() or similar clean theme\nAdd a legend with formatted labels\n\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Create detailed tract-level map\n\np2 &lt;- ggplot() +\n  # tract base map\n  geom_sf(\n    data = census_tracts,\n    color = \"grey\", size = 0.1\n  ) +\n  # county base map\n  geom_sf(\n    data = pa_counties,\n    color = \"lightblue\", size = 0.5,fill = NA,\n  ) +\n  # hospitals\n  geom_sf(\n    data = hospitals_proj,\n    shape = 16, color = \"red\",\n    size = 1, alpha = 0.9\n  ) +\n  # underserved community\n  geom_sf(\n    data = undeserved,\n    color = \"blue\"\n  )+\n  coord_sf() +\n  theme_void(base_size = 12) +\n  theme(\n    plot.title   = element_text(face = \"bold\", size = 16),\n    plot.subtitle= element_text(size = 12)\n  ) +\n  labs(\n    title    = \"Underserved Communities\",\n    subtitle = \"Blue tracts represent underserved communities; red points show hospital locations.\"\n  ) +\n  guides(fill = guide_colorbar(\n    barheight = unit(70, \"pt\"),\n    barwidth  = unit(8,  \"pt\"),\n    ticks.colour = \"black\"\n  ))\n\np2\n\n\n\n\n\n\n\n#only one very small blue tract in the center of the map\n\nRequirements:\n\nShow underserved vulnerable tracts in a contrasting color\nInclude county boundaries for context\nShow hospital locations\nUse appropriate visual hierarchy (what should stand out?)\nInclude informative title and subtitle\n\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\n\n# Histogram or density plot of distances\nggplot(vulnerable_tracts_proj) +\n  aes(x = dist_to_hospital_mi) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of distance to the nearest hospital\",\n    x = \"distance to the nearest hospital\",\n    y = \"Number of vulnerable tracts\",\n    caption  = \"Most of tracts have distance between 0 and 5.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Box plot comparing distances across regions\nggplot(vulnerable_tracts_proj) +\n  aes(x = NAMELSADCO, y = dist_to_hospital_mi, fill = NAMELSADCO) +\n  geom_boxplot() +\n  labs(\n    title = \"Distances by County Category\",\n    x = \"County\",\n    y = \"Distance\",\n    caption  = \"While most counties show little variation, four counties have huge internal differences\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")+\n  theme(\n  axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 7)\n)\n\n\n\n\n\n\n\n# Bar chart of underserved tracts by county\n#only 1 underserved tracts in my study, it is Census Tract 9601 in the Cameron County\n\n\n# Scatter plot of distance vs. vulnerable population size\nggplot(vulnerable_tracts_proj) +\n  aes(x = dist_to_hospital_mi, y = total_populationE) +\n  geom_point(color = \"steelblue\", alpha = 0.6, size = 2) +\n  geom_smooth(\n    method = \"lm\",              \n    se = TRUE,                  \n    color = \"darkred\",          \n    fill = \"pink\",              \n    linewidth = 1\n  ) +\n  labs(\n    title = \"distance vs. vulnerable population size\",\n    x = \"Distance to the nearest hospital\",\n    y = \"Vulnerable population size\",\n    caption  = str_wrap(\"More vulnerable population, little distance to hospital. Since hospitals are designed to located near population clusters, and we didn't limit this effect in this study \", width = 90)\n  ) +\n  theme_minimal() +\n  scale_x_continuous() +\n  scale_y_continuous(labels = comma)\n\n\n\n\n\n\n\n\nSuggested chart types:\n\nHistogram or density plot of distances\nBox plot comparing distances across regions\nBar chart of underserved tracts by county\nScatter plot of distance vs. vulnerable population size\n\nRequirements:\n\nClear axes labels with units\nAppropriate title\nProfessional formatting\nBrief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\n\nEducation & Youth Services\nOption A: Educational Desert Analysis\n\nData: Schools, Libraries, Recreation Centers, Census tracts (child population)\nQuestion: “Which neighborhoods lack adequate educational infrastructure for children?”\nOperations: Buffer schools/libraries (0.5 mile walking distance), identify coverage gaps, overlay with child population density\nPolicy relevance: School district planning, library placement, after-school program siting\n\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nEnvironmental Justice\nOption C: Green Space Equity\n\nData: Parks, Street Trees, Census tracts (race/income demographics)\nQuestion: “Do low-income and minority neighborhoods have equitable access to green space?”\nOperations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics\nPolicy relevance: Climate resilience, environmental justice, urban forestry investment\n\n\n\nPublic Safety & Justice\nOption D: Crime & Community Resources\n\nData: Crime Incidents, Recreation Centers, Libraries, Street Lights\nQuestion: “Are high-crime areas underserved by community resources?”\nOperations: Aggregate crime counts to census tracts or neighborhoods, count community resources per area, spatial correlation analysis\nPolicy relevance: Community investment, violence prevention strategies\n\n\n\nInfrastructure & Services\nOption E: Polling Place Accessibility\n\nData: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates)\nQuestion: “Are polling places accessible for elderly and disabled voters?”\nOperations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access\nPolicy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\n\nHealth & Wellness\nOption F: Recreation & Population Health\n\nData: Recreation Centers, Playgrounds, Parks, Census tracts (demographics)\nQuestion: “Is lack of recreation access associated with vulnerable populations?”\nOperations: Calculate recreation facilities per capita by neighborhood, buffer facilities for walking access, overlay with demographic indicators\nPolicy relevance: Public health investment, recreation programming, obesity prevention\n\n\n\n\nEmergency Services\nOption G: EMS Response Coverage\n\nData: Fire Stations, EMS stations, Population density, High-rise buildings\nQuestion: “Are population-dense areas adequately covered by emergency services?”\nOperations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas\nPolicy relevance: Emergency preparedness, station siting decisions\n\n\n\n\nArts & Culture\nOption H: Cultural Asset Distribution\n\nData: Public Art, Museums, Historic sites/markers, Neighborhoods\nQuestion: “Do all neighborhoods have equitable access to cultural amenities?”\nOperations: Count cultural assets per neighborhood, normalize by population, compare distribution across demographic groups\nPolicy relevance: Cultural equity, tourism, quality of life, neighborhood identity\n\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/\n\nMost datasets available as GeoJSON, Shapefile, or CSV with coordinates\nAlways check the Metadata for a data dictionary of the fields.\n\nAdditional Sources:\n\nPennsylvania Open Data: https://data.pa.gov/\nCensus Bureau (via tidycensus): Demographics, economic indicators, commute patterns\nTIGER/Line (via tigris): Geographic boundaries\n\n\n\nRecommended Starting Points\nIf you’re feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure:\n\nYou can access the spatial data\nYou can perform at least 2 spatial operations\n\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\n\nlibrary(sf)\n\nschools &lt;- st_read(\"Data/Schools/Schools.shp\")\n\nReading layer `Schools' from data source \n  `/Users/jinhengcen/Documents/portfolio-setup-CenJinHeng/assignments/assignment_2/data/Schools/Schools.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 495 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8378628 ymin: 4852555 xmax: -8345686 ymax: 4884813\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ncrime &lt;- st_read(\"Data/Crime/Crime.shp\")\n\nReading layer `Crime' from data source \n  `/Users/jinhengcen/Documents/portfolio-setup-CenJinHeng/assignments/assignment_2/data/Crime/Crime.shp' \n  using driver `ESRI Shapefile'\nreplacing null geometries with empty geometries\nSimple feature collection with 160388 features and 13 fields (with 6744 geometries empty)\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.50237 ymin: 39.87688 xmax: -74.95784 ymax: 42.22434\nGeodetic CRS:  WGS 84\n\nbike_network &lt;- st_read(\"Data/Bike_Network/Bike_Network.shp\")\n\nReading layer `Bike_Network' from data source \n  `/Users/jinhengcen/Documents/portfolio-setup-CenJinHeng/assignments/assignment_2/data/Bike_Network/Bike_Network.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5225 features and 8 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -8378937 ymin: 4847835 xmax: -8345146 ymax: 4883978\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nschools &lt;- st_transform(schools, 2272)\ncrime &lt;- st_transform(crime, 2272)\nbike_network &lt;- st_transform(bike_network, 2272)\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n## school type\nschools %&gt;%\n  st_drop_geometry() %&gt;%\n  count(type_speci, sort = TRUE) %&gt;%\n  ggplot(aes(x = reorder(type_speci, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  coord_flip() +\n  labs(\n    title = \"Distribution of school types\",\n    x = \"School type\",\n    y = \"Number\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n# Crime Hour\ncrime %&gt;%\n  st_drop_geometry() %&gt;%\n  count(hour) %&gt;%\n  ggplot(aes(x = hour, y = n)) +\n  geom_col(fill = \"firebrick\") +\n  labs(\n    title = \"Distribution of crime hour\",\n    x = \"Hour\",\n    y = \"Number of incidents\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n#crime types\ncrime %&gt;%\n  st_drop_geometry() %&gt;%\n  count(text_gener, sort = TRUE) %&gt;%\n  slice_max(n, n = 10) %&gt;%  \n  ggplot(aes(x = reorder(text_gener, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Crime Types\",\n    x = \"Types\",\n    y = \"Number of incidents\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n# Bike lane\nbike_length_summary &lt;- bike_network %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(TYPE) %&gt;%\n  summarise(total_length = sum(Shape__Len, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_length))\n\nggplot(bike_length_summary, aes(x = reorder(TYPE, total_length), y = total_length)) +\n  geom_bar(stat = \"identity\", fill = \"forestgreen\") +\n  coord_flip() +\n  labs(\n    title = \"Distribution of Bike Lane Type\",\n    x = \"Type\",\n    y = \"Total Length\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nQuestions to answer:\n\nWhat dataset did you choose and why? Three data sets are chosen:\n\nSchools data in shp format\nCrime incidents data points in shp format\nBike_Network in shp format Since I am going to calculate some metrics like the numbers of crime incidents and the length of bike lanes within school buffers, these data are needed.\n\nWhat is the data source and date? They are all from OpenDataPhilly. All of them are up_to_date. The crime incidents data is from 2024, because we are in the October of the 2025, so we don’t have data for the whole 2025. Using 2024 can avoid some time variation.\nHow many features does it contain? All of them have geometry, which means we can run them in spatial analysis. For the school data set, we got names, types, grade levels, phone numbers, street names, and zip codes. For the crime data, we have precise time, crime types, location details. For the bike network data, we knew street names, types, lengths and oneway.\n\nWhat CRS is it in? Did you need to transform it? EPSG:2272, which is used in south Pennsylvania.\n\n\n\nPose a research question\n\nWrite a clear research statement that your analysis will answer.\nExamples:\n\n“Do vulnerable tracts have adequate public transit access to hospitals?”\n“Are EMS stations appropriately located near vulnerable populations?”\n“Do areas with low vehicle access have worse hospital access?”\n\nMy question is “Are school zones safe for walking/biking, or are they crime hotspots?”\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+):\n\nBuffers\nSpatial joins\nSpatial filtering with predicates\nDistance calculations\nIntersections or unions\nPoint-in-polygon aggregation\n\nYour Task:\n\n# Your spatial analysis\n\n# I have loaded data sets and make simple statistical analysis for each of them in the previous chunk\n\n# Create buffer for schools\n\nschools &lt;- schools %&gt;% mutate(school_id = row_number())\nzones &lt;- st_buffer(schools, dist = 1000)\n\n# Calculate the crime number\n\ncrime_by_zone &lt;- crime %&gt;%\n  st_join(zones %&gt;% select(school_id), join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  count(school_id, name = \"crime_count\")\n\n\n# Calculate the bike lane length\nbike_in_zone &lt;- st_intersection(bike_network, zones %&gt;% select(school_id))\n\nbike_len_by_zone &lt;- bike_in_zone %&gt;%\n  mutate(len_ft = as.numeric(st_length(.))) %&gt;% \n  st_drop_geometry() %&gt;%\n  group_by(school_id) %&gt;%\n  summarise(bike_len_ft = sum(len_ft, na.rm = TRUE), .groups = \"drop\")\n\n# Combine the result\nzone_stats &lt;- zones %&gt;%\n  left_join(crime_by_zone,    by = \"school_id\") %&gt;%\n  left_join(bike_len_by_zone, by = \"school_id\")\n\n# Calculate average number of the crime incidents and bike lane length\nzone_summary &lt;- zone_stats %&gt;%\n  st_drop_geometry() %&gt;%\n  summarise(\n    mean_crime = mean(crime_count, na.rm = TRUE),\n    mean_bike_len = mean(bike_len_ft, na.rm = TRUE)\n  )\n\ncity_summary &lt;- tibble(\n  mean_crime = nrow(crime) / nrow(zones),\n  mean_bike_len = sum(st_length(bike_network)) / nrow(zones)\n)\n\n# Combine results\ncomparison &lt;- rbind(\n  data.frame(Category = \"School Zones\", \n             mean_crime = zone_summary$mean_crime, \n             mean_bike_len = zone_summary$mean_bike_len),\n  data.frame(Category = \"Citywide Average\",\n             mean_crime = city_summary$mean_crime,\n             mean_bike_len = city_summary$mean_bike_len)\n)\n\n# Draw diagram\nggplot(comparison, aes(x = Category, y = mean_crime, fill = Category)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  labs(\n    title = \"Comperison of Average Crime Incidents: School Zones VS City\",\n    y = \"Averge Crime Incidents\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nggplot(comparison, aes(x = Category, y = mean_bike_len, fill = Category)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  labs(\n    title = \"Comperison of Bike Lane Length: School Zones VS City\",\n    y = \"Average Bike Lane Length\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n# Mapping\n\n# Get tracts of Philly\ntracts_phl &lt;- get_acs(\n  geography = \"tract\",\n  state     = \"PA\",\n  county    = \"Philadelphia\",\n  variables = \"B01003_001\",\n  survey    = \"acs5\",\n  year      = 2022,\n  geometry  = TRUE,\n  cache_table = TRUE\n) %&gt;%\n  select(GEOID, NAME, pop = estimate, geometry) %&gt;%\n  st_transform(2272)    \n\n# point_on_surface\nzone_points &lt;- zone_stats |&gt;\n  mutate(geometry = st_point_on_surface(geometry),\n         bike_len_mile = bike_len_ft / 5280) |&gt;\n  st_as_sf()\n\n# Crime point map\nggplot() +\n  geom_sf(data = tracts_phl, fill = NA, color = \"grey\", linewidth = 0.2) +\n  geom_sf(data = zone_points, aes(size = crime_count), color = \"firebrick\", alpha = 0.6) +\n  scale_size_continuous(name = \"Crime count\", range = c(0, 4)) +\n  labs(title = \"School Zones: Crime Counts (Point size = count)\", x = NULL, y = NULL) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n# Bike lane point map\nggplot() +\n  geom_sf(data = tracts_phl, fill = NA, color = \"grey\", linewidth = 0.2) +\n  geom_sf(data = zone_points, aes(size = bike_len_mile), color = \"forestgreen\", alpha = 0.6) +\n  scale_size_continuous(name = \"Bike lane (miles)\", range = c(0.1, 2)) +\n  labs(title = \"School Zones: Bike Lane Length (Point size = miles)\", x = NULL, y = NULL) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\nAnalysis requirements:\n\nClear code comments explaining each step\nAppropriate CRS transformations\nSummary statistics or counts\nAt least one map showing your findings\nBrief interpretation of results (3-5 sentences)\n\nYour interpretation:\nSchool zones have fewer crime incidents and bike lane length compared to city average. More pedestrian-friendly projects should be applied to school areas. For the spatial difference, higher concentrations of crimes are observed around schools in central and southern Philadelphia. School zones with longer bike lane coverage are mostly located in the city’s core and western areas. Planning efforts should focus on improving safety in school zones with higher crime risks, especially in central and southern Philadelphia. Expanding connected and protected bike lanes around schools can promote safer and more accessible environments for students."
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTake a few moments to clean up your markdown document and then write a line or two or three about how you may have incorporated feedback that you recieved after your first assignment.\nI need to hide my API Key and printed tables!"
  },
  {
    "objectID": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#submission-requirements",
    "href": "assignments/assignment_2/Cen_Jinheng_Assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "Labs/lab_0.html",
    "href": "Labs/lab_0.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "Labs/lab_0.html#data-structure-exploration",
    "href": "Labs/lab_0.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford…\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"…\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0…\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"…\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, …\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866…\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30…\n\n# Check the column names\nnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer:\n\nHow many rows and columns does the dataset have?\nWhat types of variables do you see (numeric, character, etc.)?\nAre there any column names that might cause problems? Why?\n\nYour answers:\n\nRows: 50,000\nColumns: 7\nVariable types: character, numeric\nProblematic names: Columns like “Engine size”, “Fuel type”, and “Year of manufacture” are problematic, cuz they both have spaces inside their names."
  },
  {
    "objectID": "Labs/lab_0.html#tibble-vs-data-frame",
    "href": "Labs/lab_0.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\ncar_data %&gt;% head(10)\n\n# A tibble: 10 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestion: What differences do you notice in how they print?\nYour answer: data frames display all the rows in the rendering window, while tibbles show the first few rows."
  },
  {
    "objectID": "Labs/lab_0.html#selecting-columns",
    "href": "Labs/lab_0.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\nlibrary(dplyr)\n\n# Select just Model and Mileage columns\ncar_data %&gt;% select (Model, Mileage)\n\n# A tibble: 50,000 × 2\n   Model      Mileage\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 Fiesta      127300\n 2 718 Cayman   57850\n 3 Mondeo       39190\n 4 RAV4        210814\n 5 Polo        127869\n 6 Focus        33603\n 7 Mondeo       86686\n 8 Prius        30663\n 9 Polo         73470\n10 Focus       262514\n# ℹ 49,990 more rows\n\n# Select Manufacturer, Price, and Fuel type\ncar_data %&gt;% select (Manufacturer, Price, `Fuel type`)\n\n# A tibble: 50,000 × 3\n   Manufacturer Price `Fuel type`\n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      \n 1 Ford          3074 Petrol     \n 2 Porsche      49704 Petrol     \n 3 Ford         24072 Diesel     \n 4 Toyota        1705 Hybrid     \n 5 VW            4101 Petrol     \n 6 Ford         29204 Petrol     \n 7 Ford         14350 Diesel     \n 8 Toyota       30297 Hybrid     \n 9 VW            9977 Petrol     \n10 Ford          1049 Diesel     \n# ℹ 49,990 more rows\n\n# Challenge: Select all columns EXCEPT Engine Size\ncar_data %&gt;% select (-`Engine size`)\n\n# A tibble: 50,000 × 6\n   Manufacturer Model      `Fuel type` `Year of manufacture` Mileage Price\n   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta     Petrol                       2002  127300  3074\n 2 Porsche      718 Cayman Petrol                       2016   57850 49704\n 3 Ford         Mondeo     Diesel                       2014   39190 24072\n 4 Toyota       RAV4       Hybrid                       1988  210814  1705\n 5 VW           Polo       Petrol                       2006  127869  4101\n 6 Ford         Focus      Petrol                       2018   33603 29204\n 7 Ford         Mondeo     Diesel                       2010   86686 14350\n 8 Toyota       Prius      Hybrid                       2015   30663 30297\n 9 VW           Polo       Petrol                       2012   73470  9977\n10 Ford         Focus      Diesel                       1992  262514  1049\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "Labs/lab_0.html#renaming-columns",
    "href": "Labs/lab_0.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\ncar_data &lt;- car_data %&gt;% rename(year = `Year of manufacture`)\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"Engine size\"  \"Fuel type\"    \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: There are spaces in Year of manufacture."
  },
  {
    "objectID": "Labs/lab_0.html#calculate-car-age",
    "href": "Labs/lab_0.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\ncar_data &lt;- car_data %&gt;% mutate(age = 2025 - year)\n\n# Create a mileage_per_year column  \ncar_data &lt;- car_data %&gt;% mutate(mileage_per_year = Mileage/age)\n\n# Look at your new columns\ncar_data %&gt;% select(Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 × 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "Labs/lab_0.html#categorize-cars",
    "href": "Labs/lab_0.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is mid-range (use case_when)\ncar_data &lt;- car_data %&gt;% mutate(\n  price_category = case_when(\n    Price &lt; 15000 ~ \"budget\",\n    Price &gt; 15000 & Price &lt; 30000 ~ \"midrange\",\n    Price &gt; 30000 ~ \"mid-range\"\n  )\n)\n\n# Check your categories select the new column and show it\ncar_data %&gt;% select(price_category)\n\n# A tibble: 50,000 × 1\n   price_category\n   &lt;chr&gt;         \n 1 budget        \n 2 mid-range     \n 3 midrange      \n 4 budget        \n 5 budget        \n 6 midrange      \n 7 budget        \n 8 mid-range     \n 9 budget        \n10 budget        \n# ℹ 49,990 more rows"
  },
  {
    "objectID": "Labs/lab_0.html#basic-filtering",
    "href": "Labs/lab_0.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\ncar_data %&gt;% filter( Manufacturer == \"Toyota\")\n\n# A tibble: 12,554 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 4 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 5 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n 6 Toyota       Yaris           1.2 Petrol       1992  245990   720    33\n 7 Toyota       RAV4            2   Hybrid       2018   28381 52671     7\n 8 Toyota       Prius           1   Hybrid       2003  115291  6512    22\n 9 Toyota       Prius           1   Hybrid       1990  238571   961    35\n10 Toyota       Prius           1.8 Hybrid       2017   31958 38961     8\n# ℹ 12,544 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with mileage less than 30,000\ncar_data %&gt;% filter( Mileage &lt; 30000)\n\n# A tibble: 5,402 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 Ford         Mondeo               1.6 Diesel       2015   21834 28435    10\n 9 VW           Passat               1.6 Diesel       2018   22122 36634     7\n10 VW           Passat               1.4 Diesel       2020   21413 39310     5\n# ℹ 5,392 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find luxury cars (from price category) with low mileage\ncar_data %&gt;% filter( price_category == \"mid-range\" & Mileage &lt; 30000)\n\n# A tibble: 3,256 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 VW           Passat               1.6 Diesel       2018   22122 36634     7\n 9 VW           Passat               1.4 Diesel       2020   21413 39310     5\n10 Toyota       RAV4                 2.4 Petrol       2021    6829 66031     4\n# ℹ 3,246 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "Labs/lab_0.html#multiple-conditions",
    "href": "Labs/lab_0.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Toyota OR VW\ncar_data %&gt;% filter ( Manufacturer == \"Toyota\"|Manufacturer == \"VW\")\n\n# A tibble: 27,467 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 VW           Polo            1   Petrol       2006  127869  4101    19\n 3 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 4 VW           Polo            1.2 Petrol       2012   73470  9977    13\n 5 VW           Golf            2   Diesel       2014   83047 17173    11\n 6 VW           Golf            1.2 Diesel       2007   92697  7792    18\n 7 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 8 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 9 VW           Golf            1.6 Diesel       1989  222390   933    36\n10 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n# ℹ 27,457 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with price between $20,000 and $35,000\ncar_data %&gt;% filter (Price &gt; 20000 & Price &lt; 35000)\n\n# A tibble: 7,301 × 10\n   Manufacturer Model  `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Mondeo           1.6 Diesel       2014   39190 24072    11\n 2 Ford         Focus            1.4 Petrol       2018   33603 29204     7\n 3 Toyota       Prius            1.4 Hybrid       2015   30663 30297    10\n 4 Toyota       Prius            1.4 Hybrid       2016   43893 29946     9\n 5 Toyota       Prius            1.4 Hybrid       2016   43130 30085     9\n 6 VW           Passat           1.6 Petrol       2016   64344 23641     9\n 7 Ford         Mondeo           1.6 Diesel       2015   21834 28435    10\n 8 BMW          M5               4.4 Petrol       2008  109941 31711    17\n 9 BMW          Z4               2.2 Petrol       2014   61332 26084    11\n10 Porsche      911              3.5 Petrol       2003  107705 24378    22\n# ℹ 7,291 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find diesel cars less than 10 years old\ncar_data %&gt;% filter (age &lt; 10 & `Fuel type` == \"diesel\")\n\n# A tibble: 0 × 10\n# ℹ 10 variables: Manufacturer &lt;chr&gt;, Model &lt;chr&gt;, Engine size &lt;dbl&gt;,\n#   Fuel type &lt;chr&gt;, year &lt;dbl&gt;, Mileage &lt;dbl&gt;, Price &lt;dbl&gt;, age &lt;dbl&gt;,\n#   mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 0"
  },
  {
    "objectID": "Labs/lab_0.html#basic-summaries",
    "href": "Labs/lab_0.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\navg_mileage_by_fuel &lt;- car_data %&gt;%\n  group_by(`Fuel type`) %&gt;%\n  summarize(avg_mileage = mean(Mileage, na.rm = TRUE))\n\navg_mileage_by_fuel\n\n# A tibble: 3 × 2\n  `Fuel type` avg_mileage\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Diesel          112667.\n2 Hybrid          111622.\n3 Petrol          112795.\n\n# Count cars by manufacturer\ncount_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarise(num_cars = n())\n\ncount_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer num_cars\n  &lt;chr&gt;           &lt;int&gt;\n1 BMW              4965\n2 Ford            14959\n3 Porsche          2609\n4 Toyota          12554\n5 VW              14913"
  },
  {
    "objectID": "Labs/lab_0.html#categorical-summaries",
    "href": "Labs/lab_0.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories\nFrequency_price_categories &lt;- car_data %&gt;%\n  group_by(price_category) %&gt;%\n  summarize(num_car = n())\n\nFrequency_price_categories\n\n# A tibble: 4 × 2\n  price_category num_car\n  &lt;chr&gt;            &lt;int&gt;\n1 budget           34040\n2 mid-range         6178\n3 midrange          9779\n4 &lt;NA&gt;                 3"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html",
    "href": "assignments/assignment_1/assignment_1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the New Jersey Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#scenario",
    "href": "assignments/assignment_1/assignment_1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the New Jersey Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#learning-objectives",
    "href": "assignments/assignment_1/assignment_1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#submission-instructions",
    "href": "assignments/assignment_1/assignment_1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#data-retrieval",
    "href": "assignments/assignment_1/assignment_1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",   # Median household income\n    total_population = \"B01003_001\"       # Total pop\n  ),\n  state = my_state,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\ncounty_data &lt;- county_data %&gt;%\n  mutate(county_name = NAME %&gt;% \n           str_remove(\" County, New Jersey\")\n         )\n\n# Display the first few rows\nhead(county_data,10)\n\n# A tibble: 10 × 7\n   GEOID NAME  median_incomeE median_incomeM total_populationE total_populationM\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n 1 34001 Atla…          73113           1917            274339                NA\n 2 34003 Berg…         118714           1607            953243                NA\n 3 34005 Burl…         102615           1436            461853                NA\n 4 34007 Camd…          82005           1414            522581                NA\n 5 34009 Cape…          83870           3707             95456                NA\n 6 34011 Cumb…          62310           2205            153588                NA\n 7 34013 Esse…          73785           1477            853374                NA\n 8 34015 Glou…          99668           2605            302621                NA\n 9 34017 Huds…          86854           1782            712029                NA\n10 34019 Hunt…         133534           3236            129099                NA\n# ℹ 1 more variable: county_name &lt;chr&gt;"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#data-quality-assessment",
    "href": "assignments/assignment_1/assignment_1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\ncounty_data &lt;- county_data %&gt;%\n  mutate(median_incomeMOE = (median_incomeM / median_incomeE) * 100,\n        rel_categories = case_when(\n            median_incomeMOE &lt; 5 ~ \"High Confidence\",\n            median_incomeMOE &gt;= 5 & median_incomeMOE &lt; 10 ~ \"Moderate Confidence\",\n            median_incomeMOE &gt;= 10 ~ \"Low Confidence\"\n        )\n  )\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\nsummary_data &lt;- county_data %&gt;%\n  count(rel_categories) %&gt;%\n  mutate(percentage = n/sum(n))"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#high-uncertainty-counties",
    "href": "assignments/assignment_1/assignment_1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n#?arrange\n#?select\n#glimpse(county_data)\n\ntop_5 &lt;- county_data %&gt;%\n  arrange(desc(median_incomeMOE)) %&gt;%\n  slice_head( n = 5 ) %&gt;%\n  select(county_name, median_incomeE, median_incomeM, median_incomeMOE, rel_categories)\n\ntop_5\n\n# A tibble: 5 × 5\n  county_name median_incomeE median_incomeM median_incomeMOE rel_categories \n  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;          \n1 Cape May             83870           3707             4.42 High Confidence\n2 Salem                73378           3047             4.15 High Confidence\n3 Cumberland           62310           2205             3.54 High Confidence\n4 Atlantic             73113           1917             2.62 High Confidence\n5 Gloucester           99668           2605             2.61 High Confidence\n\n# Format as table with kable() - include appropriate column names and caption\n#?kable\nkable(top_5,digits = getOption(\"digits\"), caption = \"Top 5 counties in NJ with the highest median income MOE\")\n\n\nTop 5 counties in NJ with the highest median income MOE\n\n\n\n\n\n\n\n\n\ncounty_name\nmedian_incomeE\nmedian_incomeM\nmedian_incomeMOE\nrel_categories\n\n\n\n\nCape May\n83870\n3707\n4.419936\nHigh Confidence\n\n\nSalem\n73378\n3047\n4.152471\nHigh Confidence\n\n\nCumberland\n62310\n2205\n3.538758\nHigh Confidence\n\n\nAtlantic\n73113\n1917\n2.621969\nHigh Confidence\n\n\nGloucester\n99668\n2605\n2.613677\nHigh Confidence\n\n\n\n\n\nData Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]\nCounties with high MOE are not suitable for consideration using algorithmic decision-making, as the margin errors are substantial, which means some important details could be lost in the calculation process. The result of the algorithm could be unreliable and can not convince people. For this income data in New Jersey, Cape May is not a good choice. Factors contributing to the high uncertainty could include income variance across different groups, sample sizes, and data collecting methods."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#focus-area-selection",
    "href": "assignments/assignment_1/assignment_1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# NOTE: Since all the NJ counties are High Confidence, I chose the highest and lowest ones, combined with a moderate one\nselected_counties &lt;- county_data %&gt;%\n  filter(county_name == \"Cape May\"|county_name == \"Bergen\"|county_name == \"Union\")\n\nselected_counties\n\n# A tibble: 3 × 9\n  GEOID NAME   median_incomeE median_incomeM total_populationE total_populationM\n  &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1 34003 Berge…         118714           1607            953243                NA\n2 34009 Cape …          83870           3707             95456                NA\n3 34039 Union…          95000           2210            572079                NA\n# ℹ 3 more variables: county_name &lt;chr&gt;, median_incomeMOE &lt;dbl&gt;,\n#   rel_categories &lt;chr&gt;\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nselected_counties &lt;- selected_counties %&gt;%\n  select(county_name, median_incomeE, median_incomeMOE, rel_categories)\nkable(selected_counties,digits = getOption(\"digits\"), caption = \"Selected Counties\")\n\n\nSelected Counties\n\n\ncounty_name\nmedian_incomeE\nmedian_incomeMOE\nrel_categories\n\n\n\n\nBergen\n118714\n1.353673\nHigh Confidence\n\n\nCape May\n83870\n4.419936\nHigh Confidence\n\n\nUnion\n95000\n2.326316\nHigh Confidence\n\n\n\n\n\nComment on the output: It appears that the median income level and the MOE are somewhat correlated, the higher the median income, the lower the MOE."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#tract-level-demographics",
    "href": "assignments/assignment_1/assignment_1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nvars = c(\n    white = \"B03002_003\",   \n    Black = \"B03002_004\",\n    Hispanic = \"B03002_012\",\n    total_population = \"B03002_001\"\n  )\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\ntract_data &lt;- get_acs(\n  geography = \"tract\",\n  variables = vars,\n  state = my_state,\n  county = c(\"003\", \"009\", \"039\"),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\ntract_data &lt;- tract_data %&gt;%\n  mutate(\n    white_percentage = (whiteE / total_populationE)*100,\n    black_percentage = (BlackE / total_populationE)*100,\n    his_percentage = (HispanicE / total_populationE)*100\n  )\n\n# Add readable tract and county name columns using str_extract() or similar\n#?str_extract()\ntract_data &lt;- tract_data %&gt;%\n  mutate(\n    tract_name  = str_trim(str_split_fixed(NAME, \";\", 3)[,1]),\n    county_name = str_trim(str_remove(str_split_fixed(NAME, \";\", 3)[,2], \" County\"))\n  )"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#demographic-analysis",
    "href": "assignments/assignment_1/assignment_1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\ntop_his_tract &lt;- tract_data %&gt;%\n  arrange(desc(his_percentage)) %&gt;%\n  slice_head(n = 1)\n\ntop_his_tract\n\n# A tibble: 1 × 15\n  GEOID  NAME  whiteE whiteM BlackE BlackM HispanicE HispanicM total_populationE\n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt;\n1 34039… Cens…    218    120    310    219      2935       624              3483\n# ℹ 6 more variables: total_populationM &lt;dbl&gt;, white_percentage &lt;dbl&gt;,\n#   black_percentage &lt;dbl&gt;, his_percentage &lt;dbl&gt;, tract_name &lt;chr&gt;,\n#   county_name &lt;chr&gt;\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\nsummary_data_2 &lt;- tract_data %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    num_tract = n(),\n    avg_white_p = mean(white_percentage, na.rm = TRUE),\n    avg_black_p = mean(black_percentage, na.rm = TRUE),\n    avg_his_p = mean(his_percentage, na.rm = TRUE)\n  )\n\n# Create a nicely formatted table of your results using kable()\nkable(summary_data_2,digits = getOption(\"digits\"), caption = \"Average percentage of races\")\n\n\nAverage percentage of races\n\n\ncounty_name\nnum_tract\navg_white_p\navg_black_p\navg_his_p\n\n\n\n\nBergen\n203\n53.08512\n5.390523\n21.595950\n\n\nCape May\n33\n86.01229\n2.830430\n7.664504\n\n\nUnion\n120\n35.83627\n20.277607\n34.532339"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment_1/assignment_1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\ntract_data &lt;- tract_data %&gt;%\n  mutate(\n    white_MOE = (whiteM / whiteE)*100,\n    black_MOE = (BlackM / BlackE)*100,\n    his_MOE = (HispanicM / HispanicE)*100,\n  )\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\ntract_data &lt;- tract_data %&gt;%\n  mutate(\n    flag = if_else(\n      white_MOE &gt; 15 | black_MOE &gt; 15 | his_MOE &gt; 15,\n      \"Flag\",\n      \"Moderate\"\n    )\n  )\n\n# Create summary statistics showing how many tracts have data quality issues\nnum_flag &lt;- tract_data %&gt;%\n  filter(flag == \"Flag\") %&gt;%\n  summarise(\n    num = n()\n  ) #356"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#pattern-analysis",
    "href": "assignments/assignment_1/assignment_1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\n#NOTE: I chose white alone and calculated the white_flag, cuz all tracts are flagged in the previous step \n\ntract_data &lt;- tract_data %&gt;%\n  mutate(\n    flag_white = if_else(\n      white_MOE &gt; 15,\n      \"Flag\",\n      \"Moderate\"\n    )\n  )\n\nsummary_data_3 &lt;- tract_data %&gt;%\n  group_by(flag_white) %&gt;%\n  summarise(\n    num = n(),\n    avg_pop = mean(total_populationE, na.rm = TRUE),\n    avg_white_p = mean(white_percentage, na.rm = TRUE),\n  )\n\nkable(summary_data_3,digits = getOption(\"digits\"), caption = \"summary of flagged and unflaged group\")\n\n\nsummary of flagged and unflaged group\n\n\nflag_white\nnum\navg_pop\navg_white_p\n\n\n\n\nFlag\n256\n4440.484\n40.63099\n\n\nModerate\n100\n4840.140\n74.68121\n\n\n\n\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?] It seems like that a community with large population and high concentration of certain kind of people are likely to be not flagged. This can be explained by the sample size that MOE would be smaller if the sample size goes larger."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment_1/assignment_1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses?\n\nEquity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings?\nRoot Cause Analysis: What underlying factors drive both data quality issues and bias risk?\nStrategic Recommendations: What should the Department implement to address these systematic issues?\n\nExecutive Summary:\nIn this study, I used MOE percentages as key metrics. I divided the large dataset into several groups to see if there were differences between them. I aim to identify counties or communities that are not suitable for data-driven decision-making processes in economic and racial aspects.\nMy findings are: communities with smaller population size are in the risk of algorithmic bias, what’s more, diverse communities with different races and various groups of people have higher MOE percentage. Economically disadvantaged communities with income inequality are also liked to have the bias issue.\nThe reasons for the data quality issues and bias risk can be: (1) Sample size. Smaller populations usually have higher sample error and missing data. Minority are hard to reach out by the census survey as well. (2) Socioeconomic and Demographic variance. Diverse communties have large internal variation, which leads to higher marginal errors.\nDepartments should not just rely on the algorithm to make their decisions. On-site investigation and community engagement is crucial to know the truth. There are stories, knowledge, and so on that can not reflect from numbers. Planners should be critical about the results of data-driven analysis."
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#specific-recommendations",
    "href": "assignments/assignment_1/assignment_1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\nsummary_county &lt;- county_data %&gt;%\n  select(county_name,median_incomeE,median_incomeMOE,rel_categories)\n\nsummary_county\n\n# A tibble: 21 × 4\n   county_name median_incomeE median_incomeMOE rel_categories \n   &lt;chr&gt;                &lt;dbl&gt;            &lt;dbl&gt; &lt;chr&gt;          \n 1 Atlantic             73113             2.62 High Confidence\n 2 Bergen              118714             1.35 High Confidence\n 3 Burlington          102615             1.40 High Confidence\n 4 Camden               82005             1.72 High Confidence\n 5 Cape May             83870             4.42 High Confidence\n 6 Cumberland           62310             3.54 High Confidence\n 7 Essex                73785             2.00 High Confidence\n 8 Gloucester           99668             2.61 High Confidence\n 9 Hudson               86854             2.05 High Confidence\n10 Hunterdon           133534             2.42 High Confidence\n# ℹ 11 more rows\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\nsummary_county &lt;- summary_county %&gt;%\n  mutate(\n    recommendation = \n  case_when(\n    rel_categories == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n    rel_categories == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n    rel_categories == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n  )\n  )\n\n# Format as a professional table with kable()\n\nkable(summary_county,digits = getOption(\"digits\"), caption = \"Recommendation for algorithmic decisions\")\n\n\nRecommendation for algorithmic decisions\n\n\n\n\n\n\n\n\n\ncounty_name\nmedian_incomeE\nmedian_incomeMOE\nrel_categories\nrecommendation\n\n\n\n\nAtlantic\n73113\n2.621969\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBergen\n118714\n1.353673\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBurlington\n102615\n1.399406\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCamden\n82005\n1.724285\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCape May\n83870\n4.419936\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCumberland\n62310\n3.538758\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEssex\n73785\n2.001762\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGloucester\n99668\n2.613677\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHudson\n86854\n2.051719\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHunterdon\n133534\n2.423353\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMercer\n92697\n2.325857\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMiddlesex\n105206\n1.461894\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMonmouth\n118527\n1.605541\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMorris\n130808\n2.084735\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOcean\n82379\n1.766227\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPassaic\n84465\n1.851654\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSalem\n73378\n4.152471\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSomerset\n131948\n2.485828\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSussex\n111094\n2.460979\nHigh Confidence\nSafe for algorithmic decisions\n\n\nUnion\n95000\n2.326316\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWarren\n92620\n2.598791\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: Low MOE group (≈ 1.85 – 1.35) Counties: Passaic, Ocean, Camden, Monmouth, Middlesex, Burlington, Bergen\nCounties requiring additional oversight: Mid MOE group (≈ 2.46 – 2.00) Counties: Sussex, Hunterdon, Union, Mercer, Morris, Hudson, Essex\nCounties needing alternative approaches: High MOE group (≈ 4.42 – 2.60) Counties: Cape May, Salem, Cumberland, Atlantic, Gloucester, Warren, Somerset"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#questions-for-further-investigation",
    "href": "assignments/assignment_1/assignment_1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nDo counties with higher margins of error cluster geographically?\nDo certain counties show persistently high uncertainty?\nAre higher MOEs systematically associated with specific demographic factors?"
  },
  {
    "objectID": "assignments/assignment_1/assignment_1.html#submission-checklist",
    "href": "assignments/assignment_1/assignment_1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\n[✅] All code chunks run without errors\n[✅ All “[Fill this in]” prompts have been completed\n[✅] Tables are properly formatted and readable\n[✅] Executive summary addresses all four required components\n[✅] Portfolio navigation includes this assignment\n[✅] Census API key is properly set\n[✅] Document renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html",
    "title": "Predictive Policing - Technical Implementation",
    "section": "",
    "text": "In this exercise, you will build a spatial predictive model for burglaries using count regression and spatial features."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#about-this-exercise",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#about-this-exercise",
    "title": "Predictive Policing - Technical Implementation",
    "section": "",
    "text": "In this exercise, you will build a spatial predictive model for burglaries using count regression and spatial features."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-1.1-load-chicago-spatial-data",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-1.1-load-chicago-spatial-data",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 1.1: Load Chicago Spatial Data",
    "text": "Exercise 1.1: Load Chicago Spatial Data\n\n\nCode\n# Load police districts (used for spatial cross-validation)\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(District = dist_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 25 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load police beats (smaller administrative units)\npoliceBeats &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(Beat = beat_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 277 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\") %&gt;%\n  st_transform('ESRI:102271')\n\n\nReading layer `chicagoBoundary' from data source \n  `https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -87.8367 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304\nGeodetic CRS:  WGS 84\n\n\nCode\ncat(\"✓ Loaded spatial boundaries\\n\")\n\n\n✓ Loaded spatial boundaries\n\n\nCode\ncat(\"  - Police districts:\", nrow(policeDistricts), \"\\n\")\n\n\n  - Police districts: 25 \n\n\nCode\ncat(\"  - Police beats:\", nrow(policeBeats), \"\\n\")\n\n\n  - Police beats: 277 \n\n\n\n\n\n\n\n\nNoteCoordinate Reference System\n\n\n\nWe’re using ESRI:102271 (Illinois State Plane East, NAD83, US Feet). This is appropriate for Chicago because:\n\nIt minimizes distortion in this region\nUses feet (common in US planning)\nAllows accurate distance calculations"
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-1.2-load-burglary-data",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-1.2-load-burglary-data",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 1.2: Load Burglary Data",
    "text": "Exercise 1.2: Load Burglary Data\n\n\nCode\n# Load from provided data file (downloaded from Chicago open data portal)\nburglaries &lt;- st_read(\"data/burglaries.shp\") %&gt;% \n  st_transform('ESRI:102271')\n\n\nReading layer `burglaries' from data source \n  `/Users/jinhengcen/Documents/25fall/PPA/portfolio-setup-CenJinHeng/assignments/assignment_4/data/burglaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7482 features and 22 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 340492 ymin: 552959.6 xmax: 367153.5 ymax: 594815.1\nProjected CRS: NAD83(HARN) / Illinois East\n\n\nCode\n# Check the data\ncat(\"\\n✓ Loaded burglary data\\n\")\n\n\n\n✓ Loaded burglary data\n\n\nCode\ncat(\"  - Number of burglaries:\", nrow(burglaries), \"\\n\")\n\n\n  - Number of burglaries: 7482 \n\n\nCode\ncat(\"  - CRS:\", st_crs(burglaries)$input, \"\\n\")\n\n\n  - CRS: ESRI:102271 \n\n\nCode\ncat(\"  - Date range:\", min(burglaries$date, na.rm = TRUE), \"to\", \n    max(burglaries$date, na.rm = TRUE), \"\\n\")\n\n\n  - Date range: Inf to -Inf \n\n\nQuestion 1.1: How many burglaries are in the dataset? What time period does this cover? Why does the coordinate reference system matter for our spatial analysis?\n\nThere are 152664 burglaries in my data set (19734 in 2017). I chose Sanitation Code Complaints.\nThis data set covers period between 2000 and 2018.\nBecause the Earth is curved in the reality, yet we attempt to analyze space on a flat xy plane. We use projection to define this transformation process. Due to differences in projection methods, the data points also vary in location. Therefore, we must employ a consistent projection method to ensure the accuracy of data positioning. We also need to determine the appropriate projection method based on different geographic locations and the requirements for calculating distances.\n\n\n\n\n\n\n\nWarningCritical Pause #1: Data Provenance\n\n\n\nBefore proceeding, consider where this data came from:\nWho recorded this data? Chicago Police Department officers and detectives\nWhat might be missing?\n\nUnreported burglaries (victims didn’t call police)\nIncidents police chose not to record\nDowngraded offenses (burglary recorded as trespassing)\nSpatial bias (more patrol = more recorded crime)\n\nThink About Was there a Department of Justice investigation of CPD during this period? What did they find about data practices?"
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-1.3-visualize-point-data",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-1.3-visualize-point-data",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 1.3: Visualize Point Data",
    "text": "Exercise 1.3: Visualize Point Data\n\n\nCode\n# Simple point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = burglaries, color = \"#d62828\", size = 0.1, alpha = 0.4) +\n  labs(\n    title = \"Burglary Locations\",\n    subtitle = paste0(\"Chicago 2017, n = \", nrow(burglaries))\n  )\n\n# Density surface using modern syntax\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(burglaries)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"  # Modern ggplot2 syntax (not guide = FALSE)\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Kernel density estimation\"\n  )\n\n# Combine plots using patchwork (modern approach)\np1 + p2 + \n  plot_annotation(\n    title = \"Spatial Distribution of Burglaries in Chicago\",\n    tag_levels = 'A'\n  )\n\n\n\n\n\n\n\n\n\nQuestion 1.2: What spatial patterns do you observe? Are burglaries evenly distributed across Chicago? Where are the highest concentrations? What might explain these patterns?\n\nBurglaries are not evenly distributed. There are two clusters in the density map. Lincoln Square and South Side are the highest concentrations (told from Google map). Since I am not familiar with Chicago, I just assumed that these are suburban neighbors that have lower revenue that leads to poorly maintenance, according to “broken window theory”, there are likely more burglaries."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-2.1-understanding-the-fishnet",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-2.1-understanding-the-fishnet",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 2.1: Understanding the Fishnet",
    "text": "Exercise 2.1: Understanding the Fishnet\nA fishnet grid converts irregular point data into a regular grid of cells where we can:\n\nAggregate counts\nCalculate spatial features\nApply regression models\n\nThink of it as overlaying graph paper on a map.\n\n\nCode\n# Create 500m x 500m grid\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,  # 500 meters per cell\n  square = TRUE\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Keep only cells that intersect Chicago\nfishnet &lt;- fishnet[chicagoBoundary, ]\n\n# View basic info\ncat(\"✓ Created fishnet grid\\n\")\n\n\n✓ Created fishnet grid\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet), \"\\n\")\n\n\n  - Number of cells: 2458 \n\n\nCode\ncat(\"  - Cell size:\", 500, \"x\", 500, \"meters\\n\")\n\n\n  - Cell size: 500 x 500 meters\n\n\nCode\ncat(\"  - Cell area:\", round(st_area(fishnet[1,])), \"square meters\\n\")\n\n\n  - Cell area: 250000 square meters\n\n\nQuestion 2.1: Why do we use a regular grid instead of existing boundaries like neighborhoods or census tracts? What are the advantages and disadvantages of this approach?\n\nWe need fishnet to covert data points into areas that can be calculated for our modeling\nFishnet has a consistent size, so there are no boundary bias. It is a standard approach commonly used in policy analysis. Additionally, It is easy to create and aggregate point data. Howerver, it might split natural areas."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-2.2-aggregate-burglaries-to-grid",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-2.2-aggregate-burglaries-to-grid",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 2.2: Aggregate Burglaries to Grid",
    "text": "Exercise 2.2: Aggregate Burglaries to Grid\n\n\nCode\n# Spatial join: which cell contains each burglary?\nburglaries_fishnet &lt;- st_join(burglaries, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(countBurglaries = n())\n\n# Join back to fishnet (cells with 0 burglaries will be NA)\nfishnet &lt;- fishnet %&gt;%\n  left_join(burglaries_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(countBurglaries = replace_na(countBurglaries, 0))\n\n# Summary statistics\ncat(\"\\nBurglary count distribution:\\n\")\n\n\n\nBurglary count distribution:\n\n\nCode\nsummary(fishnet$countBurglaries)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   3.042   5.000  40.000 \n\n\nCode\ncat(\"\\nCells with zero burglaries:\", \n    sum(fishnet$countBurglaries == 0), \n    \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countBurglaries == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero burglaries: 781 / 2458 ( 31.8 %)\n\n\n\n\nCode\n# Visualize aggregated counts\nggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Burglaries\",\n    option = \"plasma\",\n    trans = \"sqrt\",  # Square root for better visualization of skewed data\n    breaks = c(0, 1, 5, 10, 20, 40)\n  ) +\n  labs(\n    title = \"Burglary Counts by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_crime()\n\n\n\n\n\n\n\n\n\nQuestion 2.2: What is the distribution of burglary counts across cells? Why do so many cells have zero burglaries? Is this distribution suitable for count regression? (Hint: look up overdispersion)\n\nCells with high amount concentrated in nearby areas, reflecting the first law of geography.\nReasons explaining cells that are zero:\n\nThey are in natural areas like parks, rivers, where few burglaries happened\nData might be missed due to errors in data collecting, cleaning, transforming process\n\nOur assumption is that variance = mean, however, the reality is that Variance &gt; Mean, cuz we have a lot cells counted zero, making the distribution not suitable for our regression model."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-4.1-load-311-abandoned-vehicle-calls",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-4.1-load-311-abandoned-vehicle-calls",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 4.1: Load 311 Abandoned Vehicle Calls",
    "text": "Exercise 4.1: Load 311 Abandoned Vehicle Calls\n\n\nCode\nabandoned_building &lt;- read_csv(\"data/311_calls.csv\")%&gt;%\n  filter(!is.na(LATITUDE), !is.na(LONGITUDE)) %&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102271')\n\ncat(\"✓ Loaded abandoned vehicle calls\\n\")\n\n\n✓ Loaded abandoned vehicle calls\n\n\nCode\ncat(\"  - Number of calls:\", nrow(abandoned_building), \"\\n\")\n\n\n  - Number of calls: 65073 \n\n\n\n\n\n\n\n\nNoteData Loading Note\n\n\n\nThe data was downloaded from Chicago’s Open Data Portal. You can now request an api from the Chicago portal and tap into the data there.\nConsider: How might the 311 reporting system itself be biased? Who calls 311? What neighborhoods have better 311 awareness?"
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-4.2-count-of-abandoned-cars-per-cell",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-4.2-count-of-abandoned-cars-per-cell",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 4.2: Count of Abandoned Cars per Cell",
    "text": "Exercise 4.2: Count of Abandoned Cars per Cell\n\n\nCode\n# Aggregate abandoned car calls to fishnet\nabandoned_fishnet &lt;- st_join(abandoned_building, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(abandoned_building = n())\n\n# Join to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(abandoned_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(abandoned_building = replace_na(abandoned_building, 0))\n\ncat(\"Abandoned car distribution:\\n\")\n\n\nAbandoned car distribution:\n\n\nCode\nsummary(fishnet$abandoned_building)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    1.00    7.00   26.47   28.00  378.00 \n\n\n\n\nCode\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abandoned_building), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"magma\") +\n  labs(title = \"Abandoned Buildings 311 Calls\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\") +\n  labs(title = \"Burglaries\") +\n  theme_crime()\n\np1 + p2 +\n  plot_annotation(title = \"Are abandoned buildings and burglaries correlated?\")\n\n\n\n\n\n\n\n\n\nQuestion 4.1: Do you see a visual relationship between abandoned buildings and burglaries? What does this suggest?\n\nSpatial pattern is not perfectly matched in these two maps: Clusters are not matched between these two maps — There are four clusters of abandoned buildings, particularly concentrated in the south-central area, while residential burglaries are more widely dispersed across the southern and northern suburbs.\nThese maps suggest that abandoned buildings and burglaries might be strongly correlated, which should be alarmed for our future model building."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-4.3-nearest-neighbor-features",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-4.3-nearest-neighbor-features",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 4.3: Nearest Neighbor Features",
    "text": "Exercise 4.3: Nearest Neighbor Features\nCount in a cell is one measure. Distance to the nearest 3 abandoned cars captures local context.\n\n\nCode\n# Calculate mean distance to 3 nearest abandoned cars\n# (Do this OUTSIDE of mutate to avoid sf conflicts)\n\n# Get coordinates\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\nabandoned_coords &lt;- st_coordinates(abandoned_building)\n\n# Calculate k nearest neighbors and distances\nnn_result &lt;- get.knnx(abandoned_coords, fishnet_coords, k = 3)\n\n# Add to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    abandoned_building.nn = rowMeans(nn_result$nn.dist)\n  )\n\ncat(\"✓ Calculated nearest neighbor distances\\n\")\n\n\n✓ Calculated nearest neighbor distances\n\n\nCode\nsummary(fishnet$abandoned_building.nn)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n   0.8906   78.5322  154.3599  261.9420  313.4905 1997.1337 \n\n\nQuestion 4.2: What does a low value of abandoned_building.nn mean? A high value? Why might this be informative?\n\nabandoned_building.nn showing mean distance to 3 nearest abandoned buildings. A low value reflects that abandoned buildings are close to each other, while a high value shows that they are far and more spread. This number can give us a simple sense of the distribution rule of our data."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-4.4-distance-to-hot-spots",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-4.4-distance-to-hot-spots",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 4.4: Distance to Hot Spots",
    "text": "Exercise 4.4: Distance to Hot Spots\nLet’s identify clusters of abandoned cars using Local Moran’s I, then calculate distance to these hot spots.\n\n\nCode\n# Function to calculate Local Moran's I\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  \n  # Create spatial weights\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  # Calculate Local Moran's I\n  local_moran &lt;- localmoran(data[[variable]], weights)\n  \n  # Classify clusters\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data %&gt;%\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      \n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n# Apply to abandoned building\nfishnet &lt;- calculate_local_morans(fishnet, \"abandoned_building\", k = 5)\n\n\n\n\nCode\n# Visualize hot spots\nggplot() +\n  geom_sf(\n    data = fishnet, \n    aes(fill = moran_class), \n    color = NA\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n    ),\n    name = \"Cluster Type\"\n  ) +\n  labs(\n    title = \"Local Moran's I: Abandoned buildings Clusters\",\n    subtitle = \"High-High = Hot spots of disorder\"\n  ) +\n  theme_crime()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Get centroids of \"High-High\" cells (hot spots)\nhotspots &lt;- fishnet %&gt;%\n  filter(moran_class == \"High-High\") %&gt;%\n  st_centroid()\n\n# Calculate distance from each cell to nearest hot spot\nif (nrow(hotspots) &gt; 0) {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot = as.numeric(\n        st_distance(st_centroid(fishnet), hotspots %&gt;% st_union())\n      )\n    )\n  \n  cat(\"✓ Calculated distance to abandoned car hot spots\\n\")\n  cat(\"  - Number of hot spot cells:\", nrow(hotspots), \"\\n\")\n} else {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(dist_to_hotspot = 0)\n  cat(\"⚠ No significant hot spots found\\n\")\n}\n\n\n✓ Calculated distance to abandoned car hot spots\n  - Number of hot spot cells: 294 \n\n\nQuestion 4.3: Why might distance to a cluster of abandoned cars be more informative than distance to a single abandoned car? What does Local Moran’s I tell us?\n\nit helps us to capture spatial dependence at regional level. (we have calculate the spatial dependence at local and neighborhood levels). It omits the influence of outliers and captures spillover effects, which improves prediction accuracy.\nLocal Moran’s I is a local measure. It shows the difference of local location from mean.\n\nThe High-High type shows that the location value is above mean and the neighbor value is also above mean, which can be interpreted as hot spot.\nThe Low-Low type shows that the location value is below mean and the neighbor value is also below mean, which can be interpreted as cold spot.\nThe High-Low type shows that the location value is above mean,but the neighbor value is below mean, which can be interpreted as outlier.\nThe Low-High type shows that the location value is below mean,but the neighbor value is above mean, which can also be interpreted as outlier.\n\n\n\n\n\n\n\n\nNote\n\n\n\nLocal Moran’s I identifies:\n\nHigh-High: Hot spots (high values surrounded by high values)\nLow-Low: Cold spots (low values surrounded by low values)\nHigh-Low / Low-High: Spatial outliers\n\nThis helps us understand spatial clustering patterns."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-6.1-poisson-regression",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-6.1-poisson-regression",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 6.1: Poisson Regression",
    "text": "Exercise 6.1: Poisson Regression\nBurglary counts are count data (0, 1, 2, 3…). We’ll use Poisson regression.\n\n\nCode\n# Create clean modeling dataset\nfishnet_model &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    uniqueID,\n    District,\n    countBurglaries,\n    abandoned_building,\n    abandoned_building.nn,\n    dist_to_hotspot\n  ) %&gt;%\n  na.omit()  # Remove any remaining NAs\n\ncat(\"✓ Prepared modeling data\\n\")\n\n\n✓ Prepared modeling data\n\n\nCode\ncat(\"  - Observations:\", nrow(fishnet_model), \"\\n\")\n\n\n  - Observations: 1708 \n\n\nCode\ncat(\"  - Variables:\", ncol(fishnet_model), \"\\n\")\n\n\n  - Variables: 6 \n\n\n\n\nCode\n# Fit Poisson regression\nmodel_poisson &lt;- glm(\n  countBurglaries ~ abandoned_building + abandoned_building.nn + \n    dist_to_hotspot,\n  data = fishnet_model,\n  family = \"poisson\"\n)\n\n# Summary\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = countBurglaries ~ abandoned_building + abandoned_building.nn + \n    dist_to_hotspot, family = \"poisson\", data = fishnet_model)\n\nCoefficients:\n                          Estimate   Std. Error z value             Pr(&gt;|z|)\n(Intercept)            1.889753853  0.032485490  58.172 &lt; 0.0000000000000002\nabandoned_building     0.001998030  0.000239185   8.354 &lt; 0.0000000000000002\nabandoned_building.nn -0.004631985  0.000178454 -25.956 &lt; 0.0000000000000002\ndist_to_hotspot       -0.000030309  0.000005665  -5.351         0.0000000876\n                         \n(Intercept)           ***\nabandoned_building    ***\nabandoned_building.nn ***\ndist_to_hotspot       ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6710.3  on 1707  degrees of freedom\nResidual deviance: 4256.4  on 1704  degrees of freedom\nAIC: 8324.7\n\nNumber of Fisher Scoring iterations: 6\n\n\nQuestion 6.1: Interpret the coefficients. Which variables are significant? What do the signs (positive/negative) tell you?\n\nVariables that are significant: abandoned_building, abandoned_building.nn, dist_to_hotspot\nThe positive coefficient for abandoned_building indicates that areas with more abandoned buildings tend to experience higher burglary counts. \nThe negative coefficient for abandoned_building.nn suggests that abandoned buildings located closer together are associated with higher burglary counts.\nThe negative coefficient for dist_to_hotspot indicates that higher near clusters of abandoned buildings (hotspots) are associated with higher burglary counts."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-6.2-check-for-overdispersion",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-6.2-check-for-overdispersion",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 6.2: Check for Overdispersion",
    "text": "Exercise 6.2: Check for Overdispersion\nPoisson regression assumes mean = variance. Real count data often violates this (overdispersion).\n\n\nCode\n# Calculate dispersion parameter\ndispersion &lt;- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 2), \"\\n\")\n\n\nDispersion parameter: 2.76 \n\n\nCode\ncat(\"Rule of thumb: &gt;1.5 suggests overdispersion\\n\")\n\n\nRule of thumb: &gt;1.5 suggests overdispersion\n\n\nCode\nif (dispersion &gt; 1.5) {\n  cat(\"⚠ Overdispersion detected! Consider Negative Binomial model.\\n\")\n} else {\n  cat(\"✓ Dispersion looks okay for Poisson model.\\n\")\n}\n\n\n⚠ Overdispersion detected! Consider Negative Binomial model."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-6.3-negative-binomial-regression",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-6.3-negative-binomial-regression",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 6.3: Negative Binomial Regression",
    "text": "Exercise 6.3: Negative Binomial Regression\nIf overdispersed, use Negative Binomial regression (more flexible).\n\n\nCode\n# Fit Negative Binomial model\nmodel_nb &lt;- glm.nb(\n  countBurglaries ~ abandoned_building + abandoned_building.nn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Summary\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = countBurglaries ~ abandoned_building + abandoned_building.nn + \n    dist_to_hotspot, data = fishnet_model, init.theta = 2.148065465, \n    link = log)\n\nCoefficients:\n                          Estimate   Std. Error z value             Pr(&gt;|z|)\n(Intercept)            1.935501094  0.056376456   34.33 &lt; 0.0000000000000002\nabandoned_building     0.001999655  0.000487666    4.10            0.0000412\nabandoned_building.nn -0.005266039  0.000256987  -20.49 &lt; 0.0000000000000002\ndist_to_hotspot       -0.000018583  0.000008768   -2.12                0.034\n                         \n(Intercept)           ***\nabandoned_building    ***\nabandoned_building.nn ***\ndist_to_hotspot       *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.1481) family taken to be 1)\n\n    Null deviance: 2936.4  on 1707  degrees of freedom\nResidual deviance: 1812.3  on 1704  degrees of freedom\nAIC: 7271.4\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.148 \n          Std. Err.:  0.135 \n\n 2 x log-likelihood:  -7261.384 \n\n\nCode\n# Compare AIC (lower is better)\ncat(\"\\nModel Comparison:\\n\")\n\n\n\nModel Comparison:\n\n\nCode\ncat(\"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\")\n\n\nPoisson AIC: 8324.7 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(AIC(model_nb), 1), \"\\n\")\n\n\nNegative Binomial AIC: 7271.4 \n\n\nQuestion 6.2: Which model fits better (lower AIC)? What does this tell you about the data?\n\nNegative Binomial has lower AIC, which means a better performance in predicting crime.\nThis suggests that the count data are overdispersed (the variance of burglaries is much greater than their mean, which violates the Poisson assumption."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-8.1-generate-final-predictions",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-8.1-generate-final-predictions",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 8.1: Generate Final Predictions",
    "text": "Exercise 8.1: Generate Final Predictions\n\n\nCode\n# Fit final model on all data\nfinal_model &lt;- glm.nb(\n  countBurglaries ~ abandoned_building + abandoned_building.nn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Add predictions back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_nb = predict(final_model, fishnet_model, type = \"response\")[match(uniqueID, fishnet_model$uniqueID)]\n  )\n\n# Also add KDE predictions (normalize to same scale as counts)\nkde_sum &lt;- sum(fishnet$kde_value, na.rm = TRUE)\ncount_sum &lt;- sum(fishnet$countBurglaries, na.rm = TRUE)\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_kde = (kde_value / kde_sum) * count_sum\n  )"
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-8.2-compare-model-vs.-kde-baseline",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-8.2-compare-model-vs.-kde-baseline",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 8.2: Compare Model vs. KDE Baseline",
    "text": "Exercise 8.2: Compare Model vs. KDE Baseline\n\n\nCode\n# Create three maps\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Actual Burglaries\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Model Predictions (Neg. Binomial)\") +\n  theme_crime()\n\np3 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"KDE Baseline Predictions\") +\n  theme_crime()\n\np1 + p2 + p3 +\n  plot_annotation(\n    title = \"Actual vs. Predicted Burglaries\",\n    subtitle = \"Does our complex model outperform simple KDE?\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate performance metrics\ncomparison &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    model_mae = mean(abs(countBurglaries - prediction_nb)),\n    model_rmse = sqrt(mean((countBurglaries - prediction_nb)^2)),\n    kde_mae = mean(abs(countBurglaries - prediction_kde)),\n    kde_rmse = sqrt(mean((countBurglaries - prediction_kde)^2))\n  )\n\ncomparison %&gt;%\n  pivot_longer(everything(), names_to = \"metric\", values_to = \"value\") %&gt;%\n  separate(metric, into = c(\"approach\", \"metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = metric, values_from = value) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"Model Performance Comparison\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel Performance Comparison\n\n\napproach\nmae\nrmse\n\n\n\n\nmodel\n2.24\n3.28\n\n\nkde\n2.06\n2.95\n\n\n\n\n\nQuestion 8.1: Does the complex model outperform the simple KDE baseline? By how much? Is the added complexity worth it?\n\nSadly, the complex model perform worse than the KDE baseline. RMSE for KDE is 2.95, while RMSE for the complex model is 3.28.\nThe added model complexity does not yield better predictive accuracy. Next step might include trying another predictor, adding more predictor, or changing the spatial feature."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-9.3-where-does-the-model-work-well",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-9.3-where-does-the-model-work-well",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 9.3: Where Does the Model Work Well?",
    "text": "Exercise 9.3: Where Does the Model Work Well?\n\n\nCode\n# Calculate errors\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    error_nb = countBurglaries - prediction_nb,\n    error_kde = countBurglaries - prediction_kde,\n    abs_error_nb = abs(error_nb),\n    abs_error_kde = abs(error_kde)\n  )\n\n# Map errors\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#2166ac\", mid = \"white\", high = \"#b2182b\",\n    midpoint = 0,\n    limits = c(-10, 10)\n  ) +\n  labs(title = \"Model Errors (Actual - Predicted)\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Abs. Error\", option = \"magma\") +\n  labs(title = \"Absolute Model Errors\") +\n  theme_crime()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nQuestion 9.2: Where does the model make the biggest errors? Are there spatial patterns in the errors? What might this reveal?\n\nModel errors perform randomly in their spatial distribution. There are a few cells recorded high value in absolute model errors, however, they are randomly dispersed.\nThis indicates that the model’s performance was not affected by spatial correlations."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-10.1-model-summary-table",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-10.1-model-summary-table",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 10.1: Model Summary Table",
    "text": "Exercise 10.1: Model Summary Table\n\n\nCode\n# Create nice summary table\nmodel_summary &lt;- broom::tidy(final_model, exponentiate = TRUE) %&gt;%\n  mutate(\n    across(where(is.numeric), ~round(., 3))\n  )\n\nmodel_summary %&gt;%\n  kable(\n    caption = \"Final Negative Binomial Model Coefficients (Exponentiated)\",\n    col.names = c(\"Variable\", \"Rate Ratio\", \"Std. Error\", \"Z\", \"P-Value\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = \"Rate ratios &gt; 1 indicate positive association with burglary counts.\"\n  )\n\n\n\nFinal Negative Binomial Model Coefficients (Exponentiated)\n\n\nVariable\nRate Ratio\nStd. Error\nZ\nP-Value\n\n\n\n\n(Intercept)\n6.928\n0.056\n34.332\n0.000\n\n\nabandoned_building\n1.002\n0.000\n4.100\n0.000\n\n\nabandoned_building.nn\n0.995\n0.000\n-20.491\n0.000\n\n\ndist_to_hotspot\n1.000\n0.000\n-2.120\n0.034\n\n\n\nNote: \n\n\n\n\n\n\n Rate ratios &gt; 1 indicate positive association with burglary counts."
  },
  {
    "objectID": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-10.2-key-findings-summary",
    "href": "assignments/assignment_4/Cen_Jinheng_Assignment4.html#exercise-10.2-key-findings-summary",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 10.2: Key Findings Summary",
    "text": "Exercise 10.2: Key Findings Summary\nTechnical Performance:\n\nCross-validation MAE: 3.28\nModel vs. KDE: KDE performs better\nMost predictive variable: abandoned_buildings.nn\n\nSpatial Patterns:\n\nBurglaries are clustered\nHot spots are located in suburb areas on the northwestern and southern side\nModel errors show random patterns\n\nModel Limitations:\n\nOverdispersion: Yes\nSpatial autocorrelation in residuals: No, model error are randomly distributed.\nCells with zero counts: 31% of the data are 0"
  },
  {
    "objectID": "midterm/appendix/Midterm_2025.html",
    "href": "midterm/appendix/Midterm_2025.html",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Content: All the technical details:\nAudience: Data scientists and technical reviewers"
  },
  {
    "objectID": "midterm/appendix/Midterm_2025.html#data-sources",
    "href": "midterm/appendix/Midterm_2025.html#data-sources",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Data Sources",
    "text": "Data Sources\n\nPrimary Dataset: Philadelphia Property Sales\nSource: Philadelphia Property Sales\nWe sourced the following variables from the property sales dataset:\n\nSale price\nSale date\nProperty characteristics:\n\nTotal Area\nTotal Livable Area\nYear Built\nCensus Tract\nExterior Condition\nInterior Condition\nFireplaces\nGarage Spaces\nMarket Value\nBasements\nNumber of Bedrooms\nNumber of Bathrooms\nNumber of Stories\nShape\n\n\n\n\nSecondary Datasets: Spatial Amenities\nSource: OpenDataPhilly\nWe sourced the following environmental and neighborhood characteristics:\n\nCrime Incidents:\nNeighborood Boundaries\nSEPTA\nBike Network\nHospital\nPhiladelphia Park and Recreation\nSchools\nNeighborhood Food Markets\nPolice Station\nFire Department\n\n\n\n\nPart 1: Data Preparation\nLoad and clean Philadelphia sales data:\n\n########################\n# REMOTE DATA SOURCES\n########################\n\nNEIGH_URL   &lt;- \"https://raw.githubusercontent.com/opendataphilly/open-geo-data/refs/heads/master/philadelphia-neighborhoods/philadelphia-neighborhoods.geojson\"\nSTOPS_URL   &lt;- \"https://hub.arcgis.com/api/v3/datasets/4f827cdbf84d4a53983cf43b8d9fd4df_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1\"\nBIKE_URL    &lt;- \"https://hub.arcgis.com/api/v3/datasets/b5f660b9f0f44ced915995b6d49f6385_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1\"\nHOSP_URL    &lt;- \"https://opendata.arcgis.com/datasets/df8dc18412494e5abbb021e2f33057b2_0.geojson\"\nPARKS_URL   &lt;- \"https://hub.arcgis.com/api/v3/datasets/d52445160ab14380a673e5849203eb64_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1\"\nSCHOOLS_URL &lt;- \"https://hub.arcgis.com/api/v3/datasets/d46a7e59e2c246c891fbee778759717e_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1\"\nFOOD_URL    &lt;- \"https://opendata.arcgis.com/datasets/53b8a1c653a74c92b2de23a5d7bf04a0_0.geojson\"\nPOLICE_URL  &lt;- \"https://hub.arcgis.com/api/v3/datasets/7e522339d7c24e8ea5f2c7780291c315_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1\"\nFIRE_URL    &lt;- \"https://opendata.arcgis.com/datasets/341526186e014aa0aa3ef7e08a394a78_0.geojson\"\nOPA_URL &lt;- \"https://opendata-downloads.s3.amazonaws.com/opa_properties_public.csv\"\nCRIME_2023_URL &lt;- \"https://phl.carto.com/api/v2/sql?filename=incidents_part1_part2&format=csv&q=SELECT%20*%20,%20ST_Y(the_geom)%20AS%20lat,%20ST_X(the_geom)%20AS%20lng%20FROM%20incidents_part1_part2%20WHERE%20dispatch_date_time%20%3E=%20%272024-01-01%27%20AND%20dispatch_date_time%20%3C%20%272025-01-01%27\"\nCRIME_2024_URL &lt;- \"https://phl.carto.com/api/v2/sql?filename=incidents_part1_part2&format=csv&q=SELECT%20*%20,%20ST_Y(the_geom)%20AS%20lat,%20ST_X(the_geom)%20AS%20lng%20FROM%20incidents_part1_part2%20WHERE%20dispatch_date_time%20%3E=%20%272023-01-01%27%20AND%20dispatch_date_time%20%3C%20%272024-01-01%27\"\n\n############################################################\n# HELPERS\n############################################################\n\nnorm_tgc &lt;- function(x) x |&gt; str_squish() |&gt; str_to_lower()\n\nviolent_set &lt;- c(\n  \"homicide - criminal\",\n  \"homicide - justifiable\",\n  \"rape\",\n  \"robbery firearm\",\n  \"robbery no firearm\",\n  \"aggravated assault firearm\",\n  \"aggravated assault no firearm\",\n  \"other assaults\",\n  \"offenses against family and children\"\n)\n\nmake_violence_bins &lt;- function(df) {\n  df %&gt;%\n    mutate(\n      key = norm_tgc(text_general_code),\n      gun_involved = str_detect(key, \"firearm|weapon\"),\n      violence_bin = case_when(\n        gun_involved         ~ \"Violent\",\n        key %in% violent_set ~ \"Violent\",\n        TRUE                 ~ \"Misdemeanor/Non-violent\"\n      ),\n      violence_bin = factor(\n        violence_bin,\n        levels = c(\"Violent\", \"Misdemeanor/Non-violent\")\n      )\n    )\n}\n\nget_block_dists &lt;- function(epsg, one_block_ft = 300) {\n  u &lt;- tryCatch(st_crs(epsg)$units_gdal, error = function(e) NA_character_)\n  if (is.na(u)) u &lt;- \"unknown\"\n  if (u %in% c(\"metre\",\"m\")) {\n    ft_to_m &lt;- 0.3048\n    list(\n      one = one_block_ft * ft_to_m,\n      two = one_block_ft * 2 * ft_to_m\n    )\n  } else {\n    list(\n      one = one_block_ft,\n      two = one_block_ft * 2\n    )\n  }\n}\n\nto_feet &lt;- function(x, units_gdal) {\n  if (isTRUE(units_gdal %in% c(\"metre\",\"m\"))) {\n    as.numeric(x) / 0.3048\n  } else {\n    as.numeric(x)\n  }\n}\n\nto_miles &lt;- function(x, units_gdal) {\n  x_num &lt;- as.numeric(x)\n  if (isTRUE(units_gdal %in% c(\"metre\",\"m\"))) {\n    x_num / 1609.344\n  } else {\n    x_num / 5280\n  }\n}\n\nnearest_dist_ft &lt;- function(from_pts, to_feats, units_gdal) {\n  if (nrow(to_feats) == 0) return(rep(NA_real_, nrow(from_pts)))\n  idx &lt;- st_nearest_feature(from_pts, to_feats)\n  d   &lt;- st_distance(from_pts, to_feats[idx, ], by_element = TRUE)\n  to_feet(d, units_gdal)\n}\n\nnorm_school_type &lt;- function(x) {\n  t &lt;- str_to_lower(str_squish(as.character(x)))\n  case_when(\n    str_detect(t, \"charter\")                 ~ \"charter\",\n    str_detect(t, \"private|independent\")     ~ \"private\",\n    str_detect(t, \"public|district\")         ~ \"public\",\n    TRUE                                     ~ NA_character_\n  )\n}\n\n############################################################\n# 1. LOAD REMOTE DATA\n############################################################\n\nRaw_data &lt;- read_csv(OPA_URL, show_col_types = FALSE)\n\ncrime2023 &lt;- read_csv(CRIME_2023_URL, show_col_types = FALSE)\ncrime2024 &lt;- read_csv(CRIME_2024_URL, show_col_types = FALSE)\n\n# these will now come from URL instead of disk\nneigh_raw  &lt;- read_sf(NEIGH_URL)\nstops_raw  &lt;- read_sf(STOPS_URL)\nbike_raw   &lt;- read_sf(BIKE_URL)\nhosp_raw   &lt;- read_sf(HOSP_URL)\nparks_raw  &lt;- read_sf(PARKS_URL)\nschools_in &lt;- read_sf(SCHOOLS_URL)\nfood_raw   &lt;- read_sf(FOOD_URL)\npol_raw    &lt;- read_sf(POLICE_URL)\nfire_raw   &lt;- read_sf(FIRE_URL)\n\n\n############################################################\n# 2. CLEAN PROPERTY SALES\n############################################################\n\nkeep_vars &lt;- c(\n  \"sale_date\", \"sale_price\",\n  \"total_area\", \"total_livable_area\",\n  \"unfinished\", \"year_built\", \"shape\",\n  \"category_code\", \"category_code_description\",\n  \"census_tract\", \"central_air\",\n  \"exterior_condition\", \"interior_condition\",\n  \"fireplaces\", \"garage_spaces\",\n  \"general_construction\", \"market_value\",\n  \"number_of_bathrooms\", \"number_of_bedrooms\",\n  \"number_stories\", \"quality_grade\",\n  \"basements\"\n)\n\nclean_data &lt;- Raw_data %&gt;%\n  select(any_of(keep_vars)) %&gt;%\n  mutate(\n    # OPA sometimes sticks a leading integer before the date\n    sale_date_chr = sub(\"^\\\\s*\\\\d+\\\\s+\", \"\", as.character(sale_date)),\n    sale_date = suppressWarnings(parse_date_time(\n      sale_date_chr,\n      orders = c(\"Y-m-d H:M:S\", \"Y-m-d\"),\n      tz = \"America/New_York\"\n    ))\n  ) %&gt;%\n  # time filter (2023-01-01 through 2024-12-31 basically)\n  filter(\n    !is.na(sale_date),\n    sale_date &gt;= ymd_hms(\"2023-01-01 00:00:00\", tz = \"America/New_York\"),\n    sale_date &lt;  ymd_hms(\"2025-01-01 00:00:00\", tz = \"America/New_York\")\n  ) %&gt;%\n  # keep only residential code 1\n  mutate(category_code = suppressWarnings(as.integer(category_code))) %&gt;%\n  filter(category_code == 1) %&gt;%\n  select(-sale_date_chr)\n\n# turn WKT parcel geometry into sf polygons; OPA CRS is EPSG:2272\nclean_data &lt;- st_as_sf(clean_data, wkt = \"shape\", crs = 2272)\n\n# NA summary (useful diagnostics)\ndat_no_geom &lt;- st_drop_geometry(clean_data)\nn_rows &lt;- nrow(dat_no_geom)\nna_summary &lt;- dat_no_geom %&gt;%\n  summarise(across(everything(), ~ sum(is.na(.)))) %&gt;%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"na_count\") %&gt;%\n  mutate(na_pct = round(100 * na_count / n_rows, 2)) %&gt;%\n  arrange(desc(na_count))\n\n# drop some bad cols and then drop rows w/ remaining NA\nclean_data &lt;- clean_data %&gt;%\n  select(-unfinished, -central_air)\n\nkeep_rows &lt;- stats::complete.cases(st_drop_geometry(clean_data))\nclean_data &lt;- clean_data[keep_rows, ]\n\n\n############################################################\n# 3. SPATIAL ENRICHMENT\n############################################################\n\ncrime2023_bins &lt;- make_violence_bins(crime2023)\ncrime2024_bins &lt;- make_violence_bins(crime2024)\n\ncrime_bins &lt;- bind_rows(crime2023_bins, crime2024_bins) %&gt;%\n  filter(!is.na(lng), !is.na(lat)) %&gt;%\n  st_as_sf(coords = c(\"lng\",\"lat\"), crs = 4326, remove = FALSE) %&gt;%\n  st_transform(CRS_TARGET)\n\ncrime_violent &lt;- filter(crime_bins, violence_bin == \"Violent\")\ncrime_petty   &lt;- filter(crime_bins, violence_bin == \"Misdemeanor/Non-violent\")\n\n# project parcels, ensure valid geom, attach an ID\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid() %&gt;%\n  mutate(prop_id = row_number())\n\n# create centroids for distance/buffer ops\nprops_centroids &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\n# make ~1 block (~300ft) and ~2 blocks (~600ft) buffers\nBLK &lt;- get_block_dists(CRS_TARGET, one_block_ft = 300)\nbuf_1blk &lt;- st_buffer(props_centroids, BLK$one)[, \"prop_id\"]\nbuf_2blk &lt;- st_buffer(props_centroids, BLK$two)[, \"prop_id\"]\n\n# track all IDs so every parcel gets a value\nprop_id_tbl &lt;- st_drop_geometry(props_centroids) %&gt;% select(prop_id)\n\nviolent_counts &lt;- st_join(buf_2blk, crime_violent, join = st_intersects, left = TRUE) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(prop_id) %&gt;%\n  summarise(violent_2blocks = sum(!is.na(lat)), .groups = \"drop\") %&gt;%\n  right_join(prop_id_tbl, by = \"prop_id\") %&gt;%\n  mutate(violent_2blocks = coalesce(violent_2blocks, 0L))\n\npetty_counts &lt;- st_join(buf_1blk, crime_petty, join = st_intersects, left = TRUE) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(prop_id) %&gt;%\n  summarise(petty_1block = sum(!is.na(lat)), .groups = \"drop\") %&gt;%\n  right_join(prop_id_tbl, by = \"prop_id\") %&gt;%\n  mutate(petty_1block = coalesce(petty_1block, 0L))\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(violent_counts, by = \"prop_id\") %&gt;%\n  left_join(petty_counts,  by = \"prop_id\")\n\n\n### 3b. Neighborhood join\n\ncandidate_cols &lt;- c(\n  \"name\",\"neighborhood\",\"neighborhood_name\",\"mapname\",\n  \"map_name\",\"label\",\"area_name\",\"neigh_name\",\"NAME\",\"LABEL\"\n)\nhit &lt;- intersect(candidate_cols, names(neigh_raw))\nNEIGH_NAME_COL &lt;- if (length(hit) &gt;= 1) hit[1] else {\n  stop(\"Couldn't find a neighborhood name column in neigh_raw.\")\n}\n\nneigh &lt;- neigh_raw %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  select(neigh_name = !!sym(NEIGH_NAME_COL))\n\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\nprops_with_neigh &lt;- st_join(props_ctr, neigh, join = st_within, left = TRUE)\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(\n    st_drop_geometry(props_with_neigh) %&gt;% select(prop_id, neigh_name),\n    by = \"prop_id\"\n  )\n\n\n### 3c. Transit access\n\nstops &lt;- stops_raw %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\n# recompute props_ctr just in case\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid() %&gt;%\n  mutate(prop_id = coalesce(prop_id, row_number()))\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\nnn_idx &lt;- st_nearest_feature(props_ctr, stops)\nnearest_stops &lt;- stops[nn_idx, ]\ndist_vec &lt;- st_distance(props_ctr, nearest_stops, by_element = TRUE)\n\ncrs_units &lt;- tryCatch(st_crs(CRS_TARGET)$units_gdal, error = function(e) NA_character_)\ndist_stop_ft &lt;- to_feet(dist_vec, crs_units)\n\ncand &lt;- c(\"stop_name\",\"name\",\"STOP_NAME\",\"stop_id\",\"id\",\"STOP_ID\")\nhit  &lt;- intersect(cand, names(stops))\nstop_id_col &lt;- if (length(hit) &gt;= 1) hit[1] else NA_character_\n\nnearest_tbl &lt;- st_drop_geometry(nearest_stops) %&gt;%\n  mutate(\n    prop_id = props_ctr$prop_id,\n    dist_stop_ft = dist_stop_ft\n  ) %&gt;%\n  { if (!is.na(stop_id_col)) {\n      select(., prop_id, dist_stop_ft,\n             nearest_stop = !!sym(stop_id_col))\n    } else {\n      select(., prop_id, dist_stop_ft)\n    }\n  }\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(nearest_tbl, by = \"prop_id\")\n\n\n### 3d. Bike network access\n\n# turn polygons into boundaries if needed (so distance-to-line makes sense)\ntypes &lt;- unique(st_geometry_type(bike_raw))\nbike_geom &lt;- st_geometry(bike_raw)\nif (any(grepl(\"POLYGON\", types))) {\n  bike_geom &lt;- st_boundary(bike_geom)\n}\n\nbike &lt;- bike_raw %&gt;%\n  st_set_geometry(bike_geom) %&gt;%\n  st_cast(\"MULTILINESTRING\", warn = FALSE) %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid()\n\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\nnn_idx &lt;- st_nearest_feature(props_ctr, bike)\nnearest_bike &lt;- bike[nn_idx, ]\ndist_vec &lt;- st_distance(props_ctr, nearest_bike, by_element = TRUE)\ndist_bike_ft &lt;- to_feet(dist_vec, crs_units)\n\ncand &lt;- c(\"name\",\"route\",\"route_name\",\"facility\",\"type\",\"street\",\"NAME\",\"ROUTE\")\nhit  &lt;- intersect(cand, names(bike))\nbike_col &lt;- if (length(hit) &gt;= 1) hit[1] else NA_character_\n\nnearest_tbl &lt;- st_drop_geometry(nearest_bike) %&gt;%\n  mutate(\n    prop_id       = props_ctr$prop_id,\n    dist_bike_ft  = dist_bike_ft\n  ) %&gt;%\n  { if (!is.na(bike_col)) {\n      select(., prop_id, dist_bike_ft,\n             nearest_bike_feature = !!sym(bike_col))\n    } else {\n      select(., prop_id, dist_bike_ft)\n    }\n  }\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(nearest_tbl, by = \"prop_id\")\n\n\n### 3e. Hospital access\n\nhosp &lt;- hosp_raw %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid()\n\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\nnn_idx &lt;- st_nearest_feature(props_ctr, hosp)\nnearest_hosp &lt;- hosp[nn_idx, ]\ndist_vec &lt;- st_distance(props_ctr, nearest_hosp, by_element = TRUE)\n\ndist_hosp_mi &lt;- to_miles(dist_vec, crs_units)\n\ncand &lt;- c(\"name\",\"NAME\",\"hospital\",\"Hospital\",\"FACILITY\",\"facility\",\n          \"HOSPITAL\",\"inst_name\",\"INST_NAME\",\"id\",\"ID\")\nhit  &lt;- intersect(cand, names(hosp))\nhosp_col &lt;- if (length(hit) &gt;= 1) hit[1] else NA_character_\n\nnearest_tbl &lt;- st_drop_geometry(nearest_hosp) %&gt;%\n  mutate(\n    prop_id       = props_ctr$prop_id,\n    dist_hosp_mi  = dist_hosp_mi\n  ) %&gt;%\n  { if (!is.na(hosp_col)) {\n      select(., prop_id, dist_hosp_mi,\n             nearest_hospital = !!sym(hosp_col))\n    } else {\n      select(., prop_id, dist_hosp_mi)\n    }\n  } %&gt;%\n  mutate(\n    service_band_code = case_when(\n      dist_hosp_mi &lt; 1                     ~ 1L,\n      dist_hosp_mi &gt;= 1 & dist_hosp_mi &lt; 5 ~ 2L,\n      TRUE                                 ~ 3L\n    ),\n    service_band = factor(\n      service_band_code,\n      levels = c(1L, 2L, 3L),\n      labels = c(\"Too Close\", \"Within Service\", \"Out of Service\"),\n      ordered = TRUE\n    )\n  )\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(nearest_tbl, by = \"prop_id\")\n\n\n### 3f. Park access\n\n# only polygon parks\nis_poly &lt;- st_geometry_type(parks_raw) %in% c(\"POLYGON\",\"MULTIPOLYGON\")\nparks &lt;- parks_raw[is_poly, ] %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid()\n\nnn_idx &lt;- st_nearest_feature(clean_data, parks)\nnearest_parks &lt;- parks[nn_idx, ]\ndist_vec &lt;- st_distance(clean_data, nearest_parks, by_element = TRUE)\n\nunits_gdal &lt;- tryCatch(st_crs(CRS_TARGET)$units_gdal, error = function(e) NA_character_)\ndist_park_ft &lt;- to_feet(dist_vec, units_gdal)\ndist_park_mi &lt;- to_miles(dist_vec, units_gdal)\n\ncand &lt;- c(\"park_name\",\"name\",\"NAME\",\"asset_name\",\"ASSET_NAME\",\n          \"site_name\",\"SITE_NAME\",\"prop_name\",\"PROP_NAME\")\nhit  &lt;- intersect(cand, names(parks))\npark_col &lt;- if (length(hit) &gt;= 1) hit[1] else NA_character_\n\nnearest_tbl &lt;- st_drop_geometry(nearest_parks) %&gt;%\n  mutate(\n    prop_id      = clean_data$prop_id,\n    dist_park_ft = dist_park_ft,\n    dist_park_mi = dist_park_mi\n  ) %&gt;%\n  { if (!is.na(park_col)) {\n      select(., prop_id, dist_park_ft, dist_park_mi,\n             nearest_park = !!sym(park_col))\n    } else {\n      select(., prop_id, dist_park_ft, dist_park_mi)\n    }\n  }\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(nearest_tbl, by = \"prop_id\")\n\n\n### 3g. School access\n\nschools &lt;- schools_in %&gt;%\n  st_make_valid() %&gt;%\n  mutate(type_norm = norm_school_type(.data[[\"type_specific\"]])) %&gt;%\n  filter(!is.na(type_norm))\n\n# if polygons, collapse to point-on-surface\nif (any(st_geometry_type(schools) %in% c(\"POLYGON\",\"MULTIPOLYGON\"))) {\n  st_geometry(schools) &lt;- st_point_on_surface(st_geometry(schools))\n}\n\nschools &lt;- schools %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nsch_pub  &lt;- filter(schools, type_norm == \"public\")\nsch_char &lt;- filter(schools, type_norm == \"charter\")\nsch_priv &lt;- filter(schools, type_norm == \"private\")\n\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid()\n\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\nunits_gdal &lt;- tryCatch(st_crs(CRS_TARGET)$units_gdal, error = function(e) NA_character_)\n\ndist_school_public_ft  &lt;- nearest_dist_ft(props_ctr, sch_pub,  units_gdal)\ndist_school_charter_ft &lt;- nearest_dist_ft(props_ctr, sch_char, units_gdal)\ndist_school_private_ft &lt;- nearest_dist_ft(props_ctr, sch_priv, units_gdal)\n\ndist_tbl &lt;- st_drop_geometry(props_ctr) %&gt;%\n  transmute(\n    prop_id,\n    dist_school_public_ft  = dist_school_public_ft,\n    dist_school_charter_ft = dist_school_charter_ft,\n    dist_school_private_ft = dist_school_private_ft\n  )\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(dist_tbl, by = \"prop_id\")\n\n\n### 3h. Food access\n\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid()\n\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\nfood_proc &lt;- food_raw %&gt;%\n  st_make_valid()\n\nif (any(st_geometry_type(food_proc) %in% c(\"POLYGON\",\"MULTIPOLYGON\"))) {\n  st_geometry(food_proc) &lt;- st_point_on_surface(st_geometry(food_proc))\n}\n\nfood_proc &lt;- food_proc %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nif (nrow(food_proc) == 0L) {\n  clean_data &lt;- clean_data %&gt;%\n    mutate(\n      dist_foodretail_ft   = NA_real_,\n      foodretail_1mi_count = 0L\n    )\n} else {\n  units_gdal &lt;- tryCatch(st_crs(CRS_TARGET)$units_gdal, error = function(e) NA_character_)\n  one_mile &lt;- if (isTRUE(units_gdal %in% c(\"metre\",\"m\"))) 1609.344 else 5280\n\n  nn_idx &lt;- st_nearest_feature(props_ctr, food_proc)\n  nearest_food &lt;- food_proc[nn_idx, ]\n  dist_vec &lt;- st_distance(props_ctr, nearest_food, by_element = TRUE)\n  dist_foodretail_ft &lt;- to_feet(dist_vec, units_gdal)\n\n  buf_1mi &lt;- st_buffer(props_ctr, one_mile)[, \"prop_id\"]\n  cnt_tbl &lt;- st_join(buf_1mi, food_proc, join = st_intersects, left = TRUE) %&gt;%\n    st_drop_geometry() %&gt;%\n    count(prop_id, name = \"foodretail_1mi_count\")\n\n  dist_tbl &lt;- st_drop_geometry(props_ctr) %&gt;%\n    transmute(prop_id, dist_foodretail_ft = dist_foodretail_ft)\n\n  clean_data &lt;- clean_data %&gt;%\n    left_join(dist_tbl, by = \"prop_id\") %&gt;%\n    left_join(cnt_tbl, by = \"prop_id\") %&gt;%\n    mutate(foodretail_1mi_count = coalesce(foodretail_1mi_count, 0L))\n}\n\n\n### 3i. Police access\n\npol_proc &lt;- pol_raw %&gt;%\n  st_make_valid()\n\nif (any(st_geometry_type(pol_proc) %in% c(\"POLYGON\",\"MULTIPOLYGON\"))) {\n  st_geometry(pol_proc) &lt;- st_point_on_surface(st_geometry(pol_proc))\n}\n\npol_proc &lt;- pol_proc %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nif (nrow(pol_proc) == 0L) {\n  clean_data &lt;- clean_data %&gt;%\n    mutate(dist_police_ft = NA_real_)\n} else {\n  clean_data &lt;- clean_data %&gt;%\n    st_transform(CRS_TARGET) %&gt;%\n    st_make_valid()\n\n  props_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\n  nn_idx &lt;- st_nearest_feature(props_ctr, pol_proc)\n  nearest_pol &lt;- pol_proc[nn_idx, ]\n  dist_vec &lt;- st_distance(props_ctr, nearest_pol, by_element = TRUE)\n  units_gdal &lt;- tryCatch(st_crs(CRS_TARGET)$units_gdal, error = function(e) NA_character_)\n  dist_police_ft &lt;- to_feet(dist_vec, units_gdal)\n\n  cand &lt;- c(\"name\",\"station\",\"precinct\",\"district\",\"NAME\",\"STATION\",\n            \"DISTRICT\",\"FACILITY\",\"facility\",\"id\",\"ID\")\n  hit  &lt;- intersect(cand, names(pol_proc))\n  pol_col &lt;- if (length(hit) &gt;= 1) hit[1] else NA_character_\n\n  nearest_tbl &lt;- st_drop_geometry(nearest_pol) %&gt;%\n    mutate(\n      prop_id        = props_ctr$prop_id,\n      dist_police_ft = dist_police_ft\n    ) %&gt;%\n    { if (!is.na(pol_col)) {\n        select(., prop_id, dist_police_ft,\n               nearest_police = !!sym(pol_col))\n      } else {\n        select(., prop_id, dist_police_ft)\n      }\n    }\n\n  clean_data &lt;- clean_data %&gt;%\n    left_join(nearest_tbl, by = \"prop_id\")\n}\n\n\n### 3j. Fire station access\n\nfire_proc &lt;- fire_raw %&gt;%\n  st_make_valid()\n\nif (any(st_geometry_type(fire_proc) %in% c(\"POLYGON\",\"MULTIPOLYGON\"))) {\n  st_geometry(fire_proc) &lt;- st_point_on_surface(st_geometry(fire_proc))\n}\n\nfire_proc &lt;- fire_proc %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nif (nrow(fire_proc) == 0L) {\n  clean_data &lt;- clean_data %&gt;%\n    mutate(dist_fire_ft = NA_real_)\n} else {\n  clean_data &lt;- clean_data %&gt;%\n    st_transform(CRS_TARGET) %&gt;%\n    st_make_valid()\n\n  props_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\n  nn_idx &lt;- st_nearest_feature(props_ctr, fire_proc)\n  nearest_fire &lt;- fire_proc[nn_idx, ]\n  dist_vec &lt;- st_distance(props_ctr, nearest_fire, by_element = TRUE)\n  units_gdal &lt;- tryCatch(st_crs(CRS_TARGET)$units_gdal, error = function(e) NA_character_)\n  dist_fire_ft &lt;- to_feet(dist_vec, units_gdal)\n\n  cand &lt;- c(\"name\",\"station\",\"facility\",\"FACILITY\",\"STATION\",\n            \"company\",\"COMPANY\",\"house\",\"HOUSE\",\"id\",\"ID\")\n  hit  &lt;- intersect(cand, names(fire_proc))\n  fire_col &lt;- if (length(hit) &gt;= 1) hit[1] else NA_character_\n\n  nearest_tbl &lt;- st_drop_geometry(nearest_fire) %&gt;%\n    mutate(\n      prop_id      = props_ctr$prop_id,\n      dist_fire_ft = dist_fire_ft\n    ) %&gt;%\n    { if (!is.na(fire_col)) {\n        select(., prop_id, dist_fire_ft,\n               nearest_fire = !!sym(fire_col))\n      } else {\n        select(., prop_id, dist_fire_ft)\n      }\n    }\n\n  clean_data &lt;- clean_data %&gt;%\n    left_join(nearest_tbl, by = \"prop_id\")\n}\n\n\n############################################################\n# 4. ACS (tract-level sociodemographic context)\n############################################################\n\n# race / ethnicity\nrace_vars &lt;- c(\n  total     = \"B03002_001\",\n  white_nh  = \"B03002_003\",\n  black_nh  = \"B03002_004\",\n  asian_nh  = \"B03002_006\",\n  hispanic  = \"B03002_012\"\n)\n\npa_race &lt;- get_acs(\n  geography = GEOG,\n  state = STATE_FIPS,\n  year = ACS_YEAR,\n  survey = \"acs5\",\n  variables = race_vars,\n  output = \"wide\"\n) %&gt;%\n  transmute(\n    GEOID,\n    pct_white    = 100 * white_nhE / totalE,\n    pct_black    = 100 * black_nhE / totalE,\n    pct_hispanic = 100 * hispanicE / totalE,\n    pct_asian    = 100 * asian_nhE / totalE\n  )\n\n# age structure\nb01001  &lt;- get_acs(\n  geography = GEOG,\n  state = STATE_FIPS,\n  year = ACS_YEAR,\n  survey = \"acs5\",\n  table = \"B01001\"\n)\n\nvdict   &lt;- load_variables(ACS_YEAR, \"acs5\", cache = TRUE)\n\nb01001l &lt;- b01001 %&gt;%\n  left_join(vdict %&gt;% select(name, label), by = c(\"variable\" = \"name\"))\n\nsum_ages &lt;- function(df, pats) {\n  df %&gt;%\n    filter(str_detect(label, paste(pats, collapse = \"|\"))) %&gt;%\n    group_by(GEOID) %&gt;%\n    summarize(estimate = sum(estimate, na.rm = TRUE), .groups = \"drop\")\n}\n\nage_total &lt;- b01001l %&gt;%\n  filter(str_detect(label, \"Total:\")) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarize(pop_total = first(estimate), .groups = \"drop\")\n\nage_25_44 &lt;- sum_ages(\n  b01001l,\n  c(\"Male:!!25 to 29 years\",\"Male:!!30 to 34 years\",\n    \"Male:!!35 to 39 years\",\"Male:!!40 to 44 years\",\n    \"Female:!!25 to 29 years\",\"Female:!!30 to 34 years\",\n    \"Female:!!35 to 39 years\",\"Female:!!40 to 44 years\")\n) %&gt;% rename(age_25_44 = estimate)\n\nage_45_64 &lt;- sum_ages(\n  b01001l,\n  c(\"Male:!!45 to 49 years\",\"Male:!!50 to 54 years\",\n    \"Male:!!55 to 59 years\",\"Male:!!60 and 61 years\",\n    \"Male:!!62 to 64 years\",\n    \"Female:!!45 to 49 years\",\"Female:!!50 to 54 years\",\n    \"Female:!!55 to 59 years\",\"Female:!!60 and 61 years\",\n    \"Female:!!62 to 64 years\")\n) %&gt;% rename(age_45_64 = estimate)\n\nage_65p &lt;- sum_ages(\n  b01001l,\n  c(\"Male:!!65 and 66 years\",\"Male:!!67 to 69 years\",\n    \"Male:!!70 to 74 years\",\"Male:!!75 to 79 years\",\n    \"Male:!!80 to 84 years\",\"Male:!!85 years and over\",\n    \"Female:!!65 and 66 years\",\"Female:!!67 to 69 years\",\n    \"Female:!!70 to 74 years\",\"Female:!!75 to 79 years\",\n    \"Female:!!80 to 84 years\",\"Female:!!85 years and over\")\n) %&gt;% rename(age_65plus = estimate)\n\npa_age &lt;- age_total %&gt;%\n  left_join(age_25_44, by = \"GEOID\") %&gt;%\n  left_join(age_45_64, by = \"GEOID\") %&gt;%\n  left_join(age_65p,   by = \"GEOID\") %&gt;%\n  mutate(\n    pct_age_25_44 = 100 * age_25_44 / pop_total,\n    pct_age_45_64 = 100 * age_45_64 / pop_total,\n    pct_age_65plus = 100 * age_65plus / pop_total\n  ) %&gt;%\n  select(GEOID, pct_age_25_44, pct_age_45_64, pct_age_65plus)\n\n# housing + rent\nhousing_vars &lt;- c(\n  hu_total  = \"B25002_001\",\n  hu_vacant = \"B25002_003\",\n  med_rent  = \"B25064_001\"\n)\npa_housing &lt;- get_acs(\n  geography = GEOG,\n  state = STATE_FIPS,\n  year = ACS_YEAR,\n  survey = \"acs5\",\n  variables = housing_vars,\n  output = \"wide\"\n) %&gt;%\n  transmute(\n    GEOID,\n    vacancy_rate   = 100 * hu_vacantE / hu_totalE,\n    med_gross_rent = med_rentE\n  )\n\n# income + poverty\ninc_pov_vars &lt;- c(\n  med_hh_inc = \"B19013_001\",\n  pov_below  = \"B17001_002\",\n  pov_univ   = \"B17001_001\"\n)\npa_inc_pov &lt;- get_acs(\n  geography = GEOG,\n  state = STATE_FIPS,\n  year = ACS_YEAR,\n  survey = \"acs5\",\n  variables = inc_pov_vars,\n  output = \"wide\"\n) %&gt;%\n  transmute(\n    GEOID,\n    med_hh_income = med_hh_incE,\n    poverty_rate  = 100 * pov_belowE / pov_univE\n  )\n\n# education\nb15003 &lt;- get_acs(\n  geography = GEOG,\n  state = STATE_FIPS,\n  year = ACS_YEAR,\n  survey = \"acs5\",\n  table = \"B15003\"\n)\nedu_total &lt;- b15003 %&gt;%\n  filter(variable == \"B15003_001\") %&gt;%\n  select(GEOID, edu_total = estimate)\n\nba_plus_codes &lt;- c(\"B15003_022\",\"B15003_023\",\"B15003_024\",\"B15003_025\")\nedu_bap &lt;- b15003 %&gt;%\n  filter(variable %in% ba_plus_codes) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarize(ba_plus = sum(estimate, na.rm = TRUE), .groups = \"drop\")\n\nhs_only_code &lt;- \"B15003_017\"\nedu_hs &lt;- b15003 %&gt;%\n  filter(variable == hs_only_code) %&gt;%\n  transmute(GEOID, hs_only = estimate)\n\npa_edu &lt;- edu_total %&gt;%\n  left_join(edu_bap, by = \"GEOID\") %&gt;%\n  left_join(edu_hs,  by = \"GEOID\") %&gt;%\n  mutate(\n    pct_ba_plus = 100 * ba_plus / edu_total,\n    pct_hs_only = 100 * hs_only / edu_total\n  ) %&gt;%\n  select(GEOID, pct_ba_plus, pct_hs_only)\n\n# unemployment\nlabor_vars &lt;- c(\n  civ_lf     = \"B23025_003\",\n  unemployed = \"B23025_005\"\n)\npa_labor &lt;- get_acs(\n  geography = GEOG,\n  state = STATE_FIPS,\n  year = ACS_YEAR,\n  survey = \"acs5\",\n  variables = labor_vars,\n  output = \"wide\"\n) %&gt;%\n  transmute(\n    GEOID,\n    unemployment_rate = 100 * unemployedE / civ_lfE\n  )\n\nPA_ACS &lt;- pa_race %&gt;%\n  left_join(pa_age,     by = \"GEOID\") %&gt;%\n  left_join(pa_housing, by = \"GEOID\") %&gt;%\n  left_join(pa_inc_pov, by = \"GEOID\") %&gt;%\n  left_join(pa_edu,     by = \"GEOID\") %&gt;%\n  left_join(pa_labor,   by = \"GEOID\")\n\n\n############################################################\n# 5. ATTACH TRACT GEOID TO PARCELS AND MERGE ACS\n############################################################\n\n# get statewide tracts, project to CRS_TARGET\npa_tracts &lt;- tracts(state = \"PA\", year = ACS_YEAR, class = \"sf\") %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  select(GEOID)\n\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\nprops2tract &lt;- st_join(props_ctr, pa_tracts, join = st_within, left = TRUE)\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(st_drop_geometry(props2tract), by = \"prop_id\") %&gt;%\n  left_join(PA_ACS, by = \"GEOID\")\n\n\n############################################################\n# 6. FINAL FILTERS, EXPORT, DERIVED FIELDS\n############################################################\n\nn_before &lt;- nrow(clean_data)\n\nclean_data &lt;- clean_data %&gt;%\n  mutate(\n    sale_price = suppressWarnings(as.numeric(sale_price))\n  ) %&gt;%\n  filter(!is.na(sale_price), sale_price &gt;= 10000)\n\nn_after &lt;- nrow(clean_data)\nmessage(\"Dropped \", n_before - n_after,\n        \" rows with sale_price &lt; $10,000 (or missing).\")\n\n# create non-spatial copy for modeling, plus psf/value_multiple like you had\nsales_data &lt;- clean_data %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(\n    psf = round(sale_price / total_livable_area, 2),\n    value_multiple = round(market_value / sale_price, 2)\n  ) %&gt;%\n  arrange(psf)\n\ngeom_backup &lt;- clean_data\n\nrm(list = setdiff(ls(), c(\"clean_data\", \"sales_data\", \"geom_backup\" )))\n\n\nLogical Underpinning:\nThe modeling pipeline builds a property-level dataset that doesn’t just look at what the house is, but where the house is — and that “where” part is doing a lot of work. Beyond the standard housing attributes (square footage, year built, condition, number of beds/baths, etc.), we engineered a bunch of spatial features that try to capture a home’s context in Philadelphia. Every parcel in the sales dataset was turned into geometry using its recorded shape, projected into a local coordinate reference system, and then enriched with measurements like “How far is this property from transit?” or “How many food retailers are within a mile?” The point is: we’re not just assuming the structure itself determines sale price. We’re explicitly measuring access, amenities, safety, and neighborhood context as part of value.\nCrime exposure is one of the biggest contextual signals we added. For every sold property, we counted the number of reported “violent” incidents within roughly two blocks (~600 feet) and the number of “non-violent/misdemeanor” incidents within about one block (~300 feet). We defined “violent” broadly — not only homicides and robberies, but any incident where a gun or weapon was mentioned in the police data. The logic is that buyers and appraisers implicitly price in perceived safety. If two houses are physically similar but one sits in an area with more firearm-related incidents, we expect downward pressure on sale price. That becomes a numeric predictor: more nearby violent incidents → potentially lower willingness to pay.\nWe also quantified access to urban infrastructure and services, because convenience is money. For each property we measured the straight-line distance to the nearest transit stop, the nearest marked bike facility, the nearest hospital, the nearest fire station, and the nearest police station. We also measured distance to the nearest park, using parcel polygons against park polygons to approximate “are you on/next to green space or are you park-poor?” These distances were then standardized (feet or miles depending on context) and in some cases bucketed. For hospitals, for example, we didn’t just record distance — we classified properties into service bands: “Too Close” (&lt;1 mile, which might mean sirens/traffic and isn’t always seen as a plus), “Within Service” (1–5 miles, generally good coverage), and “Out of Service” (5+ miles). That turns an abstract geometry calculation into something interpretable as access vs. nuisance.\nFood access is handled a little differently because it’s partly about proximity and partly about density. For each property, we calculated (1) distance to the nearest food retailer (grocery, etc.) and (2) how many food retail locations exist within a one-mile buffer around that property’s centroid. That second feature is important: being near just one corner store is not the same thing as sitting in a commercial corridor with 10+ options. A buyer will pay more, all else equal, for a place where groceries are easy, healthy food is available, and “I don’t need a car for basics” is true. That’s especially relevant in Philadelphia, where car-free living is common and neighborhood retail is a quality-of-life driver.\nWe didn’t stop at amenities and services — we also tied every property to its neighborhood fabric. Each parcel was spatially joined to a named neighborhood (via centroid-in-neighborhood polygons), and to a Census tract. From the tract, we attached socioeconomic context from the American Community Survey: racial/ethnic composition, age structure, vacancy rate, median rent, median household income, poverty rate, unemployment rate, and educational attainment (e.g. share of adults with a bachelor’s or higher). Those variables roughly proxy demand-side pressure and neighborhood stability. High-income, low-vacancy, high-education tracts tend to support higher property prices, even for physically modest homes. That’s not just aesthetics — lenders, buyers, and investors all underwrite neighborhood trajectory, not just individual bricks.\nPut together, the result is a modeling dataset where sale price (or price per square foot) isn’t being predicted just from “bedrooms and bathrooms.” It’s being predicted from “bedrooms and bathrooms, plus how safe the block feels, plus how close you are to the El or a bike lane, plus whether you can walk to food, plus how your surrounding tract looks in terms of income and vacancy.” This is exactly how real-world housing markets behave: value is hyper-local and access-based. By engineering these spatial features, we’re letting the model learn the price premium (or penalty) associated with each of those location advantages — effectively turning geography into numbers the model can reason with.\n\n\n\n\nPart 2: Exploratory Data Analysis\nHistogram: Distribution of sale prices\n\nggplot(sales_data, aes(x = sale_price)) +\n  geom_histogram(fill = \"orange\", bins= 500) +\n  geom_vline(aes(xintercept = median(sale_price, na.rm = TRUE)),\n             color = \"grey40\", size = 0.5) +\n  annotate(\"text\", \n           x = median(sales_data$sale_price, na.rm = TRUE), \n           y = Inf, vjust = 4, hjust = -0.1, \n           label = paste0(\"Median = $\", formatC(median(sales_data$sale_price, na.rm = TRUE), format = \"f\", big.mark = \",\", digits = 0)),\n           color = \"grey40\", size = 3) +\n  labs(title = \"Distribution of Philadelphia Home Sale Price\", x = \"Sale Price (USD)\", y = \"Frequency\") +\n  scale_x_continuous(labels = scales::dollar)\n\n\n\n\n\n\n\n\nInterpretation: This figure shows the distribution of Philadelphia home sale prices in 2023–2024, and the key pattern is that the market is extremely skewed. Most sales are clustered well under 500K USD, and the median sale price — about 241K USD, marked on the plot — is a good summary of what a “typical” home actually sells for in the city. But there’s also a very long tail of high-value transactions that stretches into the millions. Those ultra-expensive deals are rare, but they’re so large that they would pull the average price up, which is why the mean would be misleading here. This shape tells us Philadelphia isn’t one uniform housing market: it’s a mix of lower-cost rowhomes, modest rehabs, and a smaller luxury segment. The result is a city where most buyers transact in the low hundreds of thousands, but a small number of high-end sales coexist on the fringe at several million dollars.\nMap: Geographic distribution (map)\n\n# Re-create the neighborhood layer from the URL\ninherits(clean_data, \"sf\")\n\n[1] TRUE\n\nneigh_url &lt;- \"https://raw.githubusercontent.com/opendataphilly/open-geo-data/refs/heads/master/philadelphia-neighborhoods/philadelphia-neighborhoods.geojson\"\n\nneigh &lt;- sf::read_sf(neigh_url)\n\n# 2. Make sure CRS matches parcels\nneigh &lt;- sf::st_transform(neigh, sf::st_crs(clean_data))\n\n# 3. Build the sf of JUST the modeled sales\n#    (this step assumes both data frames still share prop_id;\n#     if prop_id got dropped from sales_data, skip the filter)\nif (\"prop_id\" %in% names(clean_data) &&\n    \"prop_id\" %in% names(sales_data)) {\n  geo_clean &lt;- clean_data %&gt;%\n    dplyr::filter(prop_id %in% sales_data$prop_id)\n} else {\n  geo_clean &lt;- clean_data\n}\n\n# 4. Draw the map\np_price_map &lt;-\n  ggplot2::ggplot() +\n  ggplot2::geom_sf(\n    data = neigh,\n    fill = NA,\n    color = \"grey70\",\n    linewidth = 0.25\n  ) +\n  ggplot2::geom_sf(\n    data = geo_clean,\n    ggplot2::aes(color = sale_price),\n    alpha = 0.7,\n    size  = 0.2\n  ) +\n  scale_color_viridis_c(\n    name      = \"Sale price (USD)\",\n    option    = \"magma\",\n    direction = -1,\n    trans     = \"log10\",\n    breaks    = c(100000, 200000, 300000, 500000, 800000, 1200000, 2000000),\n    labels    = scales::dollar,\n    limits    = c(60000, 2000000),\n    oob       = scales::squish\n  ) +\n  ggplot2::guides(\n    color = ggplot2::guide_colorbar(barheight = grid::unit(4, \"cm\"))\n  ) +\n  ggplot2::coord_sf() +\n  ggplot2::labs(\n    title    = \"Geographical Distribution of Philadelphia Home Sale Prices\",\n    subtitle = \"Each property colored by sale price (log scale)\"\n  ) +\n  ggplot2::theme_void(base_size = 12) +\n  ggplot2::theme(\n    legend.position = c(0.92, 0.25),\n    plot.title      = ggplot2::element_text(face = \"bold\", hjust = 0),\n    plot.subtitle   = ggplot2::element_text(hjust = 0)\n  )\n\np_price_map\n\n\n\n\n\n\n\n\nInterpretation: This map shows that price in Philadelphia is deeply spatial. Each point is a property sale, colored by its sale price (on a log scale so we can see variation across the whole city). You can see clusters of high-value sales in a few tight areas — darker purple concentrations in and around Center City, parts of South Philly near the core, Northern Liberties/Fishtown, and select pockets in Northwest neighborhoods. Those are the submarkets where renovated or newer housing sells at a clear premium. As you move away from those cores, especially into large areas of Southwest, lower North, and parts of far Northeast and river-adjacent industrial fringe, prices shift toward the lighter end of the palette, meaning lower transaction values. The key insight is that this isn’t a smooth gradient from “expensive downtown” to “cheap outskirts.” Instead, it’s patchy: there are block- and neighborhood-scale price islands, even within the same general region of the city. That pattern is exactly why we model price with neighborhood fixed effects — because buyers are really choosing hyperlocal markets, not just “Philadelphia, broadly.”\nScatter Plots: Price vs. structural features\n\n# Build plotting data frame with clean vars\nplot_df &lt;- sales_data %&gt;%\n  mutate(\n    beds     = number_of_bedrooms,\n    baths    = number_of_bathrooms,\n    liv_area = total_livable_area,\n    age      = 2024 - year_built,\n    beds_f   = factor(number_of_bedrooms),\n    baths_f  = factor(number_of_bathrooms)\n  ) %&gt;%\n  filter(!is.na(sale_price),\n         !is.na(liv_area),\n         !is.na(age),\n         !is.na(beds), beds &gt; 0,\n         !is.na(baths), baths &gt; 0)\n\n## Plot A: Price vs. bedrooms (categorical)\np_beds &lt;- ggplot(plot_df, aes(x = beds_f, y = sale_price)) +\n  geom_jitter(width = 0.2, alpha = 0.15, size = 0.6, color = \"steelblue\") +\n  stat_summary(fun = median, geom = \"point\", color = \"red\", size = 2) +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Bedrooms (factor)\",\n    y = \"Sale price (USD)\",\n    title = \"Price vs Bedrooms\",\n    subtitle = \"Each dot = a sale; red = median price in that bedroom category\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Plot B: Price vs. bathrooms (categorical)\np_baths &lt;- ggplot(plot_df, aes(x = baths_f, y = sale_price)) +\n  geom_jitter(width = 0.2, alpha = 0.15, size = 0.6, color = \"darkorange\") +\n  stat_summary(fun = median, geom = \"point\", color = \"red\", size = 2) +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Bathrooms (factor)\",\n    y = \"Sale price (USD)\",\n    title = \"Price vs Bathrooms\",\n    subtitle = \"Adding bathrooms tends to move price up in steps, not smoothly\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Plot C: Price vs. interior living area (continuous)\np_area &lt;- ggplot(plot_df, aes(x = liv_area, y = sale_price)) +\n  geom_point(alpha = 0.15, size = 0.6, color = \"seagreen4\") +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.7, color = \"black\") +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Living area (sq ft)\",\n    y = \"Sale price (USD)\",\n    title = \"Price vs Interior Size\",\n    subtitle = \"Larger homes almost always sell for more\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Plot D: Price vs. building age (continuous)\np_age &lt;- ggplot(plot_df, aes(x = age, y = sale_price)) +\n  geom_point(alpha = 0.15, size = 0.6, color = \"purple4\") +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.7, color = \"black\") +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Building age (years)\",\n    y = \"Sale price (USD)\",\n    title = \"Price vs Building Age\",\n    subtitle = \"Newer buildings tend to transact higher, but there's a ton of variance\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Combine into one figure (2x2 grid)\nfigure_3_structural &lt;- (p_beds | p_baths) / (p_area | p_age)\n\nfigure_3_structural\n\n\n\n\n\n\n\n\nInterpretation:\n\nPrice vs Bedrooms: Each point is an individual sale, grouped by reported bedroom count. The red dots mark the median sale price for that bedroom category. You don’t see a smooth upward climb as beds go up. Instead, most of the market activity — and most of the value — sits in the 2–4 bedroom range, and adding more bedrooms beyond that doesn’t reliably buy you a more expensive house. In fact, once you control for overall square footage in the regression, “more bedrooms in the same box” can actually be a negative signal: it often means the house has been carved up into smaller rooms instead of feeling open. That shows up here as a pretty flat (and sometimes even downward) median past 3–4 beds, rather than a luxury premium for “more bedrooms.”\nPrice vs Bathrooms: Bathrooms behave differently. As you move from 1 bath to 2 to 3+ baths, the red median markers jump upward in more distinct steps. Buyers clearly pay for bathroom count, because bathrooms are basically a livability and privacy amenity. You don’t see as much scatter in the medians as you add baths — the relationship is more monotonic. That lines up with the model result that, holding everything else constant, each additional bathroom adds meaningful sale value. In plain English: a second full bath is treated as an upgrade; a fourth bedroom is not automatically treated as an upgrade.\nPrice vs Interior Size: This panel just shows the classic size premium. Larger homes almost always sell for more. Even though there’s a long tail of huge outliers (multi-million dollar sales and extremely large properties), the main cloud is very tight and upward sloping: once you get past about 1,000–1,500 square feet of living area, prices scale up quickly. The fitted line through that cloud makes that slope obvious. That’s consistent with our regression, where each additional square foot of interior living space is worth on the order of a few dozen USD — real money — and that effect is strong even after controlling for beds and baths.\nPrice vs Building Age: Age is noisy. Newer properties (low age on the x-axis) can sell very high, which matches the story of new construction and high-end rehab commanding premiums. But as age increases, there’s a wide spread: some 100-year-old Philly rowhomes still sell for serious money, and others sell cheaply. The smooth line trends slightly downward with age, but the variance explodes. That tells you age alone isn’t destiny. In Philly’s housing stock — a lot of which is 80–120+ years old — “old” can mean “historic, updated, desirable block,” or it can mean “deferred maintenance and structural risk.” The market doesn’t punish age uniformly; it punishes age plus condition.\n\nScatter Plots: Price vs. spatial features\n\n# Build plotting data frame for spatial/context features\nplot_spatial &lt;- sales_data %&gt;%\n  select(\n    sale_price,\n    dist_park_mi,\n    violent_2blocks,\n    foodretail_1mi_count,\n    med_hh_income\n  ) %&gt;%\n  filter(\n    !is.na(sale_price),\n    sale_price &gt; 0,\n    !is.na(dist_park_mi),\n    !is.na(violent_2blocks),\n    !is.na(foodretail_1mi_count),\n    !is.na(med_hh_income)\n  )\n\n## Plot A: Price vs distance to nearest park (miles)\np_park &lt;- ggplot(plot_spatial,\n                 aes(x = dist_park_mi, y = sale_price)) +\n  geom_point(alpha = 0.15, size = 0.6, color = \"darkgreen\") +\n  geom_smooth(method = \"lm\", se = FALSE,\n              linewidth = 0.7, color = \"black\") +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Distance to nearest park (miles)\",\n    y = \"Sale price (USD)\",\n    title = \"Parks & Property Value\",\n    subtitle = \"Homes farther from parks tend to sell for slightly less,\\nthough the effect is noisy\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Plot B: Price vs violent incidents within ~2 blocks\np_crime &lt;- ggplot(plot_spatial,\n                  aes(x = violent_2blocks, y = sale_price)) +\n  geom_point(alpha = 0.15, size = 0.6, color = \"firebrick\") +\n  geom_smooth(method = \"lm\", se = FALSE,\n              linewidth = 0.7, color = \"black\") +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Violent incidents within ~600 ft\",\n    y = \"Sale price (USD)\",\n    title = \"Local Violent Crime\",\n    subtitle = \"Higher recent violent incident counts are associated with\\nlower observed sale prices\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Plot C: Price vs food access (# of retailers within 1 mile)\np_food &lt;- ggplot(plot_spatial,\n                 aes(x = foodretail_1mi_count, y = sale_price)) +\n  geom_point(alpha = 0.15, size = 0.6, color = \"goldenrod4\") +\n  geom_smooth(method = \"lm\", se = FALSE,\n              linewidth = 0.7, color = \"black\") +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Food retailers within 1 mile\",\n    y = \"Sale price (USD)\",\n    title = \"Retail Food Environment\",\n    subtitle = \"More nearby food retail sometimes tracks denser, more affordable areas\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Plot D: Price vs neighborhood income (ACS)\np_income &lt;- ggplot(plot_spatial,\n                   aes(x = med_hh_income, y = sale_price)) +\n  geom_point(alpha = 0.15, size = 0.6, color = \"navy\") +\n  geom_smooth(method = \"lm\", se = FALSE,\n              linewidth = 0.7, color = \"black\") +\n  scale_x_continuous(labels = scales::dollar) +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Median household income (USD, tract)\",\n    y = \"Sale price (USD)\",\n    title = \"Income & Home Prices\",\n    subtitle = \"Higher-income Census tracts show systematically higher sale prices\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Arrange in a 2x2 panel\nfigure_4_spatial &lt;- (p_park | p_crime) / (p_food | p_income)\n\nfigure_4_spatial\n\n\n\n\n\n\n\n\nInterpretation:\n\nPrice vs Bedrooms: The first panel plots sale price against distance to the nearest park. There’s a weak downward pattern: homes closer to parks tend to reach somewhat higher prices, while homes farther away drift slightly lower. But the cloud is noisy — you can still get expensive sales even 0.5–0.75 miles from a park — which tells us “park adjacency” isn’t a universal price driver. It matters, but not nearly as much as core housing features like condition or size. This also hints that park access is partly a neighborhood-level amenity (already priced into where you’re buying), not always a block-by-block differentiator.\nLocal Violent Crime: Here we see sale price versus the number of violent incidents within roughly 600 feet of the property. This relationship is a lot more directional: higher recent violent counts are associated with sharply lower sale prices. The bottom edge of the market clusters in high-violence blocks, and the very high-dollar sales almost never appear where violence is concentrated. This backs up what we saw in the regression: even within the same nominal neighborhood, buyers are discounting blocks that feel unsafe. Street-level safety is being priced into the deal.\nRetail Food Environment: This panel shows how sale price varies with the number of food retailers within a one-mile radius. There’s basically no clean slope: properties in areas with lots of nearby grocers/delis/bodegas span the full range from cheaper houses to million-dollar-plus sales. If anything, very high counts of food outlets often show up in dense, lower-cost parts of the city — which makes sense in Philly, where corner retail is common in rowhouse neighborhoods. So “more food within a mile” doesn’t automatically equal “higher value.” It’s more of a neighborhood-style signal (urban, walkable, older fabric) than a direct premium within that neighborhood.\nIncome & Home Prices: This last panel compares Census tract median household income to sale price. Here the slope is much clearer and positive: higher-income tracts systematically produce higher sale prices, and you almost never see ultra-high-dollar sales in the very lowest-income tracts. That’s basically the social geography of value. It reflects more than house quality — it’s also about perceived status, lending environment, school expectations, and long-run stability. This is exactly the kind of demographic gradient that still shows up in the model even after we control for physical housing traits.\n\nBubble Plot: One creative visualization\n\nacs_vars &lt;- c(\n  med_income   = \"B19013_001\",\n  poverty_rate = \"S1701_C02_001\"\n)\n\nacs_tracts &lt;- get_acs(\n  geography = \"tract\",\n  state     = \"PA\",\n  county    = \"Philadelphia\",\n  variables = acs_vars,\n  year      = 2023,\n  survey    = \"acs5\",\n  geometry  = TRUE,\n  cache_table = TRUE\n) %&gt;%\n  select(GEOID, variable, estimate, geometry) %&gt;%\n  st_transform(st_crs(geo_clean)) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  st_as_sf()\n\nacs_to_neigh &lt;- acs_tracts %&gt;%\n  st_point_on_surface() %&gt;%\n  st_join(neigh %&gt;% select(NAME)) %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(NAME)) %&gt;%\n  group_by(NAME) %&gt;%\n  summarise(\n    med_income_acs   = median(med_income, na.rm = TRUE),\n    poverty_rate_acs = median(poverty_rate, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nsales_neigh &lt;- sales_data %&gt;%\n  filter(sale_price &gt; 0) %&gt;%\n  group_by(neigh_name) %&gt;%\n  summarise(\n    n_sales      = n(),\n    median_price = median(sale_price, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nneigh_summary_fixed &lt;- sales_neigh %&gt;%\n  left_join(acs_to_neigh, by = c(\"neigh_name\" = \"NAME\")) %&gt;%\n  drop_na(med_income_acs, poverty_rate_acs)\n\np_neigh_bubble_fixed &lt;-\n  ggplot(neigh_summary_fixed,\n         aes(x = med_income_acs, y = median_price,\n             size = n_sales, color = poverty_rate_acs)) +\n  geom_point(alpha = 0.85) +\n  scale_x_continuous(labels = scales::dollar) +\n  scale_y_continuous(labels = scales::dollar) +\n  scale_color_viridis_c(option = \"plasma\", name = \"Poverty rate (%)\") +\n  scale_size_continuous(name = \"Sales count\", range = c(2, 10)) +\n  labs(\n    title = \"Neighborhood Economics and Home Prices\",\n    subtitle = \"Each point = a neighborhood (2023–2024 sales)\\nX: ACS Median household income · Y: Median sale price · Color: ACS poverty rate · Size: sales count\",\n    x = \"Median household income (USD)\",\n    y = \"Median sale price (USD)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"right\")\n\np_neigh_bubble_fixed\n\n\n\n\n\n\n\n\nInterpretation: This bubble chart is basically the “neighborhood market reality” view. Each dot is a Philadelphia neighborhood, and we’re plotting its median sale price (y-axis) against its median household income from ACS (x-axis). Two things jump out immediately. First, the slope is very positive: richer neighborhoods almost always have higher typical sale prices. You almost never see a low-income neighborhood with a 700K+ median sale price, and you don’t see high-income neighborhoods with 150K medians — so price and local income are tightly linked at the neighborhood scale. Second, the color shading (poverty rate) lines up with that same gradient: neighborhoods with high poverty (yellow) sit in the low-income / low-price corner, while wealthier neighborhoods (purple/blue, lower poverty) show higher sales prices. The dot size is how many homes sold, which tells you where volume is. You can see that most of the activity (the big circles) is happening in the middle of the distribution — not the ultra-rich enclaves and not the most distressed areas, but the working-to-middle income neighborhoods. In plain language: Philadelphia’s housing market is stratified by neighborhood wealth and poverty status, and that stratification is not subtle.\n\n\n\nPart 3: Feature Pruning\n\nselect_data &lt;- sales_data |&gt; \n  select(year_built, number_of_bathrooms, number_of_bedrooms, basements, \n         fireplaces, garage_spaces,\n         general_construction, interior_condition, exterior_condition, \n         census_tract, neigh_name, \n         value_multiple, market_value, sale_price, total_area, total_livable_area, \n         psf, violent_2blocks, petty_1block, dist_park_mi, \n         vacancy_rate, med_gross_rent, med_hh_income, pct_ba_plus, \n         pct_white, pct_black, pct_hispanic, pct_asian,\n         foodretail_1mi_count, \n         dist_school_public_ft, dist_school_charter_ft, \n         pct_age_25_44, pct_age_65plus,\n         poverty_rate, unemployment_rate) |&gt; \n  filter(number_of_bathrooms &gt; 0, number_of_bedrooms &gt; 0) |&gt; \n  mutate(age = 2024 - year_built,\n         basements = dplyr::recode(trimws(as.character(basements)),\n                              \"0\"=\"0\",\"A\"=\"1\",\"B\"=\"2\",\"C\"=\"3\",\"D\"=\"4\",\"E\"=\"5\",\n                              \"F\"=\"6\",\"G\"=\"7\",\"H\"=\"8\",\"I\"=\"9\"),\n         basements = as.integer(basements)) |&gt; \n  rename(\n    baths      = number_of_bathrooms,\n    beds       = number_of_bedrooms,\n    construction = general_construction,\n    interior   = interior_condition,\n    exterior   = exterior_condition,\n    garage     = garage_spaces,\n    tract      = census_tract,\n    neigh      = neigh_name,\n    mkt_value  = market_value,\n    area       = total_area,\n    liv_area   = total_livable_area\n  )\n\nReasoning: For the modeling stage we deliberately narrowed the feature set to variables that are (1) directly interpretable in a housing valuation context, (2) broadly available across observations, and (3) not redundant with each other. The earlier pipeline generated a huge number of spatial and contextual attributes — multiple school distance measures, different types of crime buffers, access to transit, bike lanes, fire/police proximity, hospital service bands, etc. That’s great for exploratory geography-of-equity questions, but it’s overkill (and sometimes harmful) for estimating price. Many of those features are highly collinear with each other or with neighborhood identity, and a lot of them introduce sparsity because they’re missing for certain parcels or depend on fragile spatial joins. Here, we keep classic structural housing characteristics (beds, baths, living area, age, basements, condition, garage, fireplaces), because buyers literally pay for those. We keep a small set of location/amenity signals that plausibly capitalize into price and that we trust methodologically: nearby violent incidents (violent_2blocks), food access density (foodretail_1mi_count), park proximity (dist_park_mi), and distance to schools. We also attach tract-level socioeconomic context from ACS (income, vacancy, education, race composition, poverty, unemployment, and age structure) to proxy neighborhood demand and perceived quality. Finally, we retain sale_price itself plus market_value and ratios like value_multiple and psf, because those let us study how assessment and unit price per square foot behave. In short, we’re trimming down to high-signal, low-missingness, economically meaningful predictors and dropping niche engineering steps that add noise, inflate multicollinearity, or don’t generalize well in a pricing model.\n\n# Intuition: market value should not deviate substantially \n# from the sale price\nvalue_range &lt;- select_data |&gt; \n  summarize(\n    q1 = quantile(value_multiple, 0.25, na.rm = TRUE),\n    q3 = quantile(value_multiple, 0.75, na.rm = TRUE),\n    iqr = q3 - q1,\n    lower = q1 - 1.5 * iqr,\n    upper = q3 + 1.5 * iqr)\nvalue_range\n\n# A tibble: 1 × 5\n     q1    q3   iqr lower upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.87  1.16  0.29 0.435  1.59\n\n# Intuition: psf outliers are likely non-market transactions\npsf_range &lt;- select_data |&gt; \n  summarize(\n    q1 = quantile(psf, 0.25, na.rm = TRUE),\n    q3 = quantile(psf, 0.75, na.rm = TRUE),\n    iqr = q3 - q1,\n    lower = q1 - 1.5 * iqr,\n    upper = q3 + 1.5 * iqr)\npsf_range  \n\n# A tibble: 1 × 5\n     q1    q3   iqr lower upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  132.  266.  135. -70.4  468.\n\nclean_data &lt;- select_data |&gt; \n  filter(\n    liv_area &gt; 500,\n    area &gt; 500,\n    between(psf, 40, 800),\n    between(value_multiple, 0.4, 2.0)\n  ) |&gt; \n  mutate(across(where(is.numeric), ~ round(.x, 2))) |&gt; \n  drop_na() |&gt; \n  arrange(liv_area)\nclean_data\n\n# A tibble: 17,167 × 36\n   year_built baths  beds basements fireplaces garage construction interior\n        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1       1920     1     1         4          0      0 A                   2\n 2       1920     1     2         3          0      0 A                   4\n 3       1915     1     1         4          0      0 A                   4\n 4       1920     1     2         5          0      0 A                   2\n 5       1920     1     2         3          0      0 A                   3\n 6       1920     1     2         1          0      0 A                   2\n 7       1920     1     2         1          0      0 A                   2\n 8       1920     1     2         2          0      0 A                   2\n 9       1875     1     2         3          0      0 A                   4\n10       1875     1     2         3          0      0 A                   3\n# ℹ 17,157 more rows\n# ℹ 28 more variables: exterior &lt;dbl&gt;, tract &lt;dbl&gt;, neigh &lt;chr&gt;,\n#   value_multiple &lt;dbl&gt;, mkt_value &lt;dbl&gt;, sale_price &lt;dbl&gt;, area &lt;dbl&gt;,\n#   liv_area &lt;dbl&gt;, psf &lt;dbl&gt;, violent_2blocks &lt;dbl&gt;, petty_1block &lt;dbl&gt;,\n#   dist_park_mi &lt;dbl&gt;, vacancy_rate &lt;dbl&gt;, med_gross_rent &lt;dbl&gt;,\n#   med_hh_income &lt;dbl&gt;, pct_ba_plus &lt;dbl&gt;, pct_white &lt;dbl&gt;, pct_black &lt;dbl&gt;,\n#   pct_hispanic &lt;dbl&gt;, pct_asian &lt;dbl&gt;, foodretail_1mi_count &lt;dbl&gt;, …\n\n\nInterpretation: On top of trimming the feature set, we also had to sanity-check which sales even look like real, arm’s-length residential transactions. Property data always includes junk: family transfers for 1 USD, sheriff sales, partial interest transfers, renovation shells, or clerical weirdness (like a 2,000 sq ft rowhouse supposedly selling for 9 million USD). If we blindly feed that into a price model, the model learns nonsense. So we added a set of “real estate flags” to identify and drop those bad rows.\nWe use two main diagnostics: value_multiple and psf. value_multiple is the ratio of the city’s assessed market value to the actual recorded sale price. In a normal sale, the assessment may be a little high or a little low, but it shouldn’t be wildly off. So we look at the distribution of that ratio, get the interquartile range (IQR), and compute fence cutoffs (Q1 − 1.5×IQR, Q3 + 1.5×IQR). Extreme cases — where assessed value is, say, 5× the sale price or 0.1× the sale price — usually mean something non-market happened (distress sale, title correction, bundled sale, etc.). Those points don’t reflect what a typical buyer would pay for that one house, so we trim them. We do the same idea with psf (price per square foot of livable area). psf is a super useful “unit price” signal in real estate, but absurdly low psf often means interior is trashed/gutted or it wasn’t a real arm’s-length sale, and absurdly high psf often means the record is actually for a multi-parcel package or a luxury outlier that behaves more like commercial than typical housing. By fencing psf with IQR cutoffs (and then enforcing a practical range like 40–800 USD per sq ft), we keep what looks like real open-market housing trades in Philadelphia and drop pathological edge cases.\nWe also add basic physical filters: we require reasonable built form (livable area and lot area both over 500 sq ft, because “houses” with 120 sq ft recorded living area are usually data entry mistakes or partial condo interests). Then we require that all key numeric predictors exist and aren’t missing, and we round numeric values for cleaner interpretation. The result is clean_data: a modeling dataset that’s been denoised both structurally and financially. This step matters because linear models are very sensitive to extreme outliers — a handful of fake 1 USD transfers or impossible 5,000 USD/sf lofts can dominate the fit, wreck cross-validation metrics, and drown out the more subtle spatial and neighborhood effects we actually care about.\n\n\n\nPart 4: Modeling Building\n\nclean_data$neigh &lt;- as.factor(clean_data$neigh)\nclean_data$interior &lt;- as.factor(clean_data$interior)\nclean_data$exterior &lt;- as.factor(clean_data$exterior)\nclean_data$basements &lt;- as.factor(clean_data$basements)\n\nset.seed(5080) # Set seed for reproducibility\nctrl &lt;- trainControl(method = \"cv\", number = 10) #10-Fold CV\n\n# Model 1: Structural\ncv_m1 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2),\n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 2: + Spatial\ncv_m2 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  dist_park_mi + foodretail_1mi_count + dist_school_public_ft\n  + vacancy_rate + med_gross_rent + med_hh_income + pct_ba_plus + pct_black + pct_white\n  + poverty_rate + unemployment_rate + pct_age_25_44 + pct_age_65plus,\n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 3: + Neighborhood Fixed Effects\ncv_m3 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  dist_park_mi + foodretail_1mi_count + dist_school_public_ft +\n  vacancy_rate + med_gross_rent + med_hh_income + pct_ba_plus + pct_black + pct_white +\n  poverty_rate + unemployment_rate + pct_age_25_44 + pct_age_65plus +\n  neigh, \n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 4: + Small Neighborhoods + Mkt Value\nclean_data &lt;- clean_data |&gt; \n  add_count(neigh) |&gt; \n  mutate(\n    neigh_cv = if_else(\n      n &lt; 10,                       # If fewer than 10 sales\n      \"Small_Neighborhoods\",        # Group them\n      as.character(neigh)           # Keep original\n    ),\n    neigh_cv = as.factor(neigh_cv)\n  )\n\ncv_m4 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  violent_2blocks + dist_park_mi + foodretail_1mi_count + dist_school_public_ft + \n  vacancy_rate + med_gross_rent + med_hh_income + pct_ba_plus + pct_black + pct_white +\n  poverty_rate + unemployment_rate + pct_age_25_44 + pct_age_65plus + \n  mkt_value +\n  neigh_cv, \n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 5: + Simpler + Mkt Value\ncv_m5 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  dist_park_mi + foodretail_1mi_count + dist_school_public_ft +\n  mkt_value +\n  neigh_cv,  \n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 6: Simpler Only\ncv_m6 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  dist_park_mi + foodretail_1mi_count + dist_school_public_ft + \n  neigh_cv,  \n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Compare\ndata.frame(\n  Model = c(\"Structural\", \"Spatial\", \"Fixed Effects\", \"Small Neighborhoods + Mkt Value\", \"Simpler + Mkt Value\", \"Simpler Only\"),\n  Rsquared = c(cv_m1$results$Rsquared, cv_m2$results$Rsquared, cv_m3$results$Rsquared, \n               cv_m4$results$Rsquared, cv_m5$results$Rsquared, cv_m6$results$Rsquared),\n  RMSE = c(cv_m1$results$RMSE, cv_m2$results$RMSE, cv_m3$results$RMSE, \n           cv_m4$results$RMSE, cv_m5$results$RMSE, cv_m6$results$RMSE),\n  MAE = c(cv_m1$results$MAE, cv_m2$results$MAE, cv_m3$results$MAE, \n          cv_m4$results$MAE, cv_m5$results$MAE, cv_m6$results$MAE)\n)\n\n                            Model  Rsquared      RMSE      MAE\n1                      Structural 0.6704059 136209.73 89871.92\n2                         Spatial 0.7943316 108148.61 66208.30\n3                   Fixed Effects 0.8462181  93275.49 55837.75\n4 Small Neighborhoods + Mkt Value 0.9249335  64988.85 42264.31\n5             Simpler + Mkt Value 0.9248807  64847.06 42314.87\n6                    Simpler Only 0.8378573  95629.17 57645.29\n\n\nDiscussion: We trained a sequence of models using 10-fold cross-validation to see how different groups of predictors affect out-of-sample accuracy. The first model (“Structural”) uses only property-level physical characteristics: bedrooms, bathrooms, finished basement indicators, interior/exterior condition, living area, and building age. This baseline already explains about 67% of the variation in sale price (CV R² ≈ 0.67), but it’s still pretty rough: the average out-of-sample error is large, with an RMSE of about $136K and an average absolute error (MAE) of about 90K USD. In plain English, even if you know what the house is and what shape it’s in, you’re still often off by six figures. That’s a sign that location — broadly defined — is doing a lot of work in Philadelphia’s housing market.\nWhen we layer in block-scale spatial features and neighborhood socioeconomic context (“Spatial”), accuracy improves a lot. The R² jumps from ~0.67 to ~0.79, and RMSE drops from ~136K to ~108K. This version adds things like violent incidents within two blocks, distance to parks and schools, local retail access, and census tract indicators such as income, rent, vacancy, and demographics. That improvement tells us two things: (1) buyers absolutely price the surrounding environment — safety, access, social context — not just the building; and (2) those effects are strong enough to generalize out of sample. We’re not just overfitting noise; spatial context is genuinely predictive of what a house will sell for.\nBut “Spatial” is still treating the city as one big market. The third model (“Fixed Effects”) adds neighborhood fixed effects — basically, a separate intercept for each neighborhood — which soaks up persistent price differences between places like Fishtown, Point Breeze, Chestnut Hill, etc. That pushes R² to ~0.85 and drops RMSE to about 93K. This is a big deal. It means two homes with the same size, same condition, and same local crime exposure will still trade at systematically different price levels depending on which named neighborhood they’re in. In other words: neighborhood identity itself, not just measured amenities, is priced.\nFrom there, we refine how we treat neighborhood. The fourth model (“Small Neighborhoods + Mkt Value”) does two important things: (1) it collapses very small neighborhoods (with &lt;10 observed sales) into a pooled “Small_Neighborhoods” bin so that those areas don’t produce unstable fixed effects, and (2) it adds the city’s assessed market value (mkt_value) as a predictor. This changes everything. Performance jumps massively: R² rises to ~0.92 and RMSE falls to about 65K, cutting typical error almost in half compared to the baseline structural model. MAE also drops to about 42K. This model is both high-performing and still interpretable: it blends physical house attributes, micro-spatial exposure (crime, distance to parks, etc.), socioeconomic context, and a stabilized neighborhood effect, plus an institutional pricing signal (assessed market value). That combination is our best-performing “full” model.\nThen we test whether we can simplify without losing accuracy. The fifth model (“Simpler + Mkt Value”) strips out some of the weaker spatial/ACS controls but keeps assessed value and the smoothed neighborhood factor. Its performance is basically identical to the full version: R² ≈ 0.925, RMSE ≈ 65K, MAE ≈ 42K. That tells us there’s some redundancy — once you include assessed market value and a good neighborhood structure, you don’t necessarily need every single contextual variable to hit high predictive accuracy.\nFinally, we try “Simpler Only,” which keeps neighborhood effects but drops assessed value. Performance falls back: R² slips to ~0.84 and RMSE rises to ~95K, more like the plain fixed-effects model. That last comparison is super informative: it shows that the assessed market value term is doing a ton of predictive work. It acts like a summary statistic for block-level desirability, renovation quality, curb appeal, and whatever else the city’s assessment office is “seeing.” In practice, the best cross-validated model is the version that includes (i) structural features of the home, (ii) a stabilized neighborhood factor, and (iii) assessed market value. That model is accurate, relatively compact, and realistic to deploy for valuation or forecasting.\n\nfe_m4 &lt;- feols(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n    violent_2blocks + dist_park_mi + foodretail_1mi_count + dist_school_public_ft + \n    vacancy_rate + med_gross_rent + med_hh_income + pct_ba_plus + pct_black + pct_white +\n    poverty_rate + unemployment_rate + pct_age_25_44 + pct_age_65plus + \n    mkt_value |                    # left of | = regular covariates\n    neigh_cv,                      # right of | = fixed effect(s) to absorb\n  data = clean_data\n)\n\n# Pretty table (no neighborhood FE dummies will show)\n\nmodelsummary(\n  list(\"Final Model (FE absorbed)\" = fe_m4),\n  gof_omit  = 'IC|Adj|Log|F',\n  statistic = \"({std.error})\",     # &lt;-- this is the important change\n  estimate  = \"{estimate}{stars}\",\n  stars     = c('*'=.1,'**'=.05,'***'=.01),\n  output    = \"markdown\"\n)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Final Model (FE absorbed)\n              \n        \n        \n        \n                \n                  beds\n                  -3929.073***\n                \n                \n                  \n                  (995.184)\n                \n                \n                  baths\n                  13623.045***\n                \n                \n                  \n                  (1065.965)\n                \n                \n                  basements1\n                  18508.058***\n                \n                \n                  \n                  (4550.839)\n                \n                \n                  basements2\n                  14210.622***\n                \n                \n                  \n                  (4989.263)\n                \n                \n                  basements3\n                  2395.937\n                \n                \n                  \n                  (4620.944)\n                \n                \n                  basements4\n                  -575.383\n                \n                \n                  \n                  (4667.859)\n                \n                \n                  basements5\n                  17087.536***\n                \n                \n                  \n                  (4678.263)\n                \n                \n                  basements6\n                  8197.959*\n                \n                \n                  \n                  (4749.753)\n                \n                \n                  basements7\n                  8663.131\n                \n                \n                  \n                  (5364.591)\n                \n                \n                  basements8\n                  -2475.064\n                \n                \n                  \n                  (4965.209)\n                \n                \n                  basements9\n                  20032.616\n                \n                \n                  \n                  (12634.671)\n                \n                \n                  interior2\n                  93072.347***\n                \n                \n                  \n                  (26785.855)\n                \n                \n                  interior3\n                  85301.152***\n                \n                \n                  \n                  (26728.464)\n                \n                \n                  interior4\n                  61348.031**\n                \n                \n                  \n                  (26766.330)\n                \n                \n                  interior5\n                  57064.456**\n                \n                \n                  \n                  (27627.916)\n                \n                \n                  interior6\n                  62303.755**\n                \n                \n                  \n                  (30539.868)\n                \n                \n                  interior7\n                  49088.967\n                \n                \n                  \n                  (33695.231)\n                \n                \n                  exterior2\n                  -66798.324**\n                \n                \n                  \n                  (26790.498)\n                \n                \n                  exterior3\n                  -58734.577**\n                \n                \n                  \n                  (26625.500)\n                \n                \n                  exterior4\n                  -62136.692**\n                \n                \n                  \n                  (26658.029)\n                \n                \n                  exterior5\n                  -81130.078***\n                \n                \n                  \n                  (27234.942)\n                \n                \n                  exterior6\n                  -64371.930*\n                \n                \n                  \n                  (33516.951)\n                \n                \n                  exterior7\n                  -52907.936*\n                \n                \n                  \n                  (31591.774)\n                \n                \n                  liv_area\n                  36.687***\n                \n                \n                  \n                  (1.950)\n                \n                \n                  poly(age, 2)1\n                  139432.675\n                \n                \n                  \n                  (98611.735)\n                \n                \n                  poly(age, 2)2\n                  106047.003\n                \n                \n                  \n                  (91388.684)\n                \n                \n                  violent_2blocks\n                  -66.280***\n                \n                \n                  \n                  (22.525)\n                \n                \n                  dist_park_mi\n                  -2814.173\n                \n                \n                  \n                  (5132.350)\n                \n                \n                  foodretail_1mi_count\n                  -7.208\n                \n                \n                  \n                  (76.837)\n                \n                \n                  dist_school_public_ft\n                  -0.064\n                \n                \n                  \n                  (0.757)\n                \n                \n                  vacancy_rate\n                  49.463\n                \n                \n                  \n                  (161.086)\n                \n                \n                  med_gross_rent\n                  -1.961\n                \n                \n                  \n                  (3.329)\n                \n                \n                  med_hh_income\n                  0.109**\n                \n                \n                  \n                  (0.049)\n                \n                \n                  pct_ba_plus\n                  -34.383\n                \n                \n                  \n                  (94.752)\n                \n                \n                  pct_black\n                  -120.838*\n                \n                \n                  \n                  (71.110)\n                \n                \n                  pct_white\n                  -45.076\n                \n                \n                  \n                  (73.692)\n                \n                \n                  poverty_rate\n                  122.202\n                \n                \n                  \n                  (88.883)\n                \n                \n                  unemployment_rate\n                  -184.305\n                \n                \n                  \n                  (133.658)\n                \n                \n                  pct_age_25_44\n                  20.160\n                \n                \n                  \n                  (104.720)\n                \n                \n                  pct_age_65plus\n                  412.867***\n                \n                \n                  \n                  (138.801)\n                \n                \n                  mkt_value\n                  0.853***\n                \n                \n                  \n                  (0.006)\n                \n                \n                  Num.Obs.\n                  17167\n                \n                \n                  R2\n                  0.927\n                \n                \n                  R2 Within\n                  0.824\n                \n                \n                  RMSE\n                  64006.73\n                \n                \n                  Std.Errors\n                  IID\n                \n        \n      \n    \n\n\nlm_model4 &lt;- cv_m4$finalModel\n\nCoefficient Interpretation:\nThis model is estimated with neighborhood fixed effects (via neigh_cv), which means we’re not comparing a 1M USD rowhome in Center City to a 90K USD shell in Kensington. We’re comparing houses within the same local market bucket. So every coefficient should be read as: “holding the neighborhood constant, and holding everything else constant, how does this feature change sale price?” That matters because Philadelphia has a lot of lower- and middle-income housing stock with smaller square footage, aging structures, and incremental rehab. In that environment, buyers trade off things like layout, livable condition, block safety, and perceived stability more than just raw square footage. The fixed effects soak up the broad neighborhood premium/discount (schools, reputation, amenities, hype), so what’s left is what explains why two houses on similar turf sell for different prices.\nOn the physical side of the house, three patterns dominate. First, interior and exterior condition are extremely valuable. Compared to houses in visibly poor shape, houses in better interior condition sell for about 60K–90K USD more, and houses in worse exterior condition lose about 50K–80K USD in value, even after controlling for size, beds, and baths. That tells you what really clears in Philly’s for-sale market: buyers are paying for “move-in ready and looks solid from the street,” not just square footage. Bathrooms also carry a strong premium — roughly 13K USD per additional bath — which fits a story about livability and privacy. Bedrooms are trickier. Once you control for total living area, an extra bedroom actually correlates with slightly lower sale price. The way to read that is not “buyers hate bedrooms,” but “for two houses of the same size, the one chopped into more/smaller bedrooms can actually feel tighter and cheaper.” In Philadelphia’s rowhome housing stock — especially entry-level rehab product — that’s common: developers carve an extra bedroom into the same envelope, but buyers still value openness. Living area itself is priced at about 37 USD per additional square foot, which is a clean, intuitive slope: more usable space is still worth more money, but not all square footage is created equal if the layout feels cramped.\nWe also see localized social and environmental factors priced in, even within the same neighborhood. More violent incidents within roughly two blocks (about 600 feet) are associated with lower sale prices, on the order of tens of USD per additional incident. That might sound small per incident, but block-to-block differences add up, and the key point is this survives neighborhood fixed effects. In other words, even inside a neighborhood that’s broadly considered “high crime,” buyers still distinguish between a quieter block and a hotter corner. Meanwhile, access-style amenities like distance to parks, distance to schools, and nearby food retailers don’t come through as statistically meaningful once you control for neighborhood. That suggests those amenities are already baked into which neighborhood you’re in; they’re not the reason one house sells higher than the nearly identical house down the street. Said differently: once you’re in that neighborhood market, hyperlocal safety matters more than marginal proximity to a park.\nFinally, the model captures social context at the tract level — income, race, age structure — and those patterns are uncomfortable but real. Houses in tracts with higher median household income tend to sell for more, even within the same neighborhood bucket, which says that micro-areas of relative affluence inside a neighborhood command a premium. The share of Black residents in the tract is associated with lower sale prices, holding physical quality, crime, and everything else constant. That is not a story about “house quality”; that’s a story about how the market (buyers, lenders, appraisers) continues to discount majority-Black areas. This is consistent with persistent racialized valuation in U.S. housing markets. We also see a positive association with the share of seniors (65+): blocks with more older residents tend to have slightly higher sale prices, which likely proxies for stability — long-term owner occupancy, less churn, and “quiet” blocks. These demographic effects are all still happening inside neighborhoods, which means buyers are sensitive to the social micro-geography of a block or tract, not just its ZIP code.\nOne last anchor in the model is assessed market value. The city’s assessed value is extremely predictive of the actual sale price, at about 0.85 USD on the dollar. That does two things. First, it soaks up a lot of residual variation in quality — finishes, block vibe, unseen mechanical upgrades — that isn’t fully captured by our other variables. Second, it explains why the model fit is so strong (R² ≈ 0.93 overall, ~0.82 within neighborhoods): once you hold neighborhood fixed effects and you include what the city thinks the house is worth, you’ve basically captured both the structural story and the local market story. So the way to interpret this model is: inside a given Philadelphia neighborhood market, buyers pay a lot for finish and condition, they reward livable layouts and bathrooms, they discount safety risk on your immediate block, and they still respond to coded neighborhood status signals — income, racial composition, and perceived block stability — even after we’ve neutralized broad “which neighborhood are you in?” differences.\n\n\n\nPart 5: Model Diagnostics\nDiagnostic 1: Residual Plot\n\n# Residual Plot\nclean_data$residuals &lt;- residuals(lm_model4)\nclean_data$fitted &lt;- fitted(lm_model4)\n\nggplot(clean_data, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Residual Plot\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: random scatter in the residual plot indicates that the model is not missing anything systematic (i.e. biased predictions in predictable ways)\n\n# Breusch-Pagan Test for Heteroskedacity\nlibrary(lmtest)\nbptest(lm_model4)\n\n\n    studentized Breusch-Pagan test\n\ndata:  lm_model4\nBP = 2754.8, df = 173, p-value &lt; 2.2e-16\n\n\nInterpretation: while the small p-value is evidence of heteroskedacity, we have already added a large number of hedonic variables + spatial variables + neighborhood fixed effects. Hence, we acknowledge heteroskedacity as a limitation of our model because we are focused on point prediction rather than inference or confidence intervals.\nDiagnostic 2: Q-Q Plot\n\n# Plot Q-Q Plot to test Normality of Residuals\nq &lt;- ggplot(clean_data, aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot of Residuals in House Price Prediction\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles\") +\n  theme_minimal()\n\nprint(q)\n\n\n\n\n\n\n\n\nInterpretation: the Q-Q plot above shows that points generally fall close to the diagonal line. However, the ends (both left and right tails) curve sharply away from the diagonal line. This suggests that residuals have heavy tails — i.e., there are more extreme outliers than what would be expected under a normal distribution, which makes sense for home prices given that luxury home sales could skesw the model.\nDiagnostic 3: Cook’s Distance\n\n# Add diagnostic measures to tibble\ndiagnostic_data &lt;- augment(lm_model4, data = clean_data) |&gt; \n  mutate(\n    cooks_d = .cooksd,\n    leverage = .hat,\n    is_influential = cooks_d &gt; 4/nrow(clean_data)\n  )\n\n# Plot Cook's distance\nggplot(diagnostic_data, aes(x = 1:nrow(diagnostic_data), y = cooks_d)) +\n  geom_point(aes(color = is_influential), size = 2) +\n  geom_hline(yintercept = 4/nrow(diagnostic_data), \n             linetype = \"dashed\", color = \"red\") +\n  scale_color_manual(values = c(\"grey60\", \"red\")) +\n  labs(title = \"Cook's Distance\",\n       x = \"Observation\", y = \"Cook's D\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n# Rule of thumb: Cook's D &gt; 4/n\nthreshold &lt;- 4/nrow(clean_data)\n\ninfluential &lt;- diagnostic_data |&gt;\n  filter(cooks_d &gt; threshold)  |&gt;\n  select(age, baths, beds, mkt_value, liv_area, sale_price, neigh, cooks_d) |&gt;\n  arrange(desc(cooks_d))\n\nhead(influential, 10)\n\n# A tibble: 10 × 8\n     age baths  beds mkt_value liv_area sale_price neigh            cooks_d\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;              &lt;dbl&gt;\n 1    95     5     5   4952500    14150    6325000 CHESTNUT_HILL     0.282 \n 2    22     5     5   1202700     8055    2650000 UPPER_ROXBOROUGH  0.0599\n 3   109     3     5   1135400     3495    1535000 GARDEN_COURT      0.0258\n 4   224     3     4    639400     2050    1310000 FITLER_SQUARE     0.0233\n 5   224     4     5   1600000     4096    2455650 RITTENHOUSE       0.0167\n 6     6     4     5   2620600     4748    3125000 RITTENHOUSE       0.0159\n 7    74     3     5    910100     3264    1850000 EAST_FALLS        0.0151\n 8    54     2     3   1022100     2400    1825000 SOCIETY_HILL      0.0143\n 9    54     4     4   2033600     4000    1350000 SOCIETY_HILL      0.0133\n10    99     4     6   1251000     5340     850000 MOUNT_AIRY_EAST   0.0124\n\n\nInterpretation: Overall, the vast majority of observations have very low Cook’s D values and are clustered close to 0. However, there a small number of outliers that lie above the horizontal line (i.e. the rule-of-thumb threshold). As shown in the table above, these outliers are legitimate sale transactions because they have large livable areas and are located in premium neighborhoods such as Rittenhouse, Old City, and Chestnut Hill. It is still important to note that these outliers are influential and can potentially skew coefficients.\n\n\n\nPart 6: Conclusions & Recommendations\nWhat is your final model’s accuracy?\nWe tested four models and watched how prediction quality improved as we layered in more information.\n\nStructural only (beds, baths, square footage, age, condition): RMSE ≈ 136,000 USD.\n\nSpatial context (parks, schools, food access, neighborhood socioeconomic stats, crime): RMSE ≈ 108,000 USD.\n\n\nNeighborhood fixed effects (absorbing neighborhood differences directly): RMSE ≈ 93,000 USD.\n\n\nCollapsing small neighborhoods + adding assessed market value: RMSE ≈ 65,000 USD.\n\n\nSo the final model — the “Small Neighborhoods + Mkt Value” spec — cuts average error almost in half compared to the basic structural model (136K → 65K USD). That’s a huge jump. This tells us two things. First, house-level physical traits are not enough to explain price in Philly. Second, you get massive gains in accuracy from (a) knowing where in the city the house is, down to the neighborhood and even the block, and (b) using the city’s own market assessment as a kind of summary of unobserved quality. The final model is accurate enough to be useful for price benchmarking and valuation discussions, not just academic pattern-finding.\nAlso look at R²: it climbs from about 0.67 in the structural model to ~0.92 in the final model. That means we’re capturing ~92% of the variation in sale prices with publicly available data plus neighborhood controls.\nWhich features matter most for Philadelphia prices?\n\nPhysical livability: Interior condition has a massive effect: “better interior” categories add on the order of tens of thousands of USD relative to poor interior condition. Exterior condition works the same direction but in reverse: worse exterior knocks off large chunks of value (often 50K–80K USD less). Bathrooms are also very valuable — adding one more bathroom is associated with roughly +13K USD, holding other things constant. Living area matters too: each additional square foot of livable space sells for ~37 USD.\nLayout quality, not just bedroom count: More bedrooms is not automatically “more expensive.” After controlling for total livable area, additional bedrooms are actually associated with lower prices. The story there: if two houses are the same size, the one chopped into more (smaller) bedrooms is viewed as less desirable. Buyers want usable, open-feeling space, not just “4BR” on paper. Basements in usable form, on the other hand, usually add value, and in some categories that bump is &gt;10K USD.\nImmediate block conditions (social environment): Violent incidents within roughly two blocks (≈600 ft) are priced in. Every additional violent incident nearby lowers the sale price. That’s not just “bad neighborhood vs good neighborhood,” because we absorb neighborhood fixed effects — this is within-neighborhood, block-level safety perception. So buyers are discounting homes on hotter corners, even if they’re still in an otherwise desirable area.\nNeighborhood market position / wealth signals: Median household income in the tract is positively associated with higher sale prices, and assessed market value from the city is extremely predictive (roughly 0.85 USD of sale price per 1 USD of assessed value). That basically means: if the assessor thinks you’re a high-value property, the market mostly agrees. A decent chunk of “finish quality,” “renovation level,” and “this is a block that’s turning” is being captured through that single variable.\n\nWhich neighborhoods are hardest to predict?\n\nneigh_url &lt;- \"https://raw.githubusercontent.com/opendataphilly/open-geo-data/refs/heads/master/philadelphia-neighborhoods/philadelphia-neighborhoods.geojson\"\nneigh &lt;- sf::read_sf(neigh_url)\n\n# make sure neigh has a character neighborhood name column we can join on\n# most Philly neighborhood layers use NAME\nif (!\"NAME\" %in% names(neigh)) {\n  stop(\"Neighborhood layer is missing NAME column; inspect `names(neigh)`.\")\n}\n\n############################################################\n# 1. Add per-model predictions & residuals to clean_data\n############################################################\n\nclean_data &lt;- clean_data %&gt;%\n  mutate(\n    pred_m1  = predict(cv_m1, newdata = clean_data),\n    resid_m1 = sale_price - pred_m1,\n    pred_m2  = predict(cv_m2, newdata = clean_data),\n    resid_m2 = sale_price - pred_m2,\n    pred_m4  = predict(cv_m4, newdata = clean_data),\n    resid_m4 = sale_price - pred_m4,\n    pred_m5  = predict(cv_m5, newdata = clean_data),\n    resid_m5 = sale_price - pred_m5,\n    pred_m6  = predict(cv_m6, newdata = clean_data),\n    resid_m6 = sale_price - pred_m6\n  )\n\n# standardize neighborhood name text in the sales rows\nclean_data_std &lt;- clean_data %&gt;%\n  mutate(\n    neigh = neigh %&gt;%\n      as.character() %&gt;%\n      str_trim()\n  )\n\n############################################################\n# 2. Collapse residuals to neighborhood-level stats\n############################################################\n\nneigh_resid_long &lt;- clean_data_std %&gt;%\n  select(neigh, starts_with(\"resid_\")) %&gt;%\n  pivot_longer(\n    cols = starts_with(\"resid_\"),\n    names_to = \"model\",\n    values_to = \"resid\"\n  ) %&gt;%\n  mutate(\n    # keep only the models we care about\n    model = factor(\n      model,\n      levels = c(\"resid_m1\", \"resid_m2\", \"resid_m4\", \"resid_m5\", \"resid_m6\"),\n      labels = c(\"Model 1\", \"Model 2\", \"Model 4\", \"Model 5\", \"Model 6\")\n    )\n  ) %&gt;%\n  filter(!is.na(model)) %&gt;%  # drop any others\n  group_by(neigh, model) %&gt;%\n  summarise(\n    n_sales    = n(),\n    mean_res   = mean(resid, na.rm = TRUE),                     # avg under/over\n    median_res = median(resid, na.rm = TRUE),\n    rmse_res   = sqrt(mean(resid^2, na.rm = TRUE)),\n    .groups    = \"drop\"\n  )\n\n############################################################\n# 3. Join neighborhood stats back to neighborhood polygons\n############################################################\n\nneigh_map_all &lt;- neigh %&gt;%\n  mutate(\n    neigh = NAME %&gt;%\n      as.character() %&gt;%\n      str_trim()\n  ) %&gt;%\n  left_join(neigh_resid_long, by = \"neigh\")\n\n# neigh_map_all is now sf with columns:\n# neigh, model, mean_res, etc.\n\n############################################################\n# 4. Helper: map residuals for ONE model\n#    - dynamic symmetric color scale per model\n#    - simple title (Model 1, etc), no subtitle in panels\n############################################################\n\nplot_neigh_residual_map &lt;- function(model_label, na_col = \"grey90\") {\n\n  df &lt;- neigh_map_all %&gt;% filter(model == model_label)\n\n  # symmetric color limit per model\n  lim_sym &lt;- max(abs(df$mean_res), na.rm = TRUE)\n  if (!is.finite(lim_sym) || lim_sym &lt;= 0) {\n    lim_sym &lt;- NA_real_\n  }\n\n  ggplot(df) +\n    geom_sf(\n      aes(fill = mean_res),\n      color = \"grey40\",\n      linewidth = 0.2\n    ) +\n    scale_fill_gradient2(\n      name      = \"Mean residual (USD)\",\n      low       = \"#a4133c\",\n      mid       = \"white\",\n      high      = \"#1a759f\",\n      midpoint  = 0,\n      limits    = if (is.finite(lim_sym)) c(-lim_sym, lim_sym) else waiver(),\n      oob       = squish,\n      na.value  = na_col,\n      labels    = dollar\n    ) +\n    labs(\n      title = model_label,\n      x = NULL,\n      y = NULL\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.major = element_blank(),\n      panel.grid.minor = element_blank(),\n      plot.title       = element_text(face = \"bold\", size = 12, hjust = 0),\n      axis.text        = element_blank(),\n      axis.ticks       = element_blank(),\n      legend.position  = \"right\"\n    )\n}\n\n############################################################\n# 5. Build panels for each model (dropping Model 3)\n############################################################\n\np_model1 &lt;- plot_neigh_residual_map(\"Model 1\")\np_model2 &lt;- plot_neigh_residual_map(\"Model 2\")\np_model4 &lt;- plot_neigh_residual_map(\"Model 4\")\np_model5 &lt;- plot_neigh_residual_map(\"Model 5\")\np_model6 &lt;- plot_neigh_residual_map(\"Model 6\")\n\nblank_panel &lt;- ggplot() + theme_void()\n\n############################################################\n# 6. Assemble patchwork figure (2x3 grid layout)\n############################################################\n\nresid_maps_all &lt;-\n  (p_model1 | p_model2 | p_model4) /\n  (p_model5 | p_model6 | blank_panel) +\n  plot_annotation(\n    title = \"Which Neighborhoods Are Hardest to Predict?\",\n    subtitle = paste(\n      \"Mean model residual (USD) by neighborhood.\",\n      \"Blue = model underpredicts (homes sold for MORE than predicted).\",\n      \"Red = model overpredicts (homes sold for LESS).\",\n      sep = \"\\n\"\n    ),\n    theme = theme(\n      plot.title    = element_text(face = \"bold\", size = 16, hjust = 0),\n      plot.subtitle = element_text(size = 11, hjust = 0)\n    )\n  ) &\n  theme(\n    legend.key.height = unit(0.4, \"in\"),\n    legend.key.width  = unit(0.15, \"in\")\n  )\n\n############################################################\n# 7. Print final figure for the slide\n############################################################\n\nprint(resid_maps_all)\n\n\n\n\n\n\n\n\nEquity concerns?\nYes, and they show up in the coefficients. Even after controlling for interior quality, exterior condition, square footage, bathroom count, crime on the block, and neighborhood fixed effects, properties in areas with a higher share of Black residents tend to sell for less. That is not a physical-quality story — we already held physical quality constant. It’s also not just “this is a cheaper neighborhood,” because neighborhood fixed effects are in there. It’s evidence of structural discounting: the same basic housing form is being valued lower in Blacker areas.\nWe also see that tracts with higher median household income sell for more, and tracts with more older residents (65+) sell for more. You can read that as the market attaching a price premium to “social stability” or “aging, long-tenure homeowners with resources,” and a penalty to neighborhoods racialized as lower-value. That has obvious fair housing implications. If you used a model like this to “objectively” value collateral for lending or tax assessment without adjustments, you’d be baking those disparities directly into policy. In other words, the market is not just pricing bricks and bathrooms; it’s pricing neighborhood identity — including race and class — and we can see it in the data.\nLimitations?\n\nWe model sale price, not willingness to pay in general. Off-market transfers, distressed sales, family transfers, or cash flips at weird prices all exist and can leak into the data. We tried to filter out some non-arm’s-length / broken records using safeguards like price-per-square-foot ranges and value_multiple cutoffs, but it’s not perfect.\nWe don’t observe interiors the way an appraiser or buyer does. “Interior condition” is categorical and crude. It does a lot of work in the model, but it’s still way less detailed than listing photos, finishes, HVAC age, roof age, etc. So some of what shows up in the assessed market value term is just “good renovation quality we didn’t explicitly encode.”\nTiming and direction of causality. Crime is measured during 2023–2024, and we’re relating that to sales in roughly that same window. Buyers react to reputation and perceived safety, not just the literal last 12 months of incidents. So the coefficient on violent_2blocks is “areas that are known to be hotter tend to sell for less,” not “if one assault happens on your block today, you instantly lose X USD tomorrow.”\nDemographic variables are structurally loaded. The fact that pct_black is negatively associated with price is not a statement about intrinsic property value. It’s capturing systemic bias — lending, appraisal, speculative expectations — and we’re treating that as a “feature.” That’s useful for prediction, but dangerous for policy if used naively. You would not want to bake that into automated valuation tools without guardrails, because it would literally encode discrimination.\nFixed effects hide macro gap but don’t solve it. Neighborhood fixed effects let us ask “why does this house sell for more than that house within the same neighborhood?” That’s exactly what we want for local pricing. But fixed effects also absorb neighborhood-level disparities themselves. That means we’re not modeling why one neighborhood as a whole appraises lower than another neighborhood, which is also an equity question — we’re holding that gap constant.\n\nDeclaration ChatGPT was used to amend language errors and allow for more seamless sentence sturcture/syntax. It was also used to aid in certain repetitive coding tasks. All of which were double checked for accuracy and at times redone by another team memebr to ensure ownership of both content and code."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes - Course Introduction",
    "section": "",
    "text": "bias in visualization summary can hide some important details example: Anscombe’s Quartet\n\ncan not get ACS for census block, only decenial\ncensus block groups have big margin of error (ACS), T island problem\ncensus tracts are better\n\nthe smaller of the sample, the bigger margin of error\n\n\n\nggplot ( data = your_data ) + aes ( x = variable1, y = variable2 ) + geom_something ( ) + additional_layers (color… )\naes: - X,y - color - fill - size - shape - alpha _ transparency\n\n\n\n\ndistribution\n\n\n\n\n\nleft join is preserve the table at first (often option)\nright join …. The second table\nfull join is to preserve all the tables (necessary sometimes)\ninner joint is to find the result held by both tables"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#why-visualization-matters",
    "href": "weekly-notes/week-03-notes.html#why-visualization-matters",
    "title": "Week 3 Notes - Course Introduction",
    "section": "",
    "text": "bias in visualization summary can hide some important details example: Anscombe’s Quartet\n\ncan not get ACS for census block, only decenial\ncensus block groups have big margin of error (ACS), T island problem\ncensus tracts are better\n\nthe smaller of the sample, the bigger margin of error"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#grammar-of-graphics",
    "href": "weekly-notes/week-03-notes.html#grammar-of-graphics",
    "title": "Week 3 Notes - Course Introduction",
    "section": "",
    "text": "ggplot ( data = your_data ) + aes ( x = variable1, y = variable2 ) + geom_something ( ) + additional_layers (color… )\naes: - X,y - color - fill - size - shape - alpha _ transparency"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#exploratory-data-analysis",
    "href": "weekly-notes/week-03-notes.html#exploratory-data-analysis",
    "title": "Week 3 Notes - Course Introduction",
    "section": "",
    "text": "distribution"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#join",
    "href": "weekly-notes/week-03-notes.html#join",
    "title": "Week 3 Notes - Course Introduction",
    "section": "",
    "text": "left join is preserve the table at first (often option)\nright join …. The second table\nfull join is to preserve all the tables (necessary sometimes)\ninner joint is to find the result held by both tables"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes - Course Introduction",
    "section": "",
    "text": "set of rules to solve problems\nissues of data-driven policy-making: what is true for a group of people is not always true for individuals"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#algorithm",
    "href": "weekly-notes/week-02-notes.html#algorithm",
    "title": "Week 2 Notes - Course Introduction",
    "section": "",
    "text": "set of rules to solve problems\nissues of data-driven policy-making: what is true for a group of people is not always true for individuals"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#difference-between-data-science-and-data-analysis",
    "href": "weekly-notes/week-02-notes.html#difference-between-data-science-and-data-analysis",
    "title": "Week 2 Notes - Course Introduction",
    "section": "Difference between data science and data analysis",
    "text": "Difference between data science and data analysis\n\ndata science: focus on algorithms/methods development\ndata analytics: application"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#machine-learning-and-ai",
    "href": "weekly-notes/week-02-notes.html#machine-learning-and-ai",
    "title": "Week 2 Notes - Course Introduction",
    "section": "Machine learning and AI",
    "text": "Machine learning and AI\n\nMachine learning: classification&prediction learn from data\nAI: adjust and improve across iterations"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#accidental-data",
    "href": "weekly-notes/week-02-notes.html#accidental-data",
    "title": "Week 2 Notes - Course Introduction",
    "section": "Accidental data",
    "text": "Accidental data\n\nget data from open-source sources like social media platforms, instagram"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#data-analytics-is-subjective",
    "href": "weekly-notes/week-02-notes.html#data-analytics-is-subjective",
    "title": "Week 2 Notes - Course Introduction",
    "section": "Data analytics is subjective",
    "text": "Data analytics is subjective\n\ndata cleaning\ndata coding\ndata collection\ninterpret results\nwhat variables to put in the model"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#decennial-census-american-community-survey-acs",
    "href": "weekly-notes/week-02-notes.html#decennial-census-american-community-survey-acs",
    "title": "Week 2 Notes - Course Introduction",
    "section": "Decennial Census & American Community Survey (ACS)",
    "text": "Decennial Census & American Community Survey (ACS)\n\nDecennial Census\n\nfor everyone, full sample\n9 basic questions\nconstitutional requirement\n\nAmerican Community Survey\n\n3% surveyed annually\ndetailed questions\n5-yr estimates are more reliable"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html",
    "href": "weekly-notes/week-09-notes.html",
    "title": "Week 9 Notes - Course Introduction",
    "section": "",
    "text": "Before building predictive policing models, we must ask: Should we?\nA model can be statistically valid yet socially harmful.\nTechnical performance ≠ ethical justification.\n\n\n\n\n\nPromised benefits: - Efficiency, objectivity, proactivity, data-driven policing.\nUnderlying assumptions: 1. Crime data reflects real crime (false). 2. Past data predicts future crime (often just predicts policing). 3. “Good” data can be isolated from “bad” (rarely). 4. Technology can fix social problems (cannot).\nReality: Predictive systems often reproduce and reinforce structural bias.\n\n\n\n\nTraditional “dirty” data: missing, inaccurate, inconsistent.\nExpanded definition (Richardson et al. 2019): - Data influenced by biased, unlawful, or corrupt practices.\nKey forms: 1. Fabricated or manipulated data (e.g., false arrests, downgraded offenses). 2. Systematically biased data (over-/under-policing patterns). 3. Missing or incomplete records (unreported crimes). 4. Proxy data (arrests ≠ crime, calls ≠ need).\nResult: Predictive models trained on distorted representations of reality.\n\n\n\n\nBaltimore: - Misclassification of assaults, planted evidence, data manipulation. - “Juking the stats” → inflated police success metrics.\nNYPD & CompStat: - Pressure for low crime rates → falsified data. - Downgrading serious crimes, coercing victims, fabricated arrests.\nFeedback loop: biased data → biased predictions → targeted policing → more biased data.\n\n\n\n\n\nCrime reports reflect officer discretion.\nCalls for service reflect community bias and gentrification.\n“Clean data” is a myth; all crime data is socially constructed and politically shaped.\nData records policing patterns, not crime itself.\n\n\n\n\n\nFalse positives: over-policing, surveillance, stress, community distrust.\nFalse negatives: under-protection, neglect.\nBoth disproportionately affect marginalized communities.\n\n\n\n\nTypical reforms: training, oversight, data collection, transparency.\nWhat’s missing:\n- Historical data correction, vendor accountability, algorithmic transparency.\nExample: Chicago consent decree (2019) lacked restrictions on biased data use.\n\n\n\n\nKey questions for assessing predictive systems: 1. Data provenance: When and how was data collected? Was it lawful? 2. Variable selection: Which inputs embed bias? 3. Validation: Who bears false positives/negatives? 4. Deployment: How are predictions operationalized? 5. Accountability: Who audits, explains, or challenges outputs? 6. Alternatives: Are resources used for prevention or punishment?\n\n\n\n\n\nA system predicting “where police will arrest” may be accurate but unjust.\n“Success” metrics often measure predictive policing efficiency, not safety or equity.\n\n\n\n\n\nReframe the question: - Instead of predicting crime → predict social need. - Example applications: - Eviction risk → legal aid. - Health risk → care outreach. - Dropout risk → educational support. - Food insecurity → resource allocation.\nPrinciple: Predict for care, not control."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#foundational-question",
    "href": "weekly-notes/week-09-notes.html#foundational-question",
    "title": "Week 9 Notes - Course Introduction",
    "section": "",
    "text": "Before building predictive policing models, we must ask: Should we?\nA model can be statistically valid yet socially harmful.\nTechnical performance ≠ ethical justification."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#the-promise-vs.-the-reality",
    "href": "weekly-notes/week-09-notes.html#the-promise-vs.-the-reality",
    "title": "Week 9 Notes - Course Introduction",
    "section": "",
    "text": "Promised benefits: - Efficiency, objectivity, proactivity, data-driven policing.\nUnderlying assumptions: 1. Crime data reflects real crime (false). 2. Past data predicts future crime (often just predicts policing). 3. “Good” data can be isolated from “bad” (rarely). 4. Technology can fix social problems (cannot).\nReality: Predictive systems often reproduce and reinforce structural bias."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#dirty-data-and-its-sources",
    "href": "weekly-notes/week-09-notes.html#dirty-data-and-its-sources",
    "title": "Week 9 Notes - Course Introduction",
    "section": "",
    "text": "Traditional “dirty” data: missing, inaccurate, inconsistent.\nExpanded definition (Richardson et al. 2019): - Data influenced by biased, unlawful, or corrupt practices.\nKey forms: 1. Fabricated or manipulated data (e.g., false arrests, downgraded offenses). 2. Systematically biased data (over-/under-policing patterns). 3. Missing or incomplete records (unreported crimes). 4. Proxy data (arrests ≠ crime, calls ≠ need).\nResult: Predictive models trained on distorted representations of reality."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#case-studies",
    "href": "weekly-notes/week-09-notes.html#case-studies",
    "title": "Week 9 Notes - Course Introduction",
    "section": "",
    "text": "Baltimore: - Misclassification of assaults, planted evidence, data manipulation. - “Juking the stats” → inflated police success metrics.\nNYPD & CompStat: - Pressure for low crime rates → falsified data. - Downgrading serious crimes, coercing victims, fabricated arrests.\nFeedback loop: biased data → biased predictions → targeted policing → more biased data."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#why-cleaning-isnt-enough",
    "href": "weekly-notes/week-09-notes.html#why-cleaning-isnt-enough",
    "title": "Week 9 Notes - Course Introduction",
    "section": "",
    "text": "Crime reports reflect officer discretion.\nCalls for service reflect community bias and gentrification.\n“Clean data” is a myth; all crime data is socially constructed and politically shaped.\nData records policing patterns, not crime itself."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#consequences-and-harms",
    "href": "weekly-notes/week-09-notes.html#consequences-and-harms",
    "title": "Week 9 Notes - Course Introduction",
    "section": "",
    "text": "False positives: over-policing, surveillance, stress, community distrust.\nFalse negatives: under-protection, neglect.\nBoth disproportionately affect marginalized communities."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#reform-and-consent-decrees",
    "href": "weekly-notes/week-09-notes.html#reform-and-consent-decrees",
    "title": "Week 9 Notes - Course Introduction",
    "section": "",
    "text": "Typical reforms: training, oversight, data collection, transparency.\nWhat’s missing:\n- Historical data correction, vendor accountability, algorithmic transparency.\nExample: Chicago consent decree (2019) lacked restrictions on biased data use."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#critical-evaluation-framework",
    "href": "weekly-notes/week-09-notes.html#critical-evaluation-framework",
    "title": "Week 9 Notes - Course Introduction",
    "section": "",
    "text": "Key questions for assessing predictive systems: 1. Data provenance: When and how was data collected? Was it lawful? 2. Variable selection: Which inputs embed bias? 3. Validation: Who bears false positives/negatives? 4. Deployment: How are predictions operationalized? 5. Accountability: Who audits, explains, or challenges outputs? 6. Alternatives: Are resources used for prevention or punishment?"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#technical-vs.-ethical-accuracy",
    "href": "weekly-notes/week-09-notes.html#technical-vs.-ethical-accuracy",
    "title": "Week 9 Notes - Course Introduction",
    "section": "",
    "text": "A system predicting “where police will arrest” may be accurate but unjust.\n“Success” metrics often measure predictive policing efficiency, not safety or equity."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#toward-ethical-predictive-modeling",
    "href": "weekly-notes/week-09-notes.html#toward-ethical-predictive-modeling",
    "title": "Week 9 Notes - Course Introduction",
    "section": "",
    "text": "Reframe the question: - Instead of predicting crime → predict social need. - Example applications: - Eviction risk → legal aid. - Health risk → care outreach. - Dropout risk → educational support. - Food insecurity → resource allocation.\nPrinciple: Predict for care, not control."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 Notes - Course Introduction",
    "section": "",
    "text": "Add progress = FALSE in each function call\nSet globally on the top of the doc\n\n\n\nCode\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  # Suppress tigris progress bars\n\n\n\nevaluation and echo\n\n\n\nCode\n# eval = true, run the code; echo = true, show the result"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#rendering-issues",
    "href": "weekly-notes/week-07-notes.html#rendering-issues",
    "title": "Week 7 Notes - Course Introduction",
    "section": "",
    "text": "Add progress = FALSE in each function call\nSet globally on the top of the doc\n\n\n\nCode\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  # Suppress tigris progress bars\n\n\n\nevaluation and echo\n\n\n\nCode\n# eval = true, run the code; echo = true, show the result"
  }
]